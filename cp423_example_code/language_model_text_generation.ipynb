{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "language_model_text_generation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this demo, we will explore the probabilistic language model and use the N-gram model to generate. Then we can estimate the probability of a sentence using the language models"
      ],
      "metadata": {
        "id": "9Y-_8P98yfL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A language model learns the probability of word occurrence based on sampling the text from the document collection or corpus.\n",
        "given the text sequence  $w_1,..., w_n$, the vocabulary V is defined as $V = \\{w_1, w_2, w_3,...,w_m\\}$ then the probability distribution of the next upcoming word $x_{n+1}$ in the sequence can be any word $w$ in $V$. <p>\n",
        "the conditional proability is $P(w_{n+1} | x_n, x_{n-1},...,x_2, x_1)$\n",
        "We  can also think a Language Model is a task of assigning a probability to a sentence or sequence."
      ],
      "metadata": {
        "id": "kpz85iHBy8gy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the sentence = 'I came by bus' --> tokens = ['I', 'came', 'by', 'bus'] <p>\n",
        "The probability of this sentence according to our Language model is the product of all the conditional probabilities of all the words based on their previous words. $P = P('I')*P('came'|'I')*P('by'|'came','I')*P('bus'|'by','came','I')$"
      ],
      "metadata": {
        "id": "FvqdTxKg2iTK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An N-gram is a sequence of n consecutive words <p>\n",
        "unigrams = 'I', 'came', 'by', 'bus' <p>\n",
        "bigrams = 'I came', 'came by', 'by bus'  <p>\n",
        "trigrams = 'I came by', 'came by bus'  <p>\n",
        "4-grams = 'I came by bus'  <p>\n",
        "The assumption of an n-gram language model is that the next word depend only on the previous $n-1$ words <p>\n",
        "So given a $t$ term text sequence, the probability of the next word is defined as:<p>\n",
        "$P(x_{t+1}|x_t, x_{t-1},...,x_2,x_1) = P(x(t+1)|xt, x(t-1),...,x(t-n+2)$ <p>\n",
        "So for a unigram LM, next word doesn’t depend on any of the previous words, or each word is independent."
      ],
      "metadata": {
        "id": "pQvyVlje-t3t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we use a 4-gram LM to generate text, the text starts with \"If you resort to making fun of someone’s appearance, you lost the ...\", only the last (4–1)=3 words will affect the next word, i.e. 'you lost the'. <p>\n",
        "The probability of the next upcoming word is $P(w_{t+1}|\"you\\ lost\\ the\") = \\frac{P(\"you\\ lost\\ the\\ w_{t+1}\")} {P(\"you\\ lost\\ the\")}$ <p>\n",
        "If in the corpuse, ‘you lost the’ → occurred 10000 times, and ‘you lost the game’ → occurred 2000 times, then <p>\n",
        "$P(\"game\" |\"you\\ lost\\ the\") = \\frac{P(\"you\\ lost\\ the\\ game\")} {P(\"you\\ lost\\ the\")}=\\frac{2000}{10000}=0.20$"
      ],
      "metadata": {
        "id": "mlf1MfqqABSL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us try the text generation with the help of the Brown Corpus. We start with a 4-gram LM. If the 4-gram LM is having a sparsity problem in predicting the next word, back off to trigram LM. If the same problem occurs to trigram, back off to bigram and if it occurs to bigram backoff to unigram. Since unigram doesn’t depend on the previous words randomly choose a word from the word corpus."
      ],
      "metadata": {
        "id": "MSIQIvDdDCeW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_m-i3M6QyWM7",
        "outputId": "9e832637-fb63-4b80-98c5-8fd0d31f9d0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        }
      ],
      "source": [
        "# import the corpus\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown\n",
        "words = list(brown.words())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mJkutMoETpi",
        "outputId": "872d61fe-fa8e-4002-89a9-9a7981e85a68"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1161192"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us generate the next 10 words or tokens for this sentence <p>\n",
        "I am planning to ......"
      ],
      "metadata": {
        "id": "pV1l8dY_Dzym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_sentence = 'I am planning to'"
      ],
      "metadata": {
        "id": "K322MQpfzHYR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define the N-gram LM class\n",
        "\n",
        "class NGrams:\n",
        "\n",
        "  def __init__(self, words, sentence):\n",
        "    self.words = words\n",
        "    self.sentence = sentence\n",
        "    self.tokens = sentence.split()\n",
        "\n",
        "  def get_tokens(self):\n",
        "    return self.tokens\n",
        "\n",
        "  def add_tokens(self,value):\n",
        "    temp = self.tokens\n",
        "    temp.append(value)\n",
        "    self.tokens = temp\n",
        "    return self.tokens\n",
        "\n",
        "  def unigram_model(self):\n",
        "    self.next_words = np.random.choice(words, size=3)\n",
        "    return self.next_words\n",
        "\n",
        "  def bigram_model(self):\n",
        "    next_words = []\n",
        "    for i in range(len(words)-1):\n",
        "      if words[i] == self.tokens[-1]:\n",
        "        next_words.append(words[i+1])\n",
        "    self.next_words = next_words\n",
        "    return self.next_words\n",
        "\n",
        "  def trigram_model(self):\n",
        "    next_words = []\n",
        "    for i in range(len(words)-2):\n",
        "      if words[i] == self.tokens[-2]:\n",
        "        if words[i+1] == self.tokens[-1]:\n",
        "          next_words.append(words[i+2])\n",
        "    self.next_words = next_words\n",
        "    return self.next_words\n",
        "\n",
        "  def fourgram_model(self):\n",
        "    next_words = []\n",
        "    for i in range(len(words)-3):\n",
        "      if words[i] == self.tokens[-3]:\n",
        "        if words[i+1] == self.tokens[-2]:\n",
        "          if words[i+2] == self.tokens[-1]:\n",
        "            next_words.append(words[i+3])\n",
        "    self.next_words = next_words\n",
        "    return self.next_words\n",
        "\n",
        "  def get_top_3_next_words(self,next_words):\n",
        "    next_words_dict = dict()\n",
        "    for word in next_words:\n",
        "      if not word in next_words_dict.keys():\n",
        "        next_words_dict[word] = 1\n",
        "      else:\n",
        "        next_words_dict[word] += 1\n",
        "      \n",
        "    for i,j in next_words_dict.items():\n",
        "      next_words_dict[i] = np.round(j/len(next_words),2)\n",
        "\n",
        "    return sorted(next_words_dict.items(), key = lambda k:(k[1], k[0]), reverse=True)[:3]\n",
        "\n",
        "  def model_selection(self):\n",
        "    if len(self.fourgram_model()) > 0:\n",
        "      next_words = self.fourgram_model()\n",
        "      top_words = self.get_top_3_next_words(next_words)\n",
        "      print(\"fourgram-model\")\n",
        "      return top_words\n",
        "    elif len(self.trigram_model()) > 0:\n",
        "      next_words = self.trigram_model()\n",
        "      top_words = self.get_top_3_next_words(next_words)\n",
        "      print(\"trigram-model\")\n",
        "      return top_words\n",
        "    elif len(self.bigram_model()) > 0:\n",
        "      next_words = self.bigram_model()\n",
        "      top_words = self.get_top_3_next_words(next_words)\n",
        "      print(\"bigram-model\")\n",
        "      return top_words\n",
        "    else:\n",
        "      top_words = self.unigram_model()\n",
        "      print(\"unigram-model\")\n",
        "      return top_words\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "OwU3nqvBzHcZ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NGrams(words=words, sentence=start_sentence)"
      ],
      "metadata": {
        "id": "UWlLDBl4PR65"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(10):\n",
        "  values = model.model_selection()\n",
        "  print(values)\n",
        "  value = input()\n",
        "  model.add_tokens(value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5UYdrsAPcqC",
        "outputId": "e2a9624a-a54b-4416-ae01-d113ba07ecaa"
      },
      "execution_count": 16,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fourgram-model\n",
            "[('price', 1.0)]\n",
            "price\n",
            "fourgram-model\n",
            "[('for', 1.0)]\n",
            "for\n",
            "fourgram-model\n",
            "[('the', 0.6), ('prime', 0.2), ('common', 0.2)]\n",
            "the\n",
            "fourgram-model\n",
            "[('month', 0.5), ('oil', 0.25), (\"Indians'\", 0.25)]\n",
            "month\n",
            "fourgram-model\n",
            "[('in', 1.0)]\n",
            "in\n",
            "fourgram-model\n",
            "[('which', 1.0)]\n",
            "which\n",
            "fourgram-model\n",
            "[('the', 1.0)]\n",
            "the\n",
            "fourgram-model\n",
            "[('sale', 0.03), ('outcome', 0.03), ('gospel', 0.03)]\n",
            "sale\n",
            "fourgram-model\n",
            "[('occurred', 1.0)]\n",
            "occurred\n",
            "fourgram-model\n",
            "[('as', 1.0)]\n",
            "as\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.get_tokens())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33H51KecQZRH",
        "outputId": "3f4e0055-a48a-4431-9624-6ecd4fc6d8a8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['I', 'am', 'planning', 'to', 'use', 'the', 'U.S.', 'mails', 'to', 'defraud', 'as', 'long', 'as', 'the', 'market', 'price', 'for', 'the', 'month', 'in', 'which', 'the', 'sale', 'occurred', 'as']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# join the tokens for a complete text\n",
        "print(\" \".join(model.get_tokens()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBrPHXEhQbh4",
        "outputId": "f7613925-847b-40bb-e02a-bf221a75b573"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I am planning to use the U.S. mails to defraud as long as the market price for the month in which the sale occurred as\n"
          ]
        }
      ]
    }
  ]
}