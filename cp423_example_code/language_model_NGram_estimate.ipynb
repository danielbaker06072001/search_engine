{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "language_model_NGram_estimate.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This exercise notebook demostrates how to compute the query term likelihood given a unigram, or a bigram, or a trigram language model"
      ],
      "metadata": {
        "id": "a7lGpxO-tuaV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pktKmr_ymFFJ",
        "outputId": "84a1b965-f283-465e-e872-38eedfd0695c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Package cmudict is already up-to-date!\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Package genesis is already up-to-date!\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Package inaugural is already up-to-date!\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Package names is already up-to-date!\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Package treebank is already up-to-date!\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw is already up-to-date!\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Package words is already up-to-date!\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('popular')\n",
        "nltk.download('brown')\n",
        "from nltk.corpus import brown"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = list(brown.words())"
      ],
      "metadata": {
        "id": "S8orOwrZmVOP"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdN7-hwTmk-Q",
        "outputId": "33ba6e5e-a41e-4201-9379-d951acd54585"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1161192"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_dict = dict()\n",
        "for word in words:\n",
        "    if word not in words_dict.keys():\n",
        "        words_dict[word] = 1\n",
        "    else:\n",
        "        words_dict[word] +=1"
      ],
      "metadata": {
        "id": "cirKOH_OpSas"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# buil the list with the words occuring only once\n",
        "\n",
        "occured_once = [i for i,j in words_dict.items() if j ==1]"
      ],
      "metadata": {
        "id": "ImjcptQupJ4Z"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this iteration will take some time, be patient waiting for 20 minutes\n",
        "\n",
        "count = 0\n",
        "for num, word in enumerate(words):\n",
        "    if word in occured_once:\n",
        "        words[num] = \"OOV\""
      ],
      "metadata": {
        "id": "LMoHZWirmVeV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"I have never given it much thought\""
      ],
      "metadata": {
        "id": "SIA1TFTAproI"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = sentence.split()\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "beNV8Uzlpx1v",
        "outputId": "5e13fcf3-d008-41f7-9b4e-cecbd781fee7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['I', 'have', 'never', 'given', 'it', 'much', 'thought']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for num,token in enumerate(tokens):\n",
        "    if not token in words:\n",
        "        tokens[num] = \"OOV\""
      ],
      "metadata": {
        "id": "kzMQ3RHDpx79"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert all tokens to lowercase\n",
        "\n",
        "tokens = [token.lower() for token in tokens]\n",
        "words = [word.lower() for word in words]"
      ],
      "metadata": {
        "id": "tOuRGF2Zp6N8"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bigram_probability(bigram,words):\n",
        "    count = [0,0]\n",
        "    for i in range(len(words)-1):\n",
        "        if (words[i] == bigram[0]):\n",
        "            count[1] += 1\n",
        "            if (words[i+1] == bigram[1]):\n",
        "                count[0] += 1\n",
        "    return count[0]/count[1], count"
      ],
      "metadata": {
        "id": "8GItZUXtp6Ta"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigrams = []\n",
        "for i in range(len(tokens)-1):\n",
        "    bigrams.append((tokens[i],tokens[i+1]))"
      ],
      "metadata": {
        "id": "wWNsh7Q7shXS"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for bigram in bigrams:\n",
        "    print(bigram_probability(bigram,words)[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iq8n38SZsjc1",
        "outputId": "bfd5de2c-8897-4995-eca7-ca3af9a86ed4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[259, 5164]\n",
            "[24, 3942]\n",
            "[2, 697]\n",
            "[7, 377]\n",
            "[2, 8760]\n",
            "[2, 937]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bigrams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQxT35z2sm6A",
        "outputId": "47f4f3ed-1f07-4a9f-e6be-44181ef2e953"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('i', 'have'),\n",
              " ('have', 'never'),\n",
              " ('never', 'given'),\n",
              " ('given', 'it'),\n",
              " ('it', 'much'),\n",
              " ('much', 'thought')]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def bigram_prob_sentence(tokens, bigrams):\n",
        "    prob = []\n",
        "    for bigram in bigrams:\n",
        "        p = bigram_probability(bigram,words)[0]\n",
        "        prob.append(p)\n",
        "    return np.prod(prob)"
      ],
      "metadata": {
        "id": "fur1mGPosogo"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_prob_sentence(tokens, bigrams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5HSrMvqsqlh",
        "outputId": "d8eb7650-5411-4251-86fd-95f9b63d8688"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.928268578305691e-15"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trigrams = []\n",
        "for i in range(len(tokens)-2):\n",
        "    trigrams.append((tokens[i],tokens[i+1], tokens[i+2]))"
      ],
      "metadata": {
        "id": "kzZCTg30ss0s"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trigrams"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4O72gIevsu_y",
        "outputId": "7e9ee31f-ac2d-4f7a-dfb8-d28ba1610051"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('i', 'have', 'never'),\n",
              " ('have', 'never', 'given'),\n",
              " ('never', 'given', 'it'),\n",
              " ('given', 'it', 'much'),\n",
              " ('it', 'much', 'thought')]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def trigram_probability(trigram,words):\n",
        "    count = [0,0]\n",
        "    for i in range(len(words)-2):\n",
        "        if (words[i] == trigram[0]) and (words[i+1] == trigram[1]):\n",
        "            count[1] += 1\n",
        "            if (words[i+2] == trigram[2]):\n",
        "                count[0] += 1\n",
        "    return count[0]/count[1], count"
      ],
      "metadata": {
        "id": "iXf73ObOsxE0"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for trigram in trigrams:\n",
        "    print(trigram_probability(trigram,words)[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFObDnpHsy2O",
        "outputId": "aa31f30e-b4ed-4418-f3b8-718e6a9dabdb"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8, 259]\n",
            "[0, 24]\n",
            "[0, 2]\n",
            "[1, 7]\n",
            "[1, 2]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def trigram_prob_sentence(tokens, trigrams):\n",
        "    prob = []\n",
        "    for trigram in trigrams:\n",
        "        p = trigram_probability(trigram,words)[0]\n",
        "        prob.append(p)\n",
        "    return np.prod(prob)"
      ],
      "metadata": {
        "id": "E5rAqny9sy9N"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trigram_prob_sentence(tokens, trigrams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1uhMfYVs4CD",
        "outputId": "c05a0ca0-9832-4a5d-a613-2d7cd6f90820"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unigrams = []\n",
        "for i in range(len(tokens)):\n",
        "    unigrams.append(tokens[i])"
      ],
      "metadata": {
        "id": "j66g2TUIs68w"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for unigram in unigrams:\n",
        "    print(unigram)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TgUDMds6s9Qb",
        "outputId": "23eaa1b8-03c6-4841-a410-609821ae6089"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i\n",
            "have\n",
            "never\n",
            "given\n",
            "it\n",
            "much\n",
            "thought\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def unigram_probability(unigram,words):\n",
        "    count = [0,0]\n",
        "    for i in range(len(words)):\n",
        "        count[1] += 1\n",
        "        if (words[i] == unigram[0]):\n",
        "            count[0] += 1\n",
        "    return count[0]/count[1], count"
      ],
      "metadata": {
        "id": "vl7MNypgs9Zv"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for unigram in unigrams:\n",
        "    print(unigram_probability(unigram,words)[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H975O8UQtB2_",
        "outputId": "3e5f29b4-0800-4f00-d881-2f7b9998a02d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5164, 1161192]\n",
            "[23, 1161192]\n",
            "[38, 1161192]\n",
            "[16, 1161192]\n",
            "[5164, 1161192]\n",
            "[16, 1161192]\n",
            "[77, 1161192]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def unigram_prob_sentence(tokens, unigrams):\n",
        "    prob = []\n",
        "    for unigram in unigrams:\n",
        "        p = unigram_probability(unigram,words)[0]\n",
        "        prob.append(p)\n",
        "    return np.prod(prob)"
      ],
      "metadata": {
        "id": "P0_aQeLWthXV"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unigram_prob_sentence(tokens, unigrams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFmA4_n-thcG",
        "outputId": "0e124c19-60be-46c4-d7de-989e07a49978"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.6139361322466987e-28"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    }
  ]
}