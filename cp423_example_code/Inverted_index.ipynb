{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Inverted_index.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF <p>\n",
        "TF-IDF, or Term Frequency â€” Inverse Document Frequency, is  is used to measure the importance of a word in data. It is particularly useful for scoring the words in text related computations, such as text analysis and Natural Language Processing (NLP) algorithms. <p>\n",
        "The fomula for TF-IDF: \n",
        "\\begin{align}\n",
        "        \\mathbf{tf(t,d)} &= \\frac{f_d(t)}{max f_d(w)} \\\\\n",
        "        \\mathbf{idf(t,D)} &= \\ln(\\frac{\\left|D \\right|}{\\left|\\{d\\in D : t\\in d \\} \\right|})\\\\\n",
        "        \\mathbf{tfidf(t,d,D)} &= tf(t,d) \\cdot idf(t,D)\n",
        "    \\end{align}\n",
        "where $f_d(t)$ is the frequency of term $t$ in document $d$, $D$ is the document set and $\\left| D \\right|$ is the total number of documents in $D$."
      ],
      "metadata": {
        "id": "wFyRnOIO3s2x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The formula shows that TF (or term frequency) calculates how many times a term appears in each document. However, IDF (or inverse document frequency) diminishes the term's weight if it appears in other documents. Therefore, a term is not particularly important for the specific document, as it has also appeared commonly in other documents as well. <p>\n",
        "By evaluating TF-IDF, we can find how frequent the terms used in a sentence versus how common the terms used all through the document set. "
      ],
      "metadata": {
        "id": "TgPFswX95iXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "from nltk import sent_tokenize, word_tokenize, PorterStemmer\n",
        "from nltk.corpus import stopwords    \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "9I_KYSj6EkoG"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set up the document data set."
      ],
      "metadata": {
        "id": "L2XkrKBfJ_wh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_1 = \"It is going to rain today.\"\n",
        "doc_2 = \"Today I am not going outside.\"\n",
        "doc_3 = \"I am going to watch the season premiere.\"\n",
        "\n",
        "documents = [doc_1, doc_2, doc_3]\n",
        "print(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiYH2ObRJwUq",
        "outputId": "eb06a6c2-1344-400e-a2c3-8036d546a41b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['It is going to rain today.', 'Today I am not going outside.', 'I am going to watch the season premiere.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "initiate the TF-IDF vectorizer from sklearn"
      ],
      "metadata": {
        "id": "HGpJ7NN5j5Rs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TfidfVectorizer()\n",
        "analyze = tfidf.build_analyzer()"
      ],
      "metadata": {
        "id": "EoYM9J9PAYnQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in documents:\n",
        "  print('doc: ', analyze(doc))\n",
        "\n",
        "# it shows that the TfidfVectorizer can tokenize the sentence in to single words\n",
        "# if you want to do further processing, such as stem the terms, removing stop words, you need to do it mannually"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VohfXAHW2Ba",
        "outputId": "148e2de4-c805-4be4-b3fa-dba51e4aee65"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "doc:  ['it', 'is', 'going', 'to', 'rain', 'today']\n",
            "doc:  ['today', 'am', 'not', 'going', 'outside']\n",
            "doc:  ['am', 'going', 'to', 'watch', 'the', 'season', 'premiere']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf = TfidfVectorizer(min_df=1)\n",
        "tfidf.fit(documents)\n",
        "feature_names = tfidf.get_feature_names_out()"
      ],
      "metadata": {
        "id": "mpoykqxKagI9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the terms as key of the dictionary\n",
        "feature_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_6YKrYzdh4p",
        "outputId": "515b819b-8444-44cc-e463-45006c87b09a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['am', 'going', 'is', 'it', 'not', 'outside', 'premiere', 'rain',\n",
              "       'season', 'the', 'to', 'today', 'watch'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the TF-IDF scores from the document set"
      ],
      "metadata": {
        "id": "CofAtOeEkVjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build the TF-IDF score matrix \n",
        "tfidf_mat = tfidf.transform(documents).todense()"
      ],
      "metadata": {
        "id": "VzxFHi9zg0zD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the score indices of the second document (doc_2) from the TF-IDF score matrix\n",
        "feat_idx = tfidf_mat[1:2].nonzero()[1]\n",
        "print(feat_idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKCZI33Sg924",
        "outputId": "820365c8-abb8-49ca-8e25-20e9003a36c2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0  1  4  5 11]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_scores = zip([feature_names[i] for i in feat_idx], [tfidf_mat[0,x] for x in feat_idx])"
      ],
      "metadata": {
        "id": "WyV4ULQPhOBo"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_dict = dict(tfidf_scores)"
      ],
      "metadata": {
        "id": "YtygUWuqhqj4"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFxMwGZtgpCr",
        "outputId": "df9e5313-8a7a-4889-ed19-9ca43bc2c861"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'am': 0.0,\n",
              " 'going': 0.2782452148327134,\n",
              " 'not': 0.0,\n",
              " 'outside': 0.0,\n",
              " 'today': 0.35829137488557944}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise: get the TF-IDF score for the 3rd document (doc_3)"
      ],
      "metadata": {
        "id": "BjPbHJI4uUGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# write your code here\n",
        "feat_idx = \n",
        "tfidf_scores = \n",
        "tfidf_dict_row3 = \n",
        "\n",
        "# end your code here\n",
        "print(tfidf_dict_row3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "ndS4MZT-ucKp",
        "outputId": "7d174c68-ab47-4da4-fada-81e2a273147d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-696d5ac9a577>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    feat_idx =\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The expected output should look like: <p>\n",
        "{'am': 0.0, 'going': 0.2782452148327134, 'premiere': 0.0, 'season': 0.0, 'the': 0.0, 'to': 0.35829137488557944, 'watch': 0.0}"
      ],
      "metadata": {
        "id": "VjdfU1HNu-1u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compute TF-IDF mannual with NLTK <p>\n",
        "computing TF-IDF mannual actually give you more freedom to handle the terms"
      ],
      "metadata": {
        "id": "t9oyOx5tvIW7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import nltk\n",
        "nltk.download('popular')\n",
        "# from nltk import word_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "import numpy as np\n",
        "import re"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JxONyumovHTj",
        "outputId": "eaf54f01-e19e-4a44-9736-6ae70419db4b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading collection 'popular'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw-1.4.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2021.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet31.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection popular\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# put the documents in a dictionary: {\"docID\": text}\n",
        "doc_dict = dict()\n",
        "for i in range(len(documents)):\n",
        "  doc_dict[i] = documents[i]"
      ],
      "metadata": {
        "id": "jBdO1xsgz1Ax"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbfk0KUZ0Jst",
        "outputId": "4d8a3e63-8a49-4ddd-e730-7d63711db938"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 'It is going to rain today.',\n",
              " 1: 'Today I am not going outside.',\n",
              " 2: 'I am going to watch the season premiere.'}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(doc_dict):\n",
        "  \"\"\"\n",
        "  clean extra white space, remove stop words, lower case\n",
        "  input - a dictionary of {filename : text}\n",
        "  output - a dictionary of {filename : clean text} \n",
        "\n",
        "  \"\"\"\n",
        "  clean_dict = {}\n",
        "  # stemmer = PorterStemmer()\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  stopwords_english = stopwords.words('english')\n",
        "  \n",
        "  for idx, doc in doc_dict.items():\n",
        "    # remove extra white space\n",
        "    text = re.sub(r\"\\s+\", \" \", doc)\n",
        "    # remove extra ...\n",
        "    text = re.sub(r\"\\.+\",\" \", doc)\n",
        "    # remove hyphen\n",
        "    text = re.sub(r\"-\",\"\", text)\n",
        "    # remove numbers\n",
        "    text = re.sub(r\"[~^0-9]\", \"\", text)\n",
        "    text = text.lower()\n",
        "    text_tokens = word_tokenize(text)\n",
        "    text_clean = []\n",
        "    for word in text_tokens:\n",
        "      if (word not in stopwords_english and word not in string.punctuation):\n",
        "        # stem_word = stemmer.stem(word)\n",
        "        lemmatize_word = lemmatizer.lemmatize(word=word, pos='v') # convert verbs to basic form\n",
        "        # text_clean.append(stem_word)\n",
        "        text_clean.append(lemmatize_word)\n",
        "    \n",
        "    clean_dict[idx] = text_clean\n",
        "    \n",
        "  return clean_dict"
      ],
      "metadata": {
        "id": "to7bwcsYvE-B"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_dict = clean_text(doc_dict)"
      ],
      "metadata": {
        "id": "GHmSmoDw1Gii"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YUEXXKld1H3c",
        "outputId": "3d157511-a4e2-41ca-d5d8-c9547227a084"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: ['go', 'rain', 'today'],\n",
              " 1: ['today', 'go', 'outside'],\n",
              " 2: ['go', 'watch', 'season', 'premiere']}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make the vocabulary of the dataset"
      ],
      "metadata": {
        "id": "ysk1RD8LBgz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_vocab(doc_dict):\n",
        "  \"\"\"\n",
        "  input - a dictionary of {filename : clean text} \n",
        "  output - a set of unique terms forms the dataset vocabulary\n",
        "  \"\"\"\n",
        "  total_tokens = []\n",
        "  for tokens in doc_dict.values():\n",
        "    total_tokens += tokens\n",
        "  # remove the duplicates\n",
        "  vocab = list(set(total_tokens))\n",
        "  return vocab"
      ],
      "metadata": {
        "id": "8bcP0pTxBmNd"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = make_vocab(clean_dict)\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQRYcRgOBxFk",
        "outputId": "7d93cc85-515b-4156-c394-ee5b0afc91ee"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['season', 'today', 'rain', 'premiere', 'outside', 'watch', 'go']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate TF"
      ],
      "metadata": {
        "id": "cNM9zSdWCFsR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_TF(doc_dict, vocab):\n",
        "  \"\"\"\n",
        "  input - a dictionary of {filename : clean text}, the vocabulary of the whole dataset\n",
        "  output - a dictionary of {filename : {term : count}}\n",
        "  \"\"\"\n",
        "  tf_dict = {}\n",
        "  # make the dict for filename=>{term:frequency}\n",
        "  for doc_id in doc_dict.keys():\n",
        "    tf_dict[doc_id] = {}\n",
        "\n",
        "  for word in vocab:\n",
        "    for doc_id, text in doc_dict.items():\n",
        "      term_count = text.count(word)\n",
        "      doc_length = len(text)\n",
        "      tf_dict[doc_id][word] = term_count / doc_length\n",
        "      # whether we should keep all the terms whose TF=0?\n",
        "    \n",
        "  return tf_dict"
      ],
      "metadata": {
        "id": "PoQ5w_TABxf3"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_dict = get_TF(clean_dict, vocab)\n",
        "for _, count in tf_dict.items():\n",
        "  print(count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Lbj0qOfCYYU",
        "outputId": "c30e8d95-bcba-43f0-c866-263e6cebc1f7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'season': 0.0, 'today': 0.3333333333333333, 'rain': 0.3333333333333333, 'premiere': 0.0, 'outside': 0.0, 'watch': 0.0, 'go': 0.3333333333333333}\n",
            "{'season': 0.0, 'today': 0.3333333333333333, 'rain': 0.0, 'premiere': 0.0, 'outside': 0.3333333333333333, 'watch': 0.0, 'go': 0.3333333333333333}\n",
            "{'season': 0.25, 'today': 0.0, 'rain': 0.0, 'premiere': 0.25, 'outside': 0.0, 'watch': 0.25, 'go': 0.25}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate IDF"
      ],
      "metadata": {
        "id": "olraScFED0um"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_idf(clean_dict, vocab):\n",
        "  idf_dict = {}\n",
        "  D_length = len(clean_dict.keys())\n",
        "\n",
        "  for doc_id in clean_dict.keys():\n",
        "    idf_dict[doc_id] = {}\n",
        "  \n",
        "  for word in vocab:\n",
        "    term_count = 0\n",
        "    for doc_id, text in doc_dict.items():\n",
        "      if text.count(word) > 0:\n",
        "        term_count += 1\n",
        "        idf_dict[doc_id][word] = np.log(D_length / term_count)\n",
        "      else:\n",
        "        # if the term is not found, set IDF=0\n",
        "        idf_dict[doc_id][word] = 0\n",
        "\n",
        "  return idf_dict"
      ],
      "metadata": {
        "id": "cD_YcApIDPGQ"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idf_dict = get_idf(clean_dict, vocab)"
      ],
      "metadata": {
        "id": "tFfJlCBKQ1Yo"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _, count in idf_dict.items():\n",
        "  print(count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDrWPEEoQK16",
        "outputId": "095af7e5-a80f-4ddd-adc7-65988cce7f01"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'season': 0, 'today': 1.0986122886681098, 'rain': 1.0986122886681098, 'premiere': 0, 'outside': 0, 'watch': 0, 'go': 1.0986122886681098}\n",
            "{'season': 0, 'today': 0, 'rain': 0, 'premiere': 0, 'outside': 1.0986122886681098, 'watch': 0, 'go': 0.4054651081081644}\n",
            "{'season': 1.0986122886681098, 'today': 0, 'rain': 0, 'premiere': 1.0986122886681098, 'outside': 0, 'watch': 1.0986122886681098, 'go': 0.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate TF-IDF"
      ],
      "metadata": {
        "id": "fN-z0JuOaboI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tf_idf(tf_dict, idf_dict, doc_dict, vocab):\n",
        "  tf_idf_dict = {}\n",
        "  for doc_id in doc_dict.keys():\n",
        "    tf_idf_dict[doc_id] = {}\n",
        "  \n",
        "  for word in vocab:\n",
        "    for doc_id, text_tokens in doc_dict.items():\n",
        "      tf_idf_dict[doc_id][word] = round((tf_dict[doc_id][word] * idf_dict[doc_id][word]), 4)\n",
        "  return tf_idf_dict"
      ],
      "metadata": {
        "id": "HY2XsHB7aa19"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_dict = get_tf_idf(tf_dict, idf_dict, clean_dict, vocab)"
      ],
      "metadata": {
        "id": "E6ZHjn4Za533"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for _, count in tfidf_dict.items():\n",
        "  print(count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeZN56wObCCL",
        "outputId": "ea168927-f456-4019-a7f7-eb3b7da35f8c"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'season': 0.0, 'today': 0.3662, 'rain': 0.3662, 'premiere': 0.0, 'outside': 0.0, 'watch': 0.0, 'go': 0.3662}\n",
            "{'season': 0.0, 'today': 0.0, 'rain': 0.0, 'premiere': 0.0, 'outside': 0.3662, 'watch': 0.0, 'go': 0.1352}\n",
            "{'season': 0.2747, 'today': 0.0, 'rain': 0.0, 'premiere': 0.2747, 'outside': 0.0, 'watch': 0.2747, 'go': 0.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inverted Index\n",
        "Inverted index is a data structure for full text search over a set of documents. It uses a big table where each entry corresponses a word found in the available documents. The word indices are paired along with a list of the key pairs: document id, frequency of the term in the document."
      ],
      "metadata": {
        "id": "dCbv1a8NbL73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Appearance:\n",
        "    \"\"\"\n",
        "    Represents the appearance of a term in a given document, along with the\n",
        "    frequency of appearances in the same one.\n",
        "    output - a dictionary with two key:value pairs: {'docId': #, 'frequency': #}\n",
        "    \"\"\"\n",
        "    def __init__(self, docId, frequency):\n",
        "        self.docId = docId\n",
        "        self.frequency = frequency\n",
        "        \n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        String representation of the Appearance object\n",
        "        \"\"\"\n",
        "        return str(self.__dict__)"
      ],
      "metadata": {
        "id": "tHPxFGEObczQ"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Database:\n",
        "    \"\"\"\n",
        "    In memory database representing the already indexed documents.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.db = dict()\n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        String representation of the Database object\n",
        "        \"\"\"\n",
        "        return str(self.__dict__)\n",
        "    \n",
        "    def get(self, id):\n",
        "        return self.db.get(id, None)\n",
        "    \n",
        "    def add(self, document):\n",
        "        \"\"\"\n",
        "        Adds a document to the DB.\n",
        "        \"\"\"\n",
        "        return self.db.update({document['id']: document})\n",
        "    def remove(self, document):\n",
        "        \"\"\"\n",
        "        Removes document from DB.\n",
        "        \"\"\"\n",
        "        return self.db.pop(document['id'], None)"
      ],
      "metadata": {
        "id": "zNGMs-LXbnRD"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InvertedIndex:\n",
        "    \"\"\"\n",
        "    Inverted Index class.\n",
        "    \"\"\"\n",
        "    def __init__(self, db):\n",
        "        self.index = dict()\n",
        "        self.db = db\n",
        "    def __repr__(self):\n",
        "        \"\"\"\n",
        "        String representation of the Database object\n",
        "        \"\"\"\n",
        "        return str(self.index)\n",
        "        \n",
        "    def index_document(self, document):\n",
        "        \"\"\"\n",
        "        Process a given document, save it to the DB and update the index.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Remove punctuation from the text.\n",
        "        clean_text = re.sub(r'[^\\w\\s]','', document['text'])\n",
        "        terms = clean_text.split(' ')\n",
        "        appearances_dict = dict()\n",
        "        # Dictionary with each term and the frequency it appears in the text.\n",
        "        for term in terms:\n",
        "            term_frequency = appearances_dict[term].frequency if term in appearances_dict else 0\n",
        "            appearances_dict[term] = Appearance(document['id'], term_frequency + 1)\n",
        "            \n",
        "        # Update the inverted index\n",
        "        update_dict = { key: [appearance]\n",
        "                       if key not in self.index\n",
        "                       else self.index[key] + [appearance]\n",
        "                       for (key, appearance) in appearances_dict.items() }\n",
        "        self.index.update(update_dict)\n",
        "        # Add the document into the database\n",
        "        self.db.add(document)\n",
        "        return document\n",
        "    \n",
        "    def lookup_query(self, query):\n",
        "        \"\"\"\n",
        "        Returns the dictionary of terms with their correspondent Appearances. \n",
        "        This is a very naive search since it will just split the terms and show\n",
        "        the documents where they appear.\n",
        "        \"\"\"\n",
        "        return { term: self.index[term] for term in query.split(' ') if term in self.index }"
      ],
      "metadata": {
        "id": "sx-vw5Qmb9LH"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def highlight_term(id, term, text):\n",
        "    replaced_text = text.replace(term, \"\\033[1;32;40m {term} \\033[0;0m\".format(term=term))\n",
        "    return \"--- document {id}: {replaced}\".format(id=id, replaced=replaced_text)"
      ],
      "metadata": {
        "id": "zBW42VG-c16S"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db = Database()\n",
        "index = InvertedIndex(db)\n",
        "document1 = {'id': 1, 'text': 'The big sharks of Belgium drink beer.'}\n",
        "document2 = {'id': 2,'text': 'Belgium has great beer. They drink beer all the time.'}\n",
        "document3 = {'id': 3, 'text': 'The beer always tastes great.'}\n",
        "index.index_document(document1)\n",
        "index.index_document(document2)\n",
        "index.index_document(document3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpcrNB8tc86f",
        "outputId": "0d952669-8d27-40b9-9843-e5ca17b0da0a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 3, 'text': 'The beer always tastes great.'}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search_term = \"Belgium sharks\"\n",
        "result = index.lookup_query(search_term)"
      ],
      "metadata": {
        "id": "pGibacpEeDoq"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U5xAC9EseEeQ",
        "outputId": "cf5ad8c1-9f1b-4bbd-90ac-8978162fd060"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Belgium': [{'docId': 1, 'frequency': 1}, {'docId': 2, 'frequency': 1}],\n",
              " 'sharks': [{'docId': 1, 'frequency': 1}]}"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "match_text = {}\n",
        "for term in result.keys():\n",
        "  for appearance in result[term]:\n",
        "    document = db.get(appearance.docId)\n",
        "    if appearance.docId in match_text.keys():\n",
        "      match_text[appearance.docId] = highlight_term(appearance.docId, term, match_text[appearance.docId])\n",
        "    else:\n",
        "      match_text[appearance.docId] = highlight_term(appearance.docId, term, document['text'])"
      ],
      "metadata": {
        "id": "8LL5J66BeLfs"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for text in match_text.values():\n",
        "  print(text)\n",
        "  print(\"-----------------------------\")  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gv_NlvPZl8tf",
        "outputId": "f760a4f5-f0a0-418f-9c6b-bb2c580b5aed"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- document 1: --- document 1: The big \u001b[1;32;40m sharks \u001b[0;0m of \u001b[1;32;40m Belgium \u001b[0;0m drink beer.\n",
            "-----------------------------\n",
            "--- document 2: \u001b[1;32;40m Belgium \u001b[0;0m has great beer. They drink beer all the time.\n",
            "-----------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 1\n",
        "Build a TF-IDF table for a document set"
      ],
      "metadata": {
        "id": "3x_-E0jKqUdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy the dataset to the VM file system.\n",
        "!wget https://storage.googleapis.com/pet-detect-239118/text_retrieval/documents.zip documents.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wp7qDMhyqcVi",
        "outputId": "cd3cc5a6-e3e9-4ec6-b511-bc9fa0a7ff9a"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-24 01:18:34--  https://storage.googleapis.com/pet-detect-239118/text_retrieval/documents.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.65.80, 142.250.188.208, 142.251.33.208, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.65.80|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 226023 (221K) [application/x-zip-compressed]\n",
            "Saving to: â€˜documents.zipâ€™\n",
            "\n",
            "documents.zip       100%[===================>] 220.73K  --.-KB/s    in 0.03s   \n",
            "\n",
            "2022-01-24 01:18:34 (7.22 MB/s) - â€˜documents.zipâ€™ saved [226023/226023]\n",
            "\n",
            "--2022-01-24 01:18:34--  http://documents.zip/\n",
            "Resolving documents.zip (documents.zip)... failed: Name or service not known.\n",
            "wget: unable to resolve host address â€˜documents.zipâ€™\n",
            "FINISHED --2022-01-24 01:18:34--\n",
            "Total wall clock time: 0.3s\n",
            "Downloaded: 1 files, 221K in 0.03s (7.22 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "file_name = '/content/documents.zip'\n",
        "\n",
        "with ZipFile(file_name, 'r',) as zip:\n",
        "  zip.extractall()\n",
        "  print('Done!!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a28y0ow7qdS8",
        "outputId": "e63d67d2-d9af-4127-eff0-4a604b5871f1"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parse the text document dataset into a dictionary"
      ],
      "metadata": {
        "id": "JCLRZg5AqqIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_docDict(path):\n",
        "  doc_dict = {}\n",
        "  file_names = os.listdir(path)\n",
        "\n",
        "  for file in file_names:\n",
        "    full_path = path+'/'+file\n",
        "    with open(full_path, 'r', errors='ignore') as f:\n",
        "      data = f.readlines()\n",
        "    text = \"\".join([i for i in data])\n",
        "    # remove all the \"\\n\" from the text\n",
        "    text = re.sub(\"\\n\", \" \", text)\n",
        "    doc_dict[file] = text\n",
        "  return doc_dict"
      ],
      "metadata": {
        "id": "CLDwo771qkhk"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/documents'\n",
        "doc_dict = get_docDict(path)\n",
        "doc_dict.keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYMo87j8qxa8",
        "outputId": "fa039af9-1e90-4e96-d7ab-7ffabc4461ed"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['A00-1010.pdf.txt', 'A00-1008.pdf.txt', 'A00-1002.pdf.txt', 'A00-1018.pdf.txt', 'A00-1016.pdf.txt', 'A00-1020.pdf.txt', 'A00-1014.pdf.txt', 'A00-1011.pdf.txt', 'A00-1019.pdf.txt', 'A00-1006.pdf.txt', 'A00-1004.pdf.txt', 'A00-1009.pdf.txt', 'A00-1017.pdf.txt', 'A00-1015.pdf.txt', 'A00-1003.pdf.txt', 'A00-1007.pdf.txt', 'A00-1005.pdf.txt', 'A00-1001.pdf.txt', 'A00-1013.pdf.txt', 'A00-1000.pdf.txt', 'A00-1012.pdf.txt'])"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean the text"
      ],
      "metadata": {
        "id": "itZ33jypvxgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clean_dict = clean_text(doc_dict)"
      ],
      "metadata": {
        "id": "5e-y7cyLq_wG"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_dict['A00-1000.pdf.txt'][:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ewp60nN3rOGy",
        "outputId": "f8eb8e67-bce1-4f15-a49d-1e1c4a765658"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['association', 'computational', 'linguistics', 'th', 'apply']"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make vocabulary"
      ],
      "metadata": {
        "id": "OElVsuibvz9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = make_vocab(clean_dict)\n",
        "len(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6qfcCVfvszW",
        "outputId": "39911d62-8fe1-4f9c-ab50-86b7b3a78b53"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8526"
            ]
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate TF"
      ],
      "metadata": {
        "id": "YyVl3gVIwsud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf_dict = get_TF(clean_dict, vocab)"
      ],
      "metadata": {
        "id": "cfmW_EWdwA6c"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate IDF"
      ],
      "metadata": {
        "id": "DD4bDA8Gwwn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "idf_dict = get_idf(clean_dict, vocab)"
      ],
      "metadata": {
        "id": "gxhCR3s3wWaA"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate TF-IDF"
      ],
      "metadata": {
        "id": "gpaef3Bnw8Zo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_dict = get_tf_idf(tf_dict, idf_dict, clean_dict, vocab)"
      ],
      "metadata": {
        "id": "LHG8JbkJw4tx"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_dict[\"A00-1010.pdf.txt\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8by3xq6Nw_cw",
        "outputId": "16612b15-e900-4d94-8afc-746885074866"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'menlo': 0.0,\n",
              " 'forbid': 0.0,\n",
              " 'voi': 0.0,\n",
              " 'wasteful': 0.0,\n",
              " 'eral': 0.0,\n",
              " 'nantes': 0.0,\n",
              " 'retrieve': 0.0,\n",
              " 'anti': 0.0,\n",
              " 'czechtorussian': 0.0,\n",
              " 'pmin': 0.0,\n",
              " 'highestscoring': 0.0,\n",
              " 'track': 0.0,\n",
              " 'satisfaction': 0.0,\n",
              " 'terest': 0.0,\n",
              " 'park': 0.0,\n",
              " 'investigations': 0.0,\n",
              " 'itinerary': 0.0076,\n",
              " 'canad': 0.0,\n",
              " 'besides': 0.0,\n",
              " 'esources': 0.0,\n",
              " 'rent': 0.0,\n",
              " 'oldness': 0.0,\n",
              " 'operation': 0.0,\n",
              " 'wood': 0.0,\n",
              " 'reader': 0.0,\n",
              " 'personal': 0.0,\n",
              " 'ster': 0.0,\n",
              " 'personphonenumber': 0.0,\n",
              " 'gument': 0.0,\n",
              " 'guistic': 0.0,\n",
              " 'consequtive': 0.0,\n",
              " 'failsoft': 0.0,\n",
              " 'prototyping': 0.0,\n",
              " 'henrik': 0.0,\n",
              " 'thomas': 0.0,\n",
              " 'door': 0.0,\n",
              " 'snizzle': 0.0,\n",
              " 'seize': 0.0,\n",
              " 'ib': 0.0,\n",
              " 'brenenkamp': 0.0,\n",
              " 'monly': 0.0,\n",
              " 'compact': 0.0,\n",
              " 'thanksgiving': 0.0,\n",
              " 'capital': 0.0,\n",
              " 'lauren': 0.0,\n",
              " 'freer': 0.0,\n",
              " 'transcripts': 0.0,\n",
              " 'otevit': 0.0,\n",
              " 'intermodule': 0.0,\n",
              " 'submit': 0.0,\n",
              " 'unseen': 0.0,\n",
              " 'strzalkowski': 0.0,\n",
              " 'rtifacts': 0.0,\n",
              " 'nos': 0.0,\n",
              " 'oevaluate': 0.0,\n",
              " 'thesaurus': 0.0,\n",
              " 'overcome': 0.0013,\n",
              " 'toword': 0.0,\n",
              " 'toolow': 0.0,\n",
              " 'depuis': 0.0,\n",
              " 'bootstrapping': 0.0,\n",
              " 'friends': 0.0,\n",
              " 'ment': 0.0,\n",
              " 'thorsten': 0.0,\n",
              " 'vcn': 0.0,\n",
              " 'arg_syn': 0.0,\n",
              " 'georgila': 0.0,\n",
              " 'lack': 0.0,\n",
              " 'suitability': 0.0,\n",
              " 'yuval': 0.0,\n",
              " 'joint': 0.0,\n",
              " 'dcs': 0.0,\n",
              " 'fellbaum': 0.0,\n",
              " 'theorem': 0.0,\n",
              " 'okey': 0.0,\n",
              " 'tsyac': 0.0,\n",
              " 'deter': 0.0,\n",
              " 'vytviet': 0.0,\n",
              " 'orgnationality': 0.0,\n",
              " 'german': 0.0,\n",
              " 'worst': 0.0,\n",
              " 'domainindep': 0.0,\n",
              " 'tricky': 0.0,\n",
              " 'nigam': 0.0,\n",
              " 'enclose': 0.0,\n",
              " 'c/m': 0.0,\n",
              " 'vo': 0.0,\n",
              " 'obliga': 0.0,\n",
              " 'resu': 0.0,\n",
              " 'sarther': 0.0,\n",
              " 'eurowordnet': 0.0,\n",
              " 'imits': 0.0,\n",
              " 'fundamen': 0.0,\n",
              " 'copenhagen': 0.0,\n",
              " 'coa': 0.0,\n",
              " 'glish/': 0.0,\n",
              " 'argue': 0.0,\n",
              " 'compound': 0.0,\n",
              " 'ipper': 0.0,\n",
              " 'gatt': 0.0,\n",
              " 'phoneme': 0.0,\n",
              " 'idedown': 0.0,\n",
              " 'forty': 0.0,\n",
              " 'lefttoright': 0.0,\n",
              " 'remember': 0.0,\n",
              " 'annoy': 0.0,\n",
              " 'christopher': 0.0,\n",
              " 'subgoals': 0.0013,\n",
              " 'programmable': 0.0,\n",
              " 'third': 0.0,\n",
              " 'fois': 0.0,\n",
              " 'arge': 0.0013,\n",
              " 'nonstandard': 0.0,\n",
              " 'entetion': 0.0,\n",
              " 'hej': 0.0,\n",
              " 'risk': 0.0,\n",
              " 'necessarily': 0.0,\n",
              " 'yped': 0.0,\n",
              " 'zoo': 0.0,\n",
              " 'eft': 0.0,\n",
              " 'premiere': 0.0,\n",
              " \"noun'=same_name\": 0.0,\n",
              " 'tsopanoglou': 0.0,\n",
              " 'respond': 0.0013,\n",
              " 'soredekekkoudesu': 0.0,\n",
              " 'hfvc': 0.0,\n",
              " 'collins': 0.0,\n",
              " 'cognate': 0.0,\n",
              " 'symposium': 0.0,\n",
              " 'length': 0.0,\n",
              " 'successfully': 0.0013,\n",
              " 'i=o': 0.0,\n",
              " 'sean': 0.0,\n",
              " 'se': 0.0,\n",
              " 'speechonly': 0.0,\n",
              " 'determine': 0.0102,\n",
              " 'market': 0.0,\n",
              " 'sistency': 0.0,\n",
              " 'demonstrate': 0.0013,\n",
              " 'dis': 0.0,\n",
              " 'suppliers': 0.0,\n",
              " 'mccallum': 0.0,\n",
              " 'spawar': 0.0,\n",
              " 'ashish': 0.0,\n",
              " 'respondent': 0.0,\n",
              " 'role': 0.0,\n",
              " 'understand': 0.0152,\n",
              " 'possibilities': 0.0013,\n",
              " 'collection': 0.0,\n",
              " 'effi': 0.0,\n",
              " 'npassociations': 0.0,\n",
              " 'ellipsis': 0.0,\n",
              " 'intermingle': 0.0,\n",
              " 'seed': 0.0,\n",
              " 'introspection': 0.0,\n",
              " 'second': 0.0025,\n",
              " 'behavior': 0.0,\n",
              " 'linguis': 0.0,\n",
              " 'siegel': 0.0,\n",
              " 'proper': 0.0,\n",
              " 'takeoff': 0.0,\n",
              " 'norwe': 0.0,\n",
              " 'resources': 0.0,\n",
              " 'ideal': 0.0,\n",
              " 'critique': 0.0,\n",
              " 'kamm': 0.0,\n",
              " 'listyev': 0.0,\n",
              " 'remind': 0.0,\n",
              " 'decide': 0.0,\n",
              " 'vague': 0.0,\n",
              " 'modal': 0.0,\n",
              " 'maaa': 0.0,\n",
              " 'recognition': 0.0063,\n",
              " 'bennacef': 0.0,\n",
              " 'liable': 0.0,\n",
              " 'javoxbased': 0.0,\n",
              " 'marx': 0.0,\n",
              " 'gozaimasu': 0.0,\n",
              " 'teria': 0.0,\n",
              " 'terface': 0.0,\n",
              " 'statements': 0.0,\n",
              " 'automatically': 0.0,\n",
              " 'ior': 0.0,\n",
              " 'manuscript': 0.0013,\n",
              " 'ix': 0.0,\n",
              " 'tractable': 0.0,\n",
              " 'tant': 0.0,\n",
              " 'spanish': 0.0,\n",
              " 'wonder': 0.0,\n",
              " 'correlate': 0.0,\n",
              " 'rigid': 0.0013,\n",
              " 'pacific': 0.0,\n",
              " 'summarization': 0.0,\n",
              " 'ferry': 0.0,\n",
              " 'informatics': 0.0,\n",
              " 'chinoise': 0.0,\n",
              " 'lected': 0.0,\n",
              " 'theofilidis': 0.0,\n",
              " 'featores': 0.0,\n",
              " 'cy': 0.0,\n",
              " 'cohen': 0.0,\n",
              " 'manytomany': 0.0,\n",
              " '/de': 0.0,\n",
              " 'guy': 0.0,\n",
              " 'scalable': 0.0,\n",
              " 'textbased': 0.0,\n",
              " 'l=o': 0.0,\n",
              " 'rich': 0.0,\n",
              " 'aied': 0.0,\n",
              " 'thure': 0.0,\n",
              " 'san': 0.0,\n",
              " 'nonewlnfo\\\\': 0.0,\n",
              " 'colorado': 0.0,\n",
              " 'sound': 0.0,\n",
              " 'antecedent': 0.0,\n",
              " 'reviser': 0.0,\n",
              " 'mixture': 0.0,\n",
              " 'questionnaire': 0.0,\n",
              " 'readworld': 0.0,\n",
              " 'evil': 0.0,\n",
              " 'vat': 0.0,\n",
              " 'ssion': 0.0,\n",
              " 'carry': 0.0038,\n",
              " 'confidence': 0.0,\n",
              " 'polish': 0.0,\n",
              " 'tificial': 0.0,\n",
              " 'pivot': 0.0,\n",
              " 'lative': 0.0,\n",
              " 'generic': 0.0,\n",
              " 'gree': 0.0,\n",
              " 'ocial': 0.0,\n",
              " 'arg_sem=weapon': 0.0,\n",
              " 'istence': 0.0,\n",
              " 'cherny': 0.0,\n",
              " 'suitable': 0.0013,\n",
              " 'obtain': 0.0025,\n",
              " 'signes\\\\': 0.0,\n",
              " 'unes': 0.0,\n",
              " 'congnates': 0.0,\n",
              " 'completely': 0.0,\n",
              " 'zurich': 0.0,\n",
              " 'ings': 0.0,\n",
              " 'satisfiers': 0.0025,\n",
              " 'boldface': 0.0,\n",
              " 'beyond': 0.0013,\n",
              " 'hunter': 0.0,\n",
              " 'mete': 0.0,\n",
              " 'cais': 0.0,\n",
              " 'penalise': 0.0013,\n",
              " 'cruz': 0.0,\n",
              " '//www': 0.0,\n",
              " 'corpora': 0.0,\n",
              " 'ffffff': 0.0,\n",
              " 'patrizia': 0.0,\n",
              " 'lightly': 0.0,\n",
              " 'acknowlegments': 0.0,\n",
              " 'stop': 0.0,\n",
              " 'mature': 0.0,\n",
              " 'online': 0.0,\n",
              " 'service': 0.0,\n",
              " 'lexme': 0.0,\n",
              " 'computationally': 0.0013,\n",
              " 'straightforward': 0.0013,\n",
              " 'railtel': 0.0,\n",
              " 'implementation': 0.0,\n",
              " 'strengthen': 0.0,\n",
              " 'prover': 0.0,\n",
              " 'rances': 0.0,\n",
              " 'inte': 0.0,\n",
              " 'unclear': 0.0,\n",
              " 'kehyih': 0.0,\n",
              " 'shallow': 0.0,\n",
              " 'stoplists': 0.0,\n",
              " 'mr': 0.0,\n",
              " 'illformed': 0.0,\n",
              " 'z': 0.0,\n",
              " 'lem': 0.0,\n",
              " 'whatever': 0.0,\n",
              " 'chambre': 0.0,\n",
              " 'informingthe': 0.0,\n",
              " 'northwest': 0.0,\n",
              " 'burden': 0.0,\n",
              " 'intensive': 0.0,\n",
              " 'skole': 0.0,\n",
              " 'abound': 0.0,\n",
              " 'element': 0.0,\n",
              " 'yet': 0.0,\n",
              " 'comparisons': 0.0,\n",
              " 'handtagged': 0.0,\n",
              " 'o_shiharaiwo': 0.0,\n",
              " 'nonideal': 0.0,\n",
              " 'elevor': 0.0,\n",
              " 'florida': 0.0,\n",
              " 'annotations': 0.0,\n",
              " 'content': 0.0,\n",
              " 'silja': 0.0,\n",
              " 'orgp': 0.0,\n",
              " 'om': 0.0,\n",
              " \"'practical\": 0.0,\n",
              " 'legitimate': 0.0,\n",
              " 'flank': 0.0,\n",
              " 'cycle': 0.0013,\n",
              " 'easy': 0.0,\n",
              " 'jhu': 0.0,\n",
              " 'fifteen': 0.0,\n",
              " 'g': 0.0038,\n",
              " 'sketch': 0.0,\n",
              " 'semantically': 0.0,\n",
              " 'breach': 0.0,\n",
              " 'entropybased': 0.0,\n",
              " 'somehow': 0.0,\n",
              " 'thorough': 0.0,\n",
              " 'uchicago': 0.0,\n",
              " 'nipulate': 0.0,\n",
              " 'fol': 0.0,\n",
              " 'howpart': 0.0,\n",
              " 'fu': 0.0,\n",
              " 'sixteen': 0.0,\n",
              " 'additional': 0.0025,\n",
              " 'output/meta': 0.0,\n",
              " 'achiev': 0.0,\n",
              " 'become': 0.0013,\n",
              " 'nra': 0.0,\n",
              " 'torussian': 0.0,\n",
              " 'methodologist': 0.0,\n",
              " 'sonya': 0.0,\n",
              " 'ac': 0.0,\n",
              " 'groenendijk': 0.0,\n",
              " 'document': 0.0,\n",
              " 'psychology': 0.0,\n",
              " 'mtc': 0.0,\n",
              " 'judgement': 0.0,\n",
              " 'vation': 0.0,\n",
              " 'grafik': 0.0,\n",
              " 'agent': 0.0,\n",
              " 'preserve': 0.0,\n",
              " 'bbn': 0.0038,\n",
              " 'cre': 0.0,\n",
              " 'sons': 0.0,\n",
              " 'jim': 0.0,\n",
              " 'formation': 0.0,\n",
              " 'alike': 0.0,\n",
              " 'i+': 0.0,\n",
              " 'success': 0.0076,\n",
              " 'realizao': 0.0,\n",
              " 'ruspini': 0.0,\n",
              " 'boguslavsky': 0.0,\n",
              " 'fuf': 0.0,\n",
              " 'cc': 0.0,\n",
              " 'openprompt\\\\': 0.0,\n",
              " 'sa': 0.0,\n",
              " 'sgml': 0.0,\n",
              " 'stract': 0.0,\n",
              " 'joke': 0.0,\n",
              " 'diss': 0.0,\n",
              " 'ously': 0.0,\n",
              " 'stops\\\\': 0.0,\n",
              " 'text=': 0.0,\n",
              " 'selection': 0.0,\n",
              " 'prudent': 0.0,\n",
              " 'timit': 0.0,\n",
              " 'flag': 0.0,\n",
              " 'elaborate': 0.0,\n",
              " 'current': 0.0165,\n",
              " 'res': 0.0,\n",
              " 'carefully': 0.0,\n",
              " 'gram': 0.0,\n",
              " 'reflection': 0.0,\n",
              " 'nonlocal': 0.0,\n",
              " 'mechanical': 0.0,\n",
              " 'comput': 0.0,\n",
              " 'pile': 0.0,\n",
              " 'heuristics': 0.0,\n",
              " 'pands': 0.0,\n",
              " 'anna': 0.0,\n",
              " '/event': 0.0,\n",
              " 'male': 0.0,\n",
              " 'sigir': 0.0,\n",
              " 'kleene': 0.0,\n",
              " 'ifjeune': 0.0,\n",
              " 'nables': 0.0,\n",
              " 'medical': 0.0,\n",
              " 'likeley': 0.0,\n",
              " 'vanlehn': 0.0,\n",
              " 'ifambiguousaction': 0.0,\n",
              " 'lustrative': 0.0,\n",
              " 'transition': 0.0013,\n",
              " 'multilevel': 0.0,\n",
              " 'limitations': 0.0013,\n",
              " 'bbc': 0.0,\n",
              " 'provocated': 0.0,\n",
              " 'computat': 0.0,\n",
              " 'municator': 0.0,\n",
              " 'vided': 0.0,\n",
              " 'bulk': 0.0,\n",
              " 'background': 0.0,\n",
              " 'datadriven': 0.0,\n",
              " 'duration': 0.0013,\n",
              " 'eration': 0.0,\n",
              " 'sera': 0.0,\n",
              " 'comprehend': 0.0,\n",
              " 'feel': 0.0,\n",
              " 'alised': 0.0,\n",
              " 'onsistency': 0.0,\n",
              " 'account': 0.0,\n",
              " 'adatabase': 0.0,\n",
              " 'clerk': 0.0,\n",
              " 'law': 0.0,\n",
              " 'hauspie': 0.0,\n",
              " 'intervention': 0.0,\n",
              " 'par': 0.0,\n",
              " 'systems': 0.0102,\n",
              " 'idiom': 0.0,\n",
              " 'eugene': 0.0,\n",
              " 'invert': 0.0,\n",
              " 'kbpredl': 0.0,\n",
              " 'tem': 0.0,\n",
              " 'inding': 0.0,\n",
              " 'centage': 0.0,\n",
              " 'proprietary': 0.0,\n",
              " 'flection': 0.0,\n",
              " 'tralised': 0.0,\n",
              " 'plex': 0.0,\n",
              " 'coarsely': 0.0,\n",
              " 'computerlike': 0.0,\n",
              " 'cede': 0.0,\n",
              " 'tap': 0.0,\n",
              " 'selfcontained': 0.0,\n",
              " 'lgor': 0.0,\n",
              " 'applicationindependent': 0.0,\n",
              " 'lin': 0.0,\n",
              " 'icilo': 0.0,\n",
              " 'cmdias': 0.0,\n",
              " 'ked': 0.0,\n",
              " 'panama': 0.0,\n",
              " 'intertran': 0.0,\n",
              " 'experi': 0.0,\n",
              " 'sight': 0.0,\n",
              " 'fragment': 0.0,\n",
              " 'operate': 0.0013,\n",
              " 'stu': 0.0,\n",
              " 'cremental': 0.0,\n",
              " 'preted': 0.0,\n",
              " 'syntax': 0.0025,\n",
              " 'children': 0.0,\n",
              " 'myunggil': 0.0,\n",
              " 'tierney': 0.0,\n",
              " 'set': 0.0089,\n",
              " 'franqaises': 0.0,\n",
              " 'ither': 0.0,\n",
              " '/par': 0.0,\n",
              " 'ance': 0.0,\n",
              " 'mt': 0.0,\n",
              " 'sex': 0.0,\n",
              " 'flight': 0.033,\n",
              " 'zhai': 0.0,\n",
              " 'transla': 0.0,\n",
              " 'conversationbased': 0.0,\n",
              " 'kaminsky': 0.0,\n",
              " 'psychological': 0.0,\n",
              " \"'distilling\": 0.0,\n",
              " 'lazy': 0.0,\n",
              " 'takezawa': 0.0,\n",
              " 'textto': 0.0,\n",
              " 'duces': 0.0,\n",
              " 'clsp': 0.0,\n",
              " 'education': 0.0,\n",
              " 'xtraction': 0.0,\n",
              " 'sydney': 0.0,\n",
              " 'italics': 0.0,\n",
              " 'iva': 0.0,\n",
              " 'support': 0.0,\n",
              " 'facts': 0.0,\n",
              " 'ariane': 0.0,\n",
              " 'counterpart': 0.0,\n",
              " \"'it\": 0.0,\n",
              " 'appeal': 0.0,\n",
              " 'income': 0.0,\n",
              " 'doors': 0.0,\n",
              " 'thayer': 0.0,\n",
              " 'xamples': 0.0,\n",
              " 'omission': 0.0,\n",
              " 'hvilke': 0.0,\n",
              " 'add': 0.0089,\n",
              " 'contributions': 0.0,\n",
              " 'sical': 0.0,\n",
              " 'shinn': 0.0,\n",
              " 'impressive': 0.0,\n",
              " 'lus': 0.0,\n",
              " 'tifyfailure': 0.0,\n",
              " 'lished': 0.0,\n",
              " 'ramshaw': 0.0,\n",
              " 'reorder': 0.0,\n",
              " 'lnvalidactionresolved': 0.0,\n",
              " 'recognise': 0.0,\n",
              " 'solver': 0.0,\n",
              " 'mid': 0.0,\n",
              " 'tuc': 0.0,\n",
              " 'stenton': 0.0,\n",
              " 'referencetype': 0.0,\n",
              " 'make': 0.0127,\n",
              " 'te': 0.0,\n",
              " 'placetype': 0.0,\n",
              " 'bratt': 0.0,\n",
              " 'onesentence': 0.0,\n",
              " 'monolingnai': 0.0,\n",
              " 'foreach': 0.0,\n",
              " 'beeket': 0.0,\n",
              " \"o'donnell\": 0.0,\n",
              " 'gemini': 0.0,\n",
              " 'part': 0.0,\n",
              " 'prtn': 0.0,\n",
              " 'notion': 0.0,\n",
              " 'majoritybased': 0.0,\n",
              " 'transfer': 0.0,\n",
              " 'employment': 0.0,\n",
              " 'applications': 0.0013,\n",
              " 'african': 0.0,\n",
              " 'geoffrey': 0.0,\n",
              " 'check': 0.0013,\n",
              " 'sdcurit': 0.0,\n",
              " 'rennes': 0.0,\n",
              " 'whmovement': 0.0,\n",
              " 'mjslby': 0.0,\n",
              " 'dorr': 0.0,\n",
              " 'nonspeech': 0.0,\n",
              " \"'better\": 0.0,\n",
              " 'interfere': 0.0,\n",
              " 'blue': 0.0,\n",
              " 'deliver': 0.0,\n",
              " 'repos': 0.0,\n",
              " 'imaginable': 0.0,\n",
              " 'size': 0.0,\n",
              " 'ceptability': 0.0,\n",
              " 'kabul': 0.0,\n",
              " 'ip': 0.0,\n",
              " 'repeat': 0.0025,\n",
              " 'prior': 0.0,\n",
              " 'nonetheless': 0.0,\n",
              " 'fixit': 0.0,\n",
              " 'plural': 0.0,\n",
              " 'experience': 0.0,\n",
              " 'grenoble': 0.0,\n",
              " 'basic': 0.0,\n",
              " 'e+': 0.0,\n",
              " 'michel': 0.0,\n",
              " 'justeson': 0.0,\n",
              " 'rive': 0.0,\n",
              " 'performance': 0.0025,\n",
              " 'acoustic': 0.0013,\n",
              " 'prove': 0.0013,\n",
              " 'eskenazi': 0.0,\n",
              " 'lars': 0.0,\n",
              " 'diverse': 0.0,\n",
              " 'ondaire': 0.0,\n",
              " 'tentative': 0.0,\n",
              " 'syntaxonly': 0.0,\n",
              " 'internally': 0.0,\n",
              " 'vier': 0.0,\n",
              " 'ety': 0.0,\n",
              " 'dcg': 0.0,\n",
              " 'nese': 0.0,\n",
              " 'bdard': 0.0,\n",
              " 'decelerate': 0.0,\n",
              " 'traditional': 0.0,\n",
              " 'overv': 0.0,\n",
              " 'accordingly': 0.0,\n",
              " 'plaitil': 0.0,\n",
              " 'probability': 0.0013,\n",
              " 'president': 0.0,\n",
              " 'itadakimasu': 0.0,\n",
              " 'ning': 0.0,\n",
              " 'shirai': 0.0,\n",
              " 'cheyer': 0.0,\n",
              " 'arch': 0.0,\n",
              " 'speechinterface': 0.0,\n",
              " 'braun': 0.0,\n",
              " 'lngfiirdsbussarna': 0.0,\n",
              " 'cohesive': 0.0,\n",
              " 'knowledgebased': 0.0,\n",
              " 'euseignement': 0.0,\n",
              " 'ined': 0.0,\n",
              " 'hort': 0.0,\n",
              " 'cal': 0.0,\n",
              " 'graham': 0.0,\n",
              " 'proceed': 0.0,\n",
              " 'already': 0.0038,\n",
              " 'xmltagged': 0.0,\n",
              " 'xtra': 0.0,\n",
              " 'assume': 0.0013,\n",
              " \"h'itpequw=\": 0.0,\n",
              " 'concentrate': 0.0,\n",
              " 'do': 0.0025,\n",
              " 'dat': 0.0,\n",
              " 'anticipate': 0.0,\n",
              " 'expertclient': 0.0,\n",
              " 'artif/ct': 0.0,\n",
              " 'idence': 0.0,\n",
              " 'greatest': 0.0,\n",
              " 'ch': 0.0,\n",
              " 'els': 0.0,\n",
              " 'bac': 0.0,\n",
              " 'umontreal': 0.0,\n",
              " 'lake': 0.0,\n",
              " 'ist': 0.0,\n",
              " '/www': 0.0,\n",
              " 'oz': 0.0,\n",
              " 'evans': 0.0,\n",
              " 'warn': 0.0,\n",
              " 'capt': 0.0,\n",
              " 'ross': 0.0,\n",
              " 'delimit': 0.0,\n",
              " 'cuss': 0.0,\n",
              " 'specif': 0.0,\n",
              " 'quigley': 0.0,\n",
              " 'las': 0.0,\n",
              " 'randomly': 0.0,\n",
              " 'condi': 0.0,\n",
              " 'mcgraw': 0.0,\n",
              " 'assessment': 0.0,\n",
              " 'ban': 0.0,\n",
              " 'intention': 0.0,\n",
              " 'eztending': 0.0,\n",
              " 'rest': 0.0,\n",
              " 'susan': 0.0,\n",
              " 'state\\\\': 0.0,\n",
              " 'sikorski': 0.0,\n",
              " 'oslo': 0.0,\n",
              " 'sheff': 0.0,\n",
              " 'lm': 0.0,\n",
              " 'nicola': 0.0,\n",
              " 'former': 0.0,\n",
              " '//k': 0.0,\n",
              " 'libredchange': 0.0,\n",
              " 'arne': 0.0,\n",
              " 'separately': 0.0,\n",
              " 'ape': 0.0,\n",
              " 'carenini': 0.0,\n",
              " 'dictionaries': 0.0,\n",
              " 'ontologies': 0.0,\n",
              " 'paiva': 0.0,\n",
              " 'selective': 0.0,\n",
              " 'expressions': 0.0,\n",
              " 'conflict': 0.0,\n",
              " 'imp': 0.0,\n",
              " 'ives': 0.0,\n",
              " 'geoff': 0.0,\n",
              " 'limitedprompt': 0.0,\n",
              " 'vi': 0.0,\n",
              " 'hint': 0.0,\n",
              " 'keywords': 0.0,\n",
              " 'halld': 0.0,\n",
              " 'setzer': 0.0,\n",
              " 'survey': 0.0,\n",
              " 'dialogues': 0.0,\n",
              " \"'wizards\": 0.0,\n",
              " 'principles': 0.0,\n",
              " 'gaussier': 0.0,\n",
              " 'assistance': 0.0,\n",
              " 'oflcslp': 0.0,\n",
              " 'iacs': 0.0,\n",
              " 'oc': 0.0,\n",
              " 'tudent': 0.0,\n",
              " 'scratch': 0.0,\n",
              " 'prison': 0.0,\n",
              " 'play': 0.0,\n",
              " 'suujikan': 0.0,\n",
              " 'marcu': 0.0,\n",
              " \"'ir\": 0.0,\n",
              " 'mainly': 0.0,\n",
              " 'pitfalls': 0.0,\n",
              " 'formulas': 0.0,\n",
              " 'everal': 0.0,\n",
              " 'onetoone': 0.0,\n",
              " 'disambig': 0.0,\n",
              " 'realtime': 0.0,\n",
              " 'compute': 0.0114,\n",
              " 'predefined': 0.0,\n",
              " 'christiane': 0.0,\n",
              " 'ities': 0.0,\n",
              " 'go_to': 0.0,\n",
              " 'interpretat': 0.0,\n",
              " 'eiichiro': 0.0,\n",
              " 'dence': 0.0,\n",
              " 'scale': 0.0,\n",
              " 'bootstrap': 0.0,\n",
              " 'paul_denisowski/cedict': 0.0,\n",
              " 'langlais': 0.0,\n",
              " 'rectabootstrapping': 0.0,\n",
              " 'enlgez': 0.0,\n",
              " 'language/': 0.0,\n",
              " 'eclaratively': 0.0,\n",
              " 'opic': 0.0,\n",
              " 'tillman': 0.0,\n",
              " 'restore': 0.0,\n",
              " 'hinom': 0.0,\n",
              " 'urn': 0.0,\n",
              " 'portability': 0.0,\n",
              " 'markup': 0.0,\n",
              " 'sanibel': 0.0,\n",
              " 'customisation': 0.0,\n",
              " 'migrations': 0.0,\n",
              " 'hiercx': 0.0,\n",
              " \"sex='f\": 0.0,\n",
              " 'jednoducho': 0.0,\n",
              " 'langue': 0.0,\n",
              " 'quit': 0.0,\n",
              " 'process': 0.0038,\n",
              " 'abstract': 0.0,\n",
              " 'massive': 0.0,\n",
              " 'psys': 0.0,\n",
              " 'alexis': 0.0,\n",
              " 'green': 0.0,\n",
              " 'ndtt': 0.0,\n",
              " 'fense': 0.0,\n",
              " 'ssnda': 0.0,\n",
              " 'currently': 0.0076,\n",
              " 'impressions': 0.0,\n",
              " 'responsible': 0.0013,\n",
              " 'hangups': 0.0,\n",
              " 'migrate': 0.0,\n",
              " 'zz': 0.0,\n",
              " 'luzzati': 0.0,\n",
              " 'rise': 0.0,\n",
              " 'deployment': 0.0,\n",
              " 'sunstroke': 0.0,\n",
              " \"'conversation\": 0.0,\n",
              " 'jelinek': 0.0,\n",
              " 'voked': 0.0,\n",
              " 'jaa': 0.0,\n",
              " 'tuation': 0.0,\n",
              " 'translmion': 0.0,\n",
              " 'animate': 0.0,\n",
              " 'realisation': 0.0,\n",
              " 'characterbased': 0.0,\n",
              " 'nalogy': 0.0,\n",
              " 'management': 0.0038,\n",
              " 'canonical': 0.0,\n",
              " 'pernilla': 0.0,\n",
              " 'assumption': 0.0,\n",
              " 'inguistics': 0.0,\n",
              " 'hnom': 0.0,\n",
              " 'note': 0.0,\n",
              " 'optionally': 0.0,\n",
              " 'import': 0.0,\n",
              " 'hungary': 0.0,\n",
              " 'pant': 0.0,\n",
              " 'markups': 0.0,\n",
              " 'posal': 0.0,\n",
              " 'obstacle': 0.0,\n",
              " 'erson': 0.0,\n",
              " 'villiers': 0.0,\n",
              " 'msf': 0.0,\n",
              " 'carberry': 0.0,\n",
              " 'mail': 0.0,\n",
              " 'outpuned': 0.0,\n",
              " 'saturday': 0.0,\n",
              " 'remove': 0.0013,\n",
              " 'workshop': 0.0,\n",
              " 'wordnet': 0.0,\n",
              " 'capabilities': 0.0,\n",
              " 'zile': 0.0,\n",
              " 'woolworth': 0.0,\n",
              " 'telephonebased': 0.0,\n",
              " 'express': 0.0025,\n",
              " 'kittredge': 0.0,\n",
              " 'merit': 0.0,\n",
              " 'uni': 0.0,\n",
              " 'jean': 0.0,\n",
              " 'toolkit': 0.0,\n",
              " 'levin': 0.0,\n",
              " \"'s\": 0.0203,\n",
              " 'simpler': 0.0038,\n",
              " 'erations': 0.0,\n",
              " '/en/gif/kan': 0.0,\n",
              " 'awkward': 0.0,\n",
              " 'tecedent': 0.0,\n",
              " 'technolo': 0.0,\n",
              " 'conference': 0.0,\n",
              " 'comman': 0.0,\n",
              " 'color': 0.0,\n",
              " 'meet': 0.0114,\n",
              " 'forward': 0.0,\n",
              " 'gamla': 0.0,\n",
              " 'lehtola': 0.0,\n",
              " 'possession': 0.0,\n",
              " 'wire': 0.0,\n",
              " 'kennebunkport': 0.0,\n",
              " 'school': 0.0,\n",
              " 'laypersons': 0.0,\n",
              " 'create': 0.0025,\n",
              " 'movie': 0.0,\n",
              " 'gestion': 0.0,\n",
              " 'conceptualize': 0.0,\n",
              " 'horiguchi': 0.0,\n",
              " 'hendler': 0.0,\n",
              " 'termination': 0.0,\n",
              " 'variables': 0.0013,\n",
              " 'largely': 0.0,\n",
              " 'precision': 0.0,\n",
              " 'nist': 0.0,\n",
              " 'typ': 0.0,\n",
              " 'ollars': 0.0,\n",
              " 'sage': 0.0,\n",
              " 'heldback': 0.0,\n",
              " 'instrument': 0.0,\n",
              " 'tokens': 0.0,\n",
              " 'mike': 0.0,\n",
              " 'tors': 0.0,\n",
              " 'steal': 0.0,\n",
              " 'interest': 0.0,\n",
              " 'ka': 0.0,\n",
              " 'schwarz': 0.0,\n",
              " 'miles': 0.0025,\n",
              " 'neutral': 0.0,\n",
              " 'job': 0.0,\n",
              " 'kintsch': 0.0,\n",
              " 'frank': 0.0,\n",
              " 'presum': 0.0,\n",
              " 'products': 0.0,\n",
              " 'testsuites': 0.0,\n",
              " 'series': 0.0,\n",
              " 'scenario': 0.0025,\n",
              " 'respect': 0.0,\n",
              " 'platform': 0.0,\n",
              " 'telephony': 0.0013,\n",
              " 'context': 0.0,\n",
              " 'niques': 0.0,\n",
              " 'biermann': 0.0,\n",
              " 'mit': 0.0,\n",
              " 'termined': 0.0,\n",
              " 'please': 0.0,\n",
              " 'stateoftheart': 0.0,\n",
              " 'reason': 0.0,\n",
              " 'munication': 0.0,\n",
              " 'plausibility': 0.0013,\n",
              " 'airline': 0.0063,\n",
              " 'moldovan': 0.0,\n",
              " 'party': 0.0,\n",
              " 'mann': 0.0,\n",
              " 'restriction': 0.0,\n",
              " 'robot': 0.0,\n",
              " 'berger': 0.0,\n",
              " 'desfiinat': 0.0,\n",
              " \"'product__ids\": 0.0,\n",
              " 'verbalization': 0.0013,\n",
              " 'helsinki': 0.0,\n",
              " 'nils': 0.0,\n",
              " 'imme': 0.0,\n",
              " 'sr': 0.0,\n",
              " 'severely': 0.0,\n",
              " 'heir': 0.0,\n",
              " 'automatische': 0.0,\n",
              " 'deviate': 0.0,\n",
              " 'ternational': 0.0,\n",
              " 'modifi': 0.0,\n",
              " 'quant': 0.0,\n",
              " '/title': 0.0,\n",
              " 'prosodic': 0.0,\n",
              " 'pieter': 0.0,\n",
              " 'tipster': 0.0,\n",
              " 'matic': 0.0,\n",
              " 'immediately': 0.0,\n",
              " 'headmounted': 0.0,\n",
              " 'minutes': 0.0,\n",
              " 'toward': 0.0,\n",
              " 'utive': 0.0,\n",
              " 'sempredit': 0.0,\n",
              " 'rdgions': 0.0,\n",
              " 'kill': 0.0,\n",
              " 'ca/projetsilc': 0.0,\n",
              " 'background=': 0.0,\n",
              " 'metadialogue': 0.0,\n",
              " 'productivity': 0.0,\n",
              " 'alogue': 0.0,\n",
              " 'regardless': 0.0,\n",
              " 'textbook': 0.0,\n",
              " 'however': 0.0038,\n",
              " 'perhaps': 0.0,\n",
              " 'jsnsson': 0.0,\n",
              " 'insttaneous': 0.0,\n",
              " 'referent': 0.0,\n",
              " 'recogniser': 0.0,\n",
              " 'wong': 0.0,\n",
              " 'border=': 0.0,\n",
              " 'semantics': 0.0,\n",
              " 'mar': 0.0,\n",
              " 'looselytyped': 0.0,\n",
              " 'notify': 0.0,\n",
              " 'tell': 0.0,\n",
              " 'ctors': 0.0,\n",
              " 'field': 0.0,\n",
              " 'peakers': 0.0,\n",
              " 'controversial': 0.0,\n",
              " 'labs': 0.0,\n",
              " 'blackboard': 0.0,\n",
              " 'formulation': 0.0,\n",
              " 'rac': 0.0,\n",
              " 'herence': 0.0,\n",
              " 'compare': 0.0,\n",
              " 'oun': 0.0,\n",
              " 'woods': 0.0,\n",
              " 'almost': 0.0025,\n",
              " 'locations': 0.0,\n",
              " 'hierarchy': 0.0,\n",
              " 'giveoptions': 0.0,\n",
              " 'instan': 0.0,\n",
              " 'span': 0.0,\n",
              " 'sgmliike': 0.0,\n",
              " 'strand': 0.0,\n",
              " 'dahlbick': 0.0,\n",
              " 'breadth': 0.0,\n",
              " 'karnavat': 0.0,\n",
              " 'albeit': 0.0,\n",
              " 'amined': 0.0,\n",
              " 'tepid': 0.0,\n",
              " 'befolkning': 0.0,\n",
              " 'instantiation': 0.0,\n",
              " 'aer': 0.0,\n",
              " 'applicability': 0.0,\n",
              " 'ble': 0.0,\n",
              " 'nonstopword': 0.0,\n",
              " 'ority': 0.0,\n",
              " 'xplain': 0.0,\n",
              " 'nearness': 0.0013,\n",
              " 'alexander': 0.0,\n",
              " 'summer': 0.0,\n",
              " 'attend': 0.0,\n",
              " 'lag': 0.0,\n",
              " 'ortmanns': 0.0,\n",
              " 'arens': 0.0,\n",
              " 'connectionist': 0.0,\n",
              " 'weaken': 0.0,\n",
              " 'hsinhsi': 0.0,\n",
              " 'hirschman': 0.0,\n",
              " 'with/without': 0.0,\n",
              " 'carmel': 0.0,\n",
              " \"'am\": 0.0,\n",
              " 'conform': 0.0,\n",
              " 'thirtysecond': 0.0,\n",
              " 'zukerman': 0.0,\n",
              " 'homonymous': 0.0,\n",
              " 'constitutive': 0.0,\n",
              " 'nit': 0.0,\n",
              " 'assert': 0.0,\n",
              " 'generous': 0.0,\n",
              " 'flame': 0.0051,\n",
              " 'nine': 0.0,\n",
              " 'conjuncts': 0.0,\n",
              " 'evaluator': 0.0,\n",
              " 'klavans': 0.0,\n",
              " 'candidate': 0.0,\n",
              " 'korean': 0.0,\n",
              " 'wordforword': 0.0,\n",
              " 'chinese': 0.0,\n",
              " 'imperative': 0.0,\n",
              " 'ferences': 0.0,\n",
              " 'physiology': 0.0,\n",
              " 'lance': 0.0,\n",
              " 'assertion': 0.0013,\n",
              " 'kelledy': 0.0,\n",
              " 'qu': 0.0,\n",
              " 'stockholm': 0.0,\n",
              " 'discrimination': 0.0,\n",
              " 'declarativity': 0.0,\n",
              " 'nizer': 0.0,\n",
              " 'fad': 0.0,\n",
              " 'difficulty': 0.0,\n",
              " 'rocgnition': 0.0,\n",
              " 'successful': 0.0038,\n",
              " 'jacques': 0.0,\n",
              " 'myba': 0.0,\n",
              " 'eventdenoting': 0.0,\n",
              " 'city': 0.0076,\n",
              " 'negation': 0.0,\n",
              " 'a/': 0.0,\n",
              " 'row': 0.0,\n",
              " 'five': 0.0,\n",
              " 'detect': 0.0,\n",
              " 'dwell': 0.0,\n",
              " 'tm': 0.0,\n",
              " 'dudng': 0.0,\n",
              " 'rabbit': 0.0,\n",
              " 'quantifiers': 0.0,\n",
              " 'overall': 0.0,\n",
              " 'dialogics': 0.0,\n",
              " 'renormalize': 0.0,\n",
              " 'govpattern': 0.0,\n",
              " 'orientations': 0.0,\n",
              " 'ike': 0.0,\n",
              " 'thirteen': 0.0,\n",
              " 'ce': 0.0,\n",
              " 'dual': 0.0,\n",
              " 'alright': 0.0,\n",
              " 'wasson': 0.0,\n",
              " '//humanities': 0.0,\n",
              " 'unnecessary': 0.0,\n",
              " 'exist': 0.0025,\n",
              " 'auditor': 0.0,\n",
              " 'cute': 0.0,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 2\n",
        "Make a inverted index for full text search"
      ],
      "metadata": {
        "id": "FS_iV87lxQ_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db = Database()\n",
        "index = InvertedIndex(db)"
      ],
      "metadata": {
        "id": "9TxgMM44xM2J"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, data in enumerate(doc_dict.values()):\n",
        "  doc = {'id': i, 'text': data}\n",
        "  index.index_document(doc)"
      ],
      "metadata": {
        "id": "jvG473Igxien"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "search_term = \"natural language\"\n",
        "result = index.lookup_query(search_term)"
      ],
      "metadata": {
        "id": "dtzztBfkyRVT"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "match_text = {}\n",
        "for term in result.keys():\n",
        "  for appearance in result[term]:\n",
        "    document = db.get(appearance.docId)\n",
        "    if appearance.docId in match_text.keys():\n",
        "      match_text[appearance.docId] = highlight_term(appearance.docId, term, match_text[appearance.docId])\n",
        "    else:\n",
        "      match_text[appearance.docId] = highlight_term(appearance.docId, term, document['text'])"
      ],
      "metadata": {
        "id": "JgK30UP7y7qf"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for text in match_text.values():\n",
        "  print(text)\n",
        "  print(\"-----------------------------\")  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZzsAm2YyexR",
        "outputId": "2e3d6473-d0de-4ded-cb25-ebaa4e64a602"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- document 0: --- document 0: TALK'N'TRAVEL: A CONVERSATIONAL SYSTEM FOR AIR  TRAVEL PLANNING  David Stallard  BBN Technologies, GTE  70 Fawcett St.  Cambridge, MA, USA, 02238  Stallard@bbn.com  Abstract  We describe Talk'n'Travel, a spoken  dialogue \u001b[1;32;40m language \u001b[0;0m system for making air  travel plans over the telephone.  Talk'n'Travel is a fully conversational,  mixed-initiative system that allows the  user to specify the constraints on his travel  plan in arbitrary order, ask questions, etc.,  in general spoken English. The system  operates according to a plan-based agenda  mechanism, rather than a finite state  network, and attempts to negotiate with  the user when not all of his constraints can  be met.  Introduction  This paper describes Talk'n'Travel, a spoken  \u001b[1;32;40m language \u001b[0;0m dialogue system for making complex  air travel plans over the telephone.  Talk'n'Travel is a research prototype system  sponsored under the DARPA Communicator  program (MITRE, 1999). Some other systems  in the program are Ward and Pellom (1999),  Seneff and Polifroni (2000) and Rudnicky et al  (1999). The common task of this program is a  mixed-initiative dialogue over the telephone, in  which the user plans a multi-city trip by air,  including all flights, hotels, and rental cars, all in  conversational English over the telephone.  The Communicator common task presents  special challenges. It is a complex task with  many subtasks, including the booking of each  flight, hotel, and car reservation. Because the  number of legs of the trip may be arbitrary, the  number of such subtasks is not known in  advance. Furthermore, the user has complete  freedom to say anything at any time. His  utterances can affect just the current subtask, or  multiple subtasks at once (\"I want to go from  Denver to Chicago and then to San Diego\"). He  can go back and change the specifications for  completed subtasks. And there are important  constraints, such as temporal relationships  between flights, that must be maintained for the  solution to the whole task to be coherent.  In order to meet this challenge, we have sought  to develop dialogue techniques for  Talk'n'Travel that go beyond the rigid system-  directed style of familiar IVR systems.  Talk'n'Travel is instead a mixed initiative  system that allows the user to specify constraints  on his travel plan in arbitrary order. At any  point in the dialogue, the user can supply  information other than what the system is  currently prompting for, change his mind about  information he has previously given and even  ask questions himself. The system also tries to  be helpful, eliciting constraints from the user  when necessary. Furthermore, if at any point the  constraints the user has specified cannot all be  met, the system steps in and offers a relaxation  of them in an attempt o negotiate a partial  solution with the user.  The next section gives a brief overview of the  system. Relevant components are discussed in  subsequent sections.  I System Overview  The system consists of the following modules:  speech recognizer, \u001b[1;32;40m language \u001b[0;0m understander,  dialogue manager, state manager, \u001b[1;32;40m language \u001b[0;0m  generator, and speech synthesizer. The modules  68  interact with each other via the central hub  module of the Communicator Common  Architecture.  The speech recognizer is the Byblos system  (Nguyen, 1995). It uses an acoustic model  trained from the Macrophone telephone corpus,  and a bigram/trigram \u001b[1;32;40m language \u001b[0;0m model trained  from -40K utterances derived from various  sources, including data collected under the  previous ATIS program (Dahl et al, 1994).  The speech synthesizer is Lucent's commercial  system. Synthesizer and recognizer both  interface to the telephone via Dialogics  telephony board. The database is currently a  frozen snapshot of actual flights between 40  different US cities (we are currently engaged in  interfacing to a commercial air travel website).  The various \u001b[1;32;40m language \u001b[0;0m components are written in  Java. The complete system runs on Windows  NT, and is compliant with the DARPA  Communicator Common architecture.  The present paper is concerned with the dialogue  and discourse management, \u001b[1;32;40m language \u001b[0;0m generation  and \u001b[1;32;40m language \u001b[0;0m understanding components. In the  remainder of the paper, we present more detailed  discussion of these components, beginning with  the \u001b[1;32;40m language \u001b[0;0m understander in Section 2. Section  3 discusses the discourse and dialogue  components, and Section 4, the \u001b[1;32;40m language \u001b[0;0m  generator.  2 Language Understanding  2.1 Meaning Representation  Semantic frames have proven useful as a  meaning representation for many applications.  Their simplicity and useful computational  properties have often been seen as more  important han their limitations in expressive  power, especially in simpler domains.  Even in such domains, however, flames still  have some shortcomings. While most \u001b[1;32;40m natural \u001b[0;0mly  representing equalities between slot and filler,  flames have a harder time with inequalities, uch  as 'the departure time is before 10 AM', or 'the  airline is not Delta'. These require the slot-filler  to be some sort of predicate, interval, or set  object, at a cost to simplicity uniformity. Other  problematic ases include n-ary relations ('3  miles from Denver'), and disjunctions of  properties on different slots.  In our Talk'n'Travel work, we have developed  a meaning representation formalism called path  constraints, which overcomes these problems,  while retaining the computational advantages  that made frames attractive in the first place. A  path constraint is an expression of the form :  (<path> <relation> <arguments>*)  The path is a compositional chain of one or more  attributes, and relations are 1-place or higher  predicates, whose first argument is implicitly the  path. The relation is followed by zero or more  other arguments. In the simplest case, path  constraints can be thought of as flattenings of a  tree of frames. The following represents the  constraint that the departure time of the first leg  of the itinerary is the city Boston :  LEGS.0.ORIG_CITY EQ BoSToN  Because this syntax generalizes to any relation,  however, the constraint \"departing before 10  AM\" can be represented in a syntactically  equivalent way:  LEGS.0.DEPART_TIME LT 1000  Because the number of arguments i arbitrary, it  is equally straightforward to represent a one-  place property like \"x is nonstop\" and a three  place predicate like \"x is 10 miles from Denver\".  Like flames, path constraints have a fixed  format that is indexed in a computationally  useful way, and are simpler than logical forms.  Unlike flames, however, path constraints can be  combined in arbitrary conjunctions, disjunctions,  and negations, even across different paths. Path  constraint meaning representations are also flat  lists of constraints rather than trees, making  matching rules, etc, easier to write for them.  69  2.2 The GEM Understanding System  Language understanding in Talk'n'Travel is  carried out using a system called GEM (for  Generative Extraction Model). GEM (Miller,  1998) is a probabilistic semantic grammar that is  an outgrowth of the work on the HUM system  (Miller, 1996), but uses hand-specified  knowledge in addition to probability. The hand-  specified knowledge is quite simple, and is  expressed by a two-level semantic dictionary. In  the first level, the entries map alternative word  strings to a single word class. For example, the  following entry maps several alternative forms  to the word class DEPART:  Leave, depart, get out of => DEPART  In the second level, entries map sequences of  word classes to constraints:  Name: DepartCity 1  Head: DEPART  Classes: \\[DEPART FROM CITY\\]  Meaning: (DEST_CITY EQ <CITY>)  The \"head\" feature allows the entry to pass one  of its constituent word classes up to a higher  level pattern, allowing the given pattern to be a  constituent of others.  The dictionary entries generate a probabilistic  recursive transition network (PRTN), whose  specific structure is determined by dictionary  entries. Paths through this network correspond  one-to-one with parse trees, so that given a path,  there is exactly one corresponding tree. The  probabilities for the arcs in this network can be  estimated from training data using the EM  (Expectation-Maximization) procedure.  GEM also includes a noise state to which  arbitrary input between patterns can be mapped,  making the system quite robust to ill-formed  input. There is no separate phase for handling  ungrammatical input, nor any distinction  between grammatical nd ungrammatical input.  3 Discourse and Dialogue Processing  A key feature of the Communicator task is that  the user can say anything at any time, adding or  changing information at will. He may add new  subtasks (e.g. trip legs) or modifying existing  ones. A conventional dialogue state network  approach would be therefore infeasible, as the  network would be almost unboundedly arge and  complex.  A signifigant additional problem is that changes  need not be monotonic. In particular, when  changing his mind, or correcting the system's  misinterpretations, the user may delete subtask  structures altogether, as in the subdialog:  S: What day are you returning to Chicago?  U: No, I don't want a return flight.  Because they take information away rather than  add it, scenarios like this one make it  problematic to view discourse processing as  producing a contextualized, or \"thick frame\",  version of the user's utterance. In our system,  therefore, we have chosen a somewhat different  approach.  The discourse processor, called the state  manager, computes the most likely new task  state, based on the user's input and the current  task state. It also computes a discourse vent,  representing its interpretation f what happened  in the conversation as a result of the user's  utterance.  The dialogue manager is a separate module, as  has no state managing responsibilities at all.  Rather, it simply computes the next action to  take, based on its current goal agenda, the  discourse vent returned by the state manager,  and the new state. This design has the advantage  of making the dialogue manager considerably  simpler. The discourse event also becomes  available to convey to the user as confirmation.  We discuss these two modules in more detail  below.  70  3.1 State Manager  The state manager is responsible for computing  and maintaining the current ask state. The task  state is simply the set of path constraints which  currently constrain the user's itinerary. Also  included in the task state are the history of user  and system utterances, and the current subtask  and object in focus, if any.  The state manager takes the N-best list of  recognition hypotheses as input. It invokes the  understanding module on a hypothesis to obtain  a semantic interpretation. The semantic  interpretation so obtained is subjected to the  following steps:  1. Resolve ellipses if any  2. Match input meaning to subtask(s)  3. Expand local ambiguities  4. Apply inference and coherency rules  5. Compute database satisfiers  6. Relax constraints if neccesary  7. Determine the most likely alternative and  compute the discourse vent  At any of these steps, zero or more alternative  new states can result, and are fed to the next  step. If zero states result at any step, the new  meaning representation is rejected, and another  one requested from the understander. If no more  hypotheses are available, the entire utterance is rejected, and a DONT_UNDERSTAND event is  returned to the dialogue manager.  Step 1 resolves ellipses. Ellipses include both  short responses like \"Boston\" and yes/no  responses. In this step, a complete meaning  representation such as ' (ORIQCITY EQ  BOSTON)' is generated based on the system's  prompt and the input meaning. The hypothesis  rejected if this cannot be done.  Step 2 matches the input meaning to one or more  of the subtasks of the problem. For the  Communicator p oblem, the subtasks are legs of  the user's itinerary, and matching is done based  on cities mentioned in the input meaning. The  default is the subtask currently in focus in the  dialogue.  A match to a subtask is represented by adding  the prefix for the subtask to the path of the  constraint. For example, \"I want to arrive in  Denver by 4 PM\" and then continue on to  Chicago would be :  LEGS.0.DEST_CITY EQ DENVER  LEGS.0.ARRIVE_TIME LE 1600  LEGS. 1.ORIG_CITY EQ DENVER  LEGS. 1.DEST CITY EQ CHICAGO  In Step 3, local ambiguities are expanded into  their different possibilities. These include  partially specified times such as \"2 o'clock\"  Step 4 applies inference and coherency rules.  These rules will vary from application to  application. They are written in the path  constraint formalism, augmented with variables  that can range over attributes and other values.  The following is an example, representing the  constraint a flight leg cannot be scheduled to  depart until after the preceding flight arrives:  LEGS.$N.ARRIVE  LT  LEGS. $N+ 1 .DEPART  States that violate coherency constraints are  discarded.  Step 5 computes the set of objects in the  database that satisfy the constraints on the  current subtask. This set will be empty when the  constraints are not all satisfiable, in which case  the relaxation of Step 6 is invoked. This  relaxation is a best-first search for the satisfiable  subset of the constraints that are deemed closest  to what the user originally wanted. Alternative  relaxations are scored according to a sum of  penalty scores for each relaxed constraint,  derived from earlier work by Stallard (1995).  The penalty score is the sum of two terms: one  for the relative importance of the attribute  concerned (e.g. relaxations of DEPART_DATE  are penalised more than relaxations of  AIRLINE) and the other for the nearness of the  satisfiers to the original constraint (relevant for  number-like attributes like departure time).  71  The latter allows the system to give credit to  solutions that are near fits to the user's goals,  even if they relax strongly desired constraints.  For example, suppose the user has expressed a  desire to fly on Delta and arrive by 3 PM, while  the system is only able to find a flight on Delta  that arrives at 3:15 PM. In this case, this flight,  which meets one constraint and almost meets the  other, may well satisfy the user more than a  flight on a different airline that happens to meet  the time constraint exactly.  In the final step, the alternative new states are  rank-ordered according to a pragmatic score, and  the highest-scoring alternative is chosen. The  pragmatic score is computed based on a number  of factors, including the plausibility of  disambiguated times and whether or not the state  interpreted the user as responding to the system  prompt.  The appropriate discourse event is then  deterministicaUy computed and returned. There  are several types of discourse vent. The most  common is UPDATE, which specifies the  constraints that have been added, removed, or  relaxed. Another type is REPEAT, which is  generated when the user has simply repeated  constraints the system already knows. Other  types include QUESTION, TIMEOUT, and  DONT UNDERSTAND.  3.1 Dialogue Manager  Upon receiving the new discourse event from  the state manager, the dialogue manager  determines what next action to take. Actions  can be external, such as speaking to the user or  asking him a question, or internal, such as  querying the database or other elements of the  system state. The current action is determined by  consulting a stack-based agenda of goals and  actions.  The agenda stack is in turn determined by an  application-dependent library of plans. Plans are  tree structures whose root is the name of the goal  the plan is designed to solve, and whose leaves  are either other goal names or actions. An  example of a plan is the following:  Completeltinerary =>  (Prompt \"How can I help you?\")  (forall legs $n  GetRoutelnfo  GetSpecificFlight  GetHotelAndCar  GetNextLeg))  This is a plan for achieving the goal  Completeltinerary. It begins with a open-ended  prompt and then iterates over values of the  variable $N for which constraints on the prefix  LEGS.$N exist, working on high-level subgoals,  such as getting the route and booking a flight,  for each leg. The last goal determines whether  there is another leg to the itinerary, in which  case the itera  The system begins the interaction with the high-  level goal START on its stack. At each step, the  system examines the top of its goal stack and  either executes it if it is an action suitable for  execution, or replaces it on the stack with its  plan steps if it is a goal.  Actions are objects with success and relevancy  predicates and an execute method, somewhat  similar to the \"handlers\" of Rudnicky and Xu  (1999). An action has an underlying oal, such  as finding out the user's constraints on some  attribute. The action's success predicate will  return true if this underlying goal has been  achieved, and its relevancy predicate will return  true if it is still relevant to the current situation.  Before carrying out an action, the dialogue  manager first checks to see if its success  predicate returns false and its relevancy  predicate returns true. If either condition is not  met, the action is popped off the stack and  disposed of without being executed. Otherwise,  the action's execute method is invoked.  The system includes a set of actions that are  built in, and may be parameterized for each each  domain. For example, the action type ELICIT is  parameterized by an attribute A, a path prefix P,  and verbalization string S. Its success predicate  returns true if the path 'P.A' is constrained in the  current state. Its execute method generates a meaning frame that is passed to the \u001b[1;32;40m language \u001b[0;0m  72  generator, ultimately prompting the user with a  question such as \"What city are you flying to?\"  Once an action's execute method is invoked, it  remains on the stack for the next cycle, where it  is tested again for success and relevancy. In this  case, if the success condition is met - that is, if  the user did indeed reply with a specification of  his destination city - the action is popped off the  stack. If the system did not receive this  information, either because the user made a  stipulation about some different attribute, asked  a question, or simply was not understood, the  action remains on the stack to be executed again.  Of course, the user may have already specified  the destination city in a previous utterance. In  this case, the action is already satisfied, and is  not executed. In this way, the user has  flexibility in how he actually carries out the  dialogue.  In certain situations, other goals and actions may  be pushed onto the stack, temporarily  interrupting the execution of the current plan.  For example, the user himself may ask a  question. In this case, an action to answer the  question is created, and pushed onto the stack.  The dialogue manager then executes this action  to answer the user's question before continuing  on with the plan. Or the state manager may  generate a clarification question, which the  dialogue manager seeks to have the user answer.  Actions can also have a set of conditional  branchings that are tested after the action is  executed. If present, these determine the next  action to execute or goal to work on. For  example, the action that asks the user \"Do you  want a return flight to X?\" specifies the branch  to be taken when the user replies in the negative.  This branch includes an action that asks the user  \"Is Y your final destination?\", an action that is  executed if the user did not specify an additional  destination along with his negative reply.  Unlike the approach taken by Ward and Pellom  (1999), which seeks to avoid scripting entirely  by driving the dialogue off the current status of  the itinerary, the Talk'n'Travel dialogue  manager thus seeks to allow partially scripted  dialogue where appropriate to the situation.  4 Language Generation  The \u001b[1;32;40m language \u001b[0;0m generator takes a meaning frame  from the dialogue manager, and generates a text  string in English for it. It uses a set of pattern-  based rules that map constraints into alternative  syntactic realisations. For example, the  following rule allows a constraint on departure  time to be realized as \"leave at 3 PM\"  or \"3 PM flight\":  LEG.$N.DEPART_TIME EQ $X  =~ \\[leave at $X\\],  \\[nom-comp $X\\]  Different realization rules can be selected for  depending upon whether the constraint is to be  realized as an assertion or as a description. The  generation algorithm assembles the selected  realizations for each constraint into a simplified  syntax tree, selecting appropriate inflections of  verb and noun heads as it does so. Terminal  values in constraints are realized as type-specific  nominals, such as \"3 PM\" or \"Delta\".  A crucial feature of the generation process is  that it adds to each prompt a paraphrase of the  most recent discourse event, corresponding to  what the system thinks the user just said. This  helps keep the conversation grounded in terms  of mutual understanding between the  participants.  5 Example Scenario  The following is an example dialog with the  system:  S: Welcome to Talk'n'Travel. How can I help  you?  U: I need a flight from Boston to Denver  tomorrow.  S: On Thursday, November 18, flying from  Boston to Denver. What time do you want to  leave?  U: I want to leave before 10 AM.  S: Leaving before 10 AM. Do you have an  airline preference?  U: Continental.  73  S: Continental. I 'm sorry, there are no flights on  Continental leaving before 10 AM. I can get  you a flight on Continental leaving at 10:10 AM  instead. Would that be OK?  U: Yes.  S: OK, I've got you on Continental flight 429,  departing Boston at 10:10 AM, arriving at  Denver at 3:44 PM. Do you want to book a  return flight from Denver to Boston?  U: Yes, on Monday.  S: Alright, we'll add a flight on Monday,  November 22, from Boston to Denver to your  itinerary. I have a Continental flight at 7 AM.  Is that OK?  U. Anything later?  S: Leaving after 7 AM. There are no flights on  Continental after 7 AM. I can get you a flight  after 7 AM on Frontier, Northwest, and United  instead. Would that be OK?  U: How about United.  S: United. I have a United flight at 8:01 AM. Is  that flight OK?  U: Yes.  S: OK, I've got you on United flight 1762,  departing Denver at 8:01 AM, arriving at Boston  at 1:42 PM.  6 Current Status and Conclusions  The Talk'n'Travel system described here was  successfully demonstrated at the DARPA  Communicator Compare and Contrast  Workshop in June 1999. We are currently  collecting data with test subjects and are using  the results to improve the system's performance  in all areas, in preparation for the forthcoming  common evaluation of Communicator systems  in June 2000.  8 of the subjects were successful. Of successful  sessions, the average duration was 387 seconds,  with a minimum of 272 and a maximum of 578.  The average number of user utterances was 25,  with a minimum of 18 and a maximum of 37.  The word error rate of the recognizer was  11.8%.  The primary cause of failure to complete the  scenario, as well as excessive time spent on  completing it, was corruption of the discourse  state due to recognition or interpretation errors.  While the system informs the user of the change  in state after every utterance, the user was not  always successful in correcting it when it made  errors, and sometimes the user did not even  notice when the system had made an error. If the  user is not attentive at the time, or happens not  to understand what the synthesizer said, there is  no implicit way for him to find out afterwards  what the system thinks his constraints are.  While preliminary, these results point to two  directions for future work. One is that the system  needs to be better able to recognize and deal  with problem situations in which the dialogue is  not advancing. The other is that the system  needs to be more communicative about its  current understanding of the user's goals, even  at points in the dialogue at which it might be  assumed that user and system were in  agreement.  Acknowledgements  This work was sponsored by DARPA and  monitored by SPAWAR Systems Center under  Contract No. N66001-99-D-8615.  To determine the performance of the system, we  ran an informal experiment in which 11 different  subjects called into the system and attempted to  use it to solve a travel problem. None of the  subjects were system developers. Each subject  had a single session in which he was given a  three-city trip to plan, including dates of travel,  constraints on departure and arrival times, airline  preferences.  The author wishes to thank Scott Miller for the  use of his GEM system.  References  MITRE (1999) DARPA Communicator homepage  http://fofoca.mitre.org\\].  Ward W., and Pellom, B. (1999) The CU  Communicator System. In 1999 IEEE Workshop  on Automatic Speech Recognition and  Understanding, Keystone, Colorado.  -/4.  Miller S. (1998) The Generative Extraction Model.  Unpublished manuscript.  Dahl D., Bates M., Brown M., Fisher, W. Hunicke-  Smith K., Pallet D., Pao C., Rudnicky A., and  Shriberg E. (1994) Expanding the scope of the  ATIS task. In Proceedings of the ARPA Spoken  Language Technology Workshop, Plainsboro, NJ.,  pp 3-8.  Constantinides P., Hansma S., Tchou C. and  Rudnicky, A. (1999) A schema-based approach to  dialog control. Proceedings oflCSLP, Paper 637.  Rudnicky A., Thayer, E., Constantinides P., Tchou  C., Shern, R., Lenzo K., Xu W., Oh A. (1999)  Creating \u001b[1;32;40m natural \u001b[0;0m dialogs in the Carnegie Mellon  Communicator system. Proceedings of  Eurospeech, 1999, Vol 4, pp. 1531-1534  Rudnicky A., and Xu W. (1999) An agenda-based  dialog management architecture for soken  \u001b[1;32;40m language \u001b[0;0m systems. In 1999 IEEE Workshop on  Automatic Speech Recognition and Understanding,  Keystone, Colorado.  Seneff S., and Polifroni, J. (2000) Dialogue  Management in the Mercury Flight Reservation  System. ANLP Conversational Systems Workshop.  Nguyen L., Anastasakos T., Kubala F., LaPre C.,  Makhoul J., Schwartz R., Yuan N., Zavaliagkos  G., and Zhao Y. (1995) The 1994 BBN/BYBLOS  Speech Recognition System, In Proc of ARPA  Spoken Language Systems Technology Workshop,  Austin, Texas, pp. 77-81.  Stallard D. (1995) The Initial Implementation of the  BBN ATIS4 Dialog System, In Proc of ARPA  Spoken Language Systems Technology Workshop,  Austin, Texas, pp. 208-211.  Miller S. and Stallard D. (i996) A Fully Statistical  Approach to Natural Language Interfaces, In Proc  of the 34 th Annual Meeting of the Association for  Computational Linguistics, Santa Cruz, California.  76  \n",
            "-----------------------------\n",
            "--- document 1: --- document 1: Plan-Based Dialogue Management in a Physics Tutor  Reva Freedman  Learning Research and Development Center  University of Pittsburgh  Pittsburgh, PA 15260  freedrk+@pitt, edu  http://www.pitt, edu/~freedrk  Abstract  This paper describes an application of APE (the  Atlas Planning Engine), an integrated planning and  execution system at the heart of the Atlas dialogue  management system. APE controls a mixed-  initiative dialogue between a human user and a  host system, where turns in the 'conversation' may  include graphical actions and/or written text. APE  has full unification and can handle arbitrarily  nested discourse constructs, making it more  powerful than dialogue managers based on finite-  state machines. We illustrate this work by  describing Atlas-Andes, an intelligent tutoring  system built using APE with the Andes physics  tutor as the host.  1 Introduction  The purpose of the Atlas project is to enlarge the  scope of student interaction in an intelligent  tutoring system (ITS) to include coherent  conversational sequences, including both written  text and GUI actions. A key component of Atlas  is APE, the Atlas Planning Engine, a \"just-in-  time\" planner specialized for easy construction  and quick generation of hierarchically organized  dialogues. APE is a domain- and task-independent  system. Although to date we have used APE as a  dialogue manager for intelligent tutoring systems,  APE could also be used to manage other types of  human-computer conversation, such as an advice-  giving system or an interactive help system.  Planning is an essential component of a  dialogue-based ITS. Although there are many  reasons for using \u001b[1;32;40m natural \u001b[0;0m anguage in an ITS, as  soon as the student gives an unexpected response  to a tutor question, the tutor needs to be able to  This research was supported by NSF grant number  9720359 to CIRCLE, the Center for Interdisciplinary  Research on Constructive Learning Environments atthe  University of Pittsburgh and Carnegie-Mellon  University.  plan in order to achieve its goals as well as  respond appropriately to the student's tatement.  Yet classical planning is inappropriate for  dialogue generation precisely because it assumes  an unchanging world. A more appropriate  approach is the \"practical reason\" approach  pioneered by Bratman (1987, 1990). According to  Bratman, human beings maintain plans and prefer  to follow them, but they are also capable of  changing the plans on the fly when needed.  Bratman's approach has been introduced into  computer science under the name of reactive  planning (Georgeff and Ingrand 1989, Wilkins et  al. 1995).  In this paper we discuss the rationale for the use  of reactive planning as well as the use of the  hierarchical task network (HTN) style of plan  operators. Then we describe APE (the Atlas  Planning Engine), a dialogue planner we have  implemented to embody the above concepts. We  demonstrate he use of APE by showing how we  have used it to add a dialogue capability to an  existing ITS, the Andes physics tutor. By showing  dialogues that Atlas-Andes can generate, we  demonstrate the advantages of this architecture  over the finite-state machine approach to dialogue  management.  2 Integrated planning and execution for  dialogue generation  2.1 'Practical reason' and the BDI model  For an ITS, planning is required in order to ensure  a coherent conversation as well as to accomplish  tutorial goals. But it is impossible to plan a whole  conversation in advance when the student can  respond freely at every turn, just as human beings  cannot plan their daily lives in advance because of  possible changes in conditions. Classical planning  algorithms are inappropriate because the tutor  must be able to change plans based on the  52  student's responses.  For this reason we have adopted the ideas of the  philosopher Michael Bratman (1987, 1990).  Bratman uses the term \"practical reason\" to  describe his analysis since he is concerned with  how to reason about practical matters. For human  beings, planning is required in order to  accomplish one's goals. Bratman's key insight is  that human beings tend to follow a plan once they  have one, although they are capable of dropping  an intention or changing a partial plan when  necessary. In other words, human beings do not  decide what to do from scratch at each turn.  Bratman and others who have adopted his  approach use a tripartite mental model that  includes beliefs, desires and intentions (Bratman,  Israel and Pollack 1988, Pollack 1992, Georgeff  et al. 1998), hence the name \"BDI model.\"  Beliefs, which are uninstantiated plans in the  speaker's head, are reified by the plan library.  Desires are expressed as the agent's goals.  Intentions, or plan steps that the agent has  committed to but not yet acted on, are stored in an  agenda. Thus the agent's partial plan for  achieving a goal is a network of intentions. A plan  can be left in a partially expanded state until it is  necessary to refine it further.  2.2 Implementation via reactive planning  Bratman's approach has been elaborated in a  computer science context by subsequent  researchers (Bratman, Israel and Pollack 1988,  Pollack 1992, Georgeff et al. 1998). Reactive  planning (Georgeff and Ingrand 1989, Wilkins et  al. 1995), originally known as \"integrated  planning and execution,\" is one way of  implementing Bratman's model. Originally  developed for real-time control of the space  shuttle, reactive planning has since been used in a  variety of other domains. For the Atlas project we  have developed a reactive planner called APE  (Atlas Planning Engine) which uses these ideas to  conduct a conversation. After each student  response, the planner can choose to continue with  its previous intention or change something in the  plan to respond better to the student's utterance.  Like most reactive planners, APE is a  hierarchical task network (HTN) style planner  (Yang 1990, Erol, Hendler and Nau 1994).  Hierarchical decomposition asserts that each goal  can be achieved via a series of subgoals instead of  relying on means-end reasoning. Hierarchical  decomposition is more appropriate to dialogue  generation for a number of reasons. First,  decomposition is better suited to the type of large-  scale dialogue planning required in a real-world  tutoring system, as it is easier to establish what a  human speaker will say in a given situation than  to be able to understand why in sufficient detail  and generality to do means-end planning. Second,  Hierarchical decomposition minimizes search  time. Third, our dialogues are task-oriented and  have a hierarchical structure (Grosz and Sidner  1986). In such a case, matching the structure of  the domain simplifies operator development  because they can often be derived from transcripts  of human tutoring sessions. The hierarchy  information is also useful in determining  appropriate referring expressions. Fourth, inter-  leaved planning and execution is important for  dialogue generation because we cannot predict he  human user's future utterances. In an HTN-based  system, it is straightforward to implement  interleaved planning and execution because one  only needs to expand the portion of the plan that  is about to be executed. Finally, the conversation  is in a certain sense the trace of the plan. In other  words, we care much more about the actions  generated by the planner than the states involved,  whether implicitly or explicitly specified.  Hierarchical decomposition provides this trace  \u001b[1;32;40m natural \u001b[0;0mly.  3 Background: the Andes physics tutor  Andes (Gertner, Conati and VanLehn 1998) is an  intelligent tutoring system in the domain of first-  year college physics. Andes teaches via coached  problem solving (VanLehn 1996). In coached  problem solving, the tutoring system tracks the  student as the latter attempts to solve a problem.  If the student gets stuck or deviates too far from a  correct solution path, the tutoring system provides  hints and other assistance.  A sample Andes problem is shown in mid-  solution in Figure 1. A physics problem is given  in the upper-left corner with a picture below it.  Next to the picture the student has begun to  sketch the vectors involved using the GUI buttons  along the left-hand edge of the screen. As the  53  student draws vectors, Andes and the student  cooperatively fill in the variable definitions in the  upper-right corner. Later the student will use the  space below to write equations connecting the  variables.  In this example, the elevator is decelerating, so  the acceleration vector should face the opposite  direction from the velocity vector. (If the  acceleration vector went the same direction as the  velocity vector, the speed of the elevator would  increase and it would crash into the ground.) This  is an important issue in beginning physics; it  occurs in five Andes problems.  When such errors occur, Andes turns the  incorrect item red and provides hints to students  in the lower-left corner of the screen. A sample of  these hints, shown in the order a student would  encounter them, is shown in Fig. 2. But hints are  an output-only form of \u001b[1;32;40m natural \u001b[0;0m \u001b[1;32;40m language \u001b[0;0m; the  student can't take the initiative or ask a question.  In addition, there is no way for the system to ask  the student a question or lead the student through  a multi-step directed line of reasoning. Thus there  is no way to use some of the effective rhetorical  methods used by skilled human tutors, such as  analogy and reductio ad absurdum. Current  psychological research suggests that active  methods, where students have to answer  questions, will improve the performance of  tutoring systems.  4 Structure of the Atlas Planning Engine  Figure3 shows a sample plan operator. For  legibility, the key elements have been rendered in  English instead of in Lisp. The hiercx slot  provides a way for the planner to be aware of the  context in which a decomposition is proposed.  Items in the hiercx slot are instantiated and added  to the transient database only so long as the  operator which spawned them is in the agenda.  To initiate a planning session, the user invokes  the planner with an initial goal. The system  searches the operator library to find all operators  whose goal field matches the next goal on the  agenda and whose filter conditions and precon-  An elevator slows to a stop from an initial downward velocity  of 10.0 m\\]s in 2.00 seconds. A passenger in the elevator is  holding a 3.00 kilogram package by a vertical string.  What is the tension in the string during the process?  i ........ ii ....... i i i   . I Y  ~TO e',ev~o, at 10 m/s  elev~or at a stop  mass of p~:w'.,I,,~  magnitude of the inst~~taneous Velocity of pack, age ~ {rkne TO v._w  magnitude of the avelage Acceleratiorl of package ,dudng TO... a._x  v_v  a~  - - I   pkg  Figure I: Screen shot of the Andes physics tutor  54  S: (draws acceleration vector in same direction as velocity)  T: Wrong.  S: What's wrongwith that?  T: Think about he direction of the acceleration vector.  S: Please explain further.  T: Remember that the direction of acceleration is the direction of the change in velocity.  S: Please explain further.  T: The'direction of the acceleration vector is straight up.  S: (draws acceleration vector correctly)  Figure 2: Andes hint sequence formatted as dialogue  ditions are satisfied. Goals are represented in  first-order logic without quantifiers and matched  via unification. Since APE is intended especially  for generation of hierarchically organized task-  oriented iscourse, each operator has a multi-step  recipe in the style of Wilkins (1988). When a  match is found, the matching goal is removed  from the agenda and is replaced by the steps in  the recipe. APE has two kinds of primitive  actions; one ends a turn and the other doesn't.  From the point of view of discourse generation,  the most important APE recipe items are those  allowing the planner to change the agenda when  necessary. These three types of recipe items make  APE more powerful than a classical planner.   Fact: Evaluate a condition. If false, skip the  rest of the recipe. Fact is used to allow run-time  decision making by bypassing the rest of an  operator when circumstances change during its  execution. Fact can be used with retry-at to  implement a loop just as in Prolog.   Retry-at. The purpose of retry-at is to allow  the planner to back up to a choice point and make  a new decision. It removes goals sequentially  from the top of the agenda, a full operator at a  time, until the supplied argument is false. Then it  restores the parent goal of the last operator  removed, so that further planning can choose a  new way to achieve it. Retry-at implements a Prolog-like choice of alternatives, but it differs  from backtracking in that the new operator is  chosen based on conditions that apply when the  retry operation is executed, rather than on a list of  possible operators formed when the original  operator was chosen. For retry-at o be useful, the  author must provide multiple operators for the  same goal. Each operator must have a set of  preconditions enabling it to be chosen at the  appropriate ime.   Prune-replace: The intent of prune-replace is  (de f -operator  hand le -same-d i rec t ion   :goal  (...)  : f i l te r  ()  :p recond (...)  ; We have  asked  a quest ion  about  acce le ra t ion   ; ... and the s tudent  has g iven  an answer   ; ... f rom wh ich  we can deduce  that  s /he  th inks  accel ,  and ve loc i ty  go in  ; the same d i rec t ion   ; and  we have  not  g iven  the exp lanat ion  be low yet   : rec ipe  (...)  ; Te l l  the s tudent :  \"But i f  the acce le ra t ion  went  the same  d i rec t ion  as the ve loc i ty ,  then the e levator  wou ld  be speed ing  up.\"   ; Mark  that  we are g iv ing  th is  exp lanat ion   ; Te l l  the s tudent  that  tu tor  is request ing  another  answer  (\"Try aga in . \" )   ; Ed i t  the agenda (us ing prune-replace) so that  respond ing  to another   answer  is at the top  of the agenda  :h ie rcx  ())  Figure 3: Sample plan operator  55  to allow the planner to remove goals from the  agenda based on a change in circumstances. It  removes goals sequentially from the top of the  agenda, one at a time, until the supplied argument  becomes false. Then it replaces the removed goals  with an optional ist of new goals. Prune-replace  allows a type of decision-making frequently used  in dialogue generation. When a conversation  partner does not give the expected response, one  would often like to remove the next goal from the  agenda and replace it with one or more  replacement goals. Prune-replace implements a generalized version of this concept.  APE is domain-independent a d communicates  with a host system via an API. As a partner in a  dialogue, it needs to obtain information from the  world as well as produce output turns.  Preconditions on plan operators can be used to  access information from external knowledge  sources. APE contains a recipe item type that can  be used to execute an external program such as a  call to a GUI interface. APE also has recipe items  allowing the user to assert and retract facts in a  knowledge base. Further details about the APE  planner can be found in (Freedman, 2000).  5 Implementation of At las-Andes  5.1 Architecture of Atlas-Andes  The first system we have implemented with APE  is a prototype Atlas-Andes system that replaces  the hints usually given for an incorrect  acceleration vector by a choice of generated  subdialogues. Figure 4 shows the architecture of  Atlas-Andes; any other system built with APE  would look similar. Robust \u001b[1;32;40m natural \u001b[0;0m \u001b[1;32;40m language \u001b[0;0m  understanding in Atlas-Andes is provided by  Ros6's CARMEL system (Ros6 2000); it uses the  spelling correction algorithm devised by Elmi and  Evens (1998).  5.2 Structure of human tutorial dialogues  In an earlier analysis (Kim, Freedman and Evens  1998) we showed that a significant portion of  human-human tutorial dialogues can be modeled  with the hierarchical structure of task-oriented  dialogues (Grosz and Sidner 1986). Furthermore,  a main building block of the discourse hierarchy,  corresponding to the transaction level in  Conversation Analysis (Sinclair and Coulthard  1975), matches the tutoring episode defined by  VanLehn et al. (1998). A tutoring episode  consists of the turns necessary to help the student  make one correct entry on the interface.  NLU  (CARMEL) Plan Library  User APE  < Interface I  I I  GUI Transient  Interpreter Knowledge  (Andes) Base  Host  (Andes)  Figure 4: Interface between Atlas and host system  56  To obtain empirical data for the Atlas-Andes  plan operators, we analyzed portions of a corpus  of human tutors helping students olve similar  physics problems. Two experienced tutors were  used. Tutor A was a graduate student in computer  science who had majored in physics; tutor B was  a professional physics tutor.  The complete corpus contained solutions to five  physics problems by 41 students each. We  analyzed every tutoring episode dealing with the  acceleration vector during deceleration, totaling  29 examples divided among 20 students and both  tutors. The tutors had very different styles.  Tutor A tended to provide encouragement rather  than content, making those transcripts less useful  for deriving an information-based approach.  Tutor B used an information-based approach, but  after one wrong answer tended to complete the  solution as a monologue. Largely following  tutor B's approach to sequence and content, we  isolated six ways of teaching the student about  direction of acceleration.  5.3 Sample output and evaluation  Figure 5 shows an example of text that can be  generated by the Atlas-Andes ystem, showing an  analogy-based approach to teaching this content.  The operator library used to generate this text  could generate a combinatorially arge number of  versions of this dialogue as well as selected  examples of other ways of teaching about  direction of acceleration.  This operator library used to generate this text  contained 1l 1 plan operators, divided as follows:  Tutoring schemata  Switching between schemata  API and GUI handling  Answer handling  Domain-dep. lex. insertion  Domain-indep. lex. insertion  TOTAL  4 4%  5 4%  33 30%  35 31%  24 22%  10 9%  111 100%  We are currently working on components hat will  allow us to increase the number of physics  concepts covered without a corresponding  increase in the number of operators. The schema  switching operators prevent the tutor from  repeating itself during a physics problem. They  could be reduced or eliminated by a general  discourse history component that tutoring schema  operators could refer to. Domain-dependent  lexical insertion refers to the choice of lexical  items such as car and east in the sample dialogue,  while domain-independent iexical insertion refers  to items such as OK and exactly. Both categories  could be eliminated, or at least severely reduced,  through the use of a text realization package.  Together that would provide a one-third reduction  in the number of operators needed. As the set of  API and GUI handling operators is fixed, that  would reduce by half the number of application  operators needed.  The largest remaining category of operators is  the answer handlers. These operators handle a  variety of answers for each of the five questions  that the system can ask. The answers we  recognize include categories such as \"don't  know\" as well as specific answers (e.g. a direction  perpendicular to the correct answer) which we  recognize because the tutor has specific replies  for them. In order to reduce the number of  S: (draws acceleration vector in same direction as velocity)  T: What is the definition of acceleration?  S: Don't know.  T: OK, let's try this. If a car was driving along east, which way would you have to push on it  to make it stop?  S: West.  T: Exactly. The opposite direction. So the net force goes the opposite direction, and so does  the acceleration. Try to draw the acceleration vector again now.  S: (draws acceleration vector correctly)  Figure 5: Example of generated ialogue  57  operators further, we must investigate more  general methods of handling student errors. In  particular, we plan to investigate error-classifying  predicates that apply to more than one question as  well as the use of intention-based predicates.  Since the system only covers one rule of physics,  albeit in a variety of ways, we plan to make some  of these efficiency improvements before adding  new rules of physics and testing it with users.  Preconditions for the operators in the plan  library utilize discourse or interaction history, the  current goal hierarchy, recent information such as  the tutor's current goal and the student's latest  response, shared information such as a model of  objects on the screen, and domain knowledge. As  an example of the latter, if the student draws an  acceleration vector which is incorrect but not  opposite to the velocity vector, a different  response will be generated.  5.4 Discussion  Many previous dialogue-based ITSs have been  implemented with finite-state machines, either  simple or augmented. In the most common finite  state mode\\[, each time the human user issues an  utterance, the processor educes it to one of a  small number of categories. These categories  represent the possible transitions between states.  Thus history can be stored, and context  considered, only by expanding the number of  states. This approach puts an arbitrary restriction  on the amount of context or depth of  conversational nesting that can be considered.  More importantly, it misses the significant  generalization that these types of dialogues are  hierarchical: larger units contain repeated  instances of the same smaller units in different  sequences and instantiated with different values.  Furthermore, the finite-state machine approach  does not allow the author to drop one line of  attack and replace it by another without hard-  coding every possible transition.  It is also clear that the dialogue-based approach  has many benefits over the hint-sequence  approach. In addition to providing a multi-step  teaching methods with new content, it can  respond flexibly to a variety of student answers at  each step and take context into account when  generating a reply.  6 Related work   Wenger (1987), still the chief textbook on ITSs,  states that using a global planner to control an ITS  is too inefficient to try. This is no longer true, if  indeed it ever was. Vassileva (1995) proposes a  system based on AND-OR graphs with a separate  set of rules for reacting to unexpected events.  Lehuen, Nicolle and Luzzati (1996) present a  method of dialogue analysis that produces  schemata very similar to ours. Earlier dialogue-  based ITSs that use augmented finite-state  machines or equivalent include CIRCSIM-Tutor  (Woo et al. 1991, Zhouet al. 1999) and the  system described by Woolf (1984). Cook (1998)  uses levels of finite-state machines. None of these  systems provides for predicates with variables or  unification.  7 Conclusions  In this paper we described APE, an integrated  planner and execution system that we have  implemented as part of the Atlas dialogue  manager. APE uses HTN-style operators and is  based on reactive planning concepts. Although  APE is intended largely for use in domains with  hierarchical, multi-turn plans, it can be used to  implement any conversation-based system, where  turns in the 'conversation' may include graphical  actions and/or text. We illustrated the use of APE  with an example from the Atlas-Andes physics  tutor. We showed that previous models based on  finite-state machines are insufficient to handle the  nested subdialogues and abandoned partial  subdialogues that occur in practical applications.  We showed how APE generated a sample  dialogue that earlier systems could not handle.  Acknowledgments  We thank Abigail Gertner for her generous  assistance with the Andes system, and Michael  Ringenberg for indispensible programming  support. Carolyn Ros6 built the CARMEL  \u001b[1;32;40m natural \u001b[0;0m \u001b[1;32;40m language \u001b[0;0m understanding component.  Mohammed EImi and Michael Glass of Illinois  Institute of Technology provided the spelling  correction code. We thank Pamela Jordan and the  referees for their comments.  B8  References  Bratman, M. E. 1987. Intentions, Plans, and Practical  Reason. Cambridge, MA: Harvard.  Bratman, M. E. 1990. What is Intention? In P.R.  Cohen, J. Morgan and M. E. Pollack, Intentions in  Communication. Cambridge, MA: MIT Press.  Bratman, M. E., Israel, D. J. and Pollack, M.E. 1988.  Plans and Resource-Bounded Practical Reasoning.  Computational Intelligence 4(4): 349-355.  Cook, J. 1998. Knowledge Mentoring as a Framework  for Designing Computer-Based Agents for Sup-  porting Musical Composition Learning. PhD. diss.,  Computing Department, The Open University.  EImi, M.A. and Evens, M.W. 1998. Spelling  Correction using Context. In Proceedings of the 17th  COLING/36th ACL (COLING-ACL '98), Montreal.  Erol, K., Hendler, J. and Nau, D.S. 1994. HTN  Planning: Complexity and Expressivity. In  Proceedings of the Twelfth National Conference on  Artificial Intelligence (AAAI '94), Seattle.  Freedman, R. 2000 (to appear). Using a Reactive  Planner as the Basis for a Dialogue Agent. In  Proceedings of the Thirteenth Florida Artificial  Intelligence Research Symposium (FLAIRS'00),  Orlando.  Gertner, A.S., Conati, C. and VanLehn, K. 1998.  Procedural Help in Andes: Generating Hints Using a  Bayesian Network Student Model. In Proceedings of  the Fifteenth National Conference on Artificial  Intelligence (AAAI '98), Madison.  Georgeff, M. P. and Ingrand, F. F. 1989. Decision-  Making in an Embedded Reasoning System. In  Proceedings of the Eleventh International Joint  Conference on Artificial Intelligence (IJCAI '89),  Detroit.  Georgeff, M.P., Pell, B., Pollack, M. E., Tambe, M.  and Wooldridge, M. 1998. The Belief-Desire-  Intention Model of Agency. In N. Jenning, J. Muller,  and M. Wooldridge (Eds.), Intelligent Agents V.  Springer.  Grosz, B.J. and Sidner, C.L. 1986. Attention,  Intentions, and the Structure of Discourse.  Computational Linguistics 12(3): 175-204.  Kim, J., Freedman, R. and Evens, M. 1998.  Responding to Unexpected Student Utterances in  CIRCSIM-Tutor v. 3: Analysis of Transcripts. In  Proceedings of the Eleventh Florida Artificial  Intelligence Research Symposium (FLAIRS '98),  Sanibel Island.  Lehuen, J., Nicolle, A. and Luzzati, D. 1996. Un  mod61e hypoth6tico-exp6rimental dynamique pour la  gestion des dialogues homme-machine. In Actes du  dixi6me congr6s de reconnaissance d s formes et  intelligence artificielle (RFIA '96), Rennes.  Pollack, M.E. 1992. The Uses of Plans. Artificial  Intelligence 57(1): 43-69.  Ros6, C. P. 2000. A Framework for Robust Semantic  Interpretation. In Proceedings of the First Annual  Conference of the North American Chapter of the  Association for Computational Linguistics  (NAACL '00).  Sinclair, J. M. and Coulthard, R. M. 1975. Towards an  Analysis of Discourse: The English Used by  Teachers and Pupils. London: Oxford University  Press.  VanLehn, K. 1996. Conceptual and Meta Learning  during Coached Problem Solving. In Intelligent  Tutoring Systems.\" Third International Conference  (ITS '96), Montreal. Berlin: Springer. LNCS 1086.  VanLehn, K., Siler, S., Murray, C. and Baggett, W.  1998. What Makes a Tutorial Event Effective? In  Proceedings of the Twenty-first Annual Conference  of the Cognitive Science Society, Madison. Hillsdale,  N J: Erlbaum.  Vassileva, J. 1995. Reactive Instructional Planning to  Support Interacting Teaching Strategies. In  Proceedings of the Seventh World Conference on AI  and Education (AI-ED '95), Washington, D.C.  Charlottesville, VA: AACE.  Wenger, E. 1987. Artificial Intelligence and Tutoring  Systems.\" Computational nd Cognitive Approaches  to the Communication of Knowledge. San Mateo,  CA: Morgan Kaufmann.  Wilkins, D. 1988. Practical Planning: Extending the  Classical AI Planning Paradigm. San Mateo, CA:  Morgan Kaufmann.  Wilkins, D., Myers, K., Lowrance, J. and Wesley, L.  1995. Planning and Reacting in Uncertain and  Dynamic Environments. Journal of Experimental  and Theoretical Artificial Intelligence 7:121-152.  Woo, C., Evens, M.W., Michael, J.A. and Rovick,  A.A. 1991. Dynamic Instructional Planning for an  Intelligent Physiology Tutoring System. In  Proceedings of the Fourth Annual 1EEE Computer-  Based Medical Systems Symposium, Baltimore.  Woolf, B. 1984. Context-Dependent Planning in a  Machine Tutor. Ph.D. diss., Dept. of Computer and  Information Science, University of Massachusetts at  Amherst. COINS Technical Report 84-21.  Yang, Q. 1990. Formalizing planning knowledge for  hierarchical planning. Computational Intelligence  6(I): 12-24.  Zhou, Y., Freedman, R., Glass, M., Michael, J.A.,  Rovick, A.A. and Evens, M.W. 1999. Delivering  Hints in a Dialogue-Based Intelligent Tutoring  System. In Proceedings of the Sixteenth National  Conference on Artificial Intelligence (AAAI '99),  Orlando, FL.  59  \n",
            "-----------------------------\n",
            "--- document 2: --- document 2: Machine Translation of Very Close Languages  Jan HAJI(~  Computer Science Dept.  Johns Hopkins University  3400 N. Charles St., Baltimore,  MD 21218, USA  hajic@cs.jhu.edu  Jan HRIC  KTI MFF UK  Malostransk6 nfim.25  Praha 1, Czech Republic, 11800  hric@barbora.m ff.cuni.cz  Vladislav KUBON  OFAL MFF UK  Malostransk6 mim.25  Praha 1, Czech Republic, 11800  vk@ufal.mff.cuni.cz  Abstract  Using examples of the transfer-based MT  system between Czech and Russian  RUSLAN and the word-for-word MT system  with morphological disambiguation between  Czech and Slovak (~ESILKO we argue that  for really close \u001b[1;32;40m language \u001b[0;0ms it is possible to  obtain better translation quality by means of  simpler methods. The problem of translation  to a group of typologically similar \u001b[1;32;40m language \u001b[0;0ms  using a pivot \u001b[1;32;40m language \u001b[0;0m is also discussed here.  Introduction  Although the field of machine translation has a  very long history, the number of really successful  systems is not very impressive. Most of the funds  invested into the development of various MT  systems have been wasted and have not  stimulated a development of techniques which  would allow to translate at least technical texts  from a certain limited domain. There were, of  course, exceptions, which demonstrated that  under certain conditions it is possible to develop  a system which will save money and efforts  invested into human translation. The main reason  why the field of MT has not met the expectations  of sci-fi literature, but also the expectations of  scientific community, is the complexity of the  task itself. A successful automatic translation  system requires an application of techniques from  several areas of computational inguistics  (morphology, syntax, semantics, discourse  analysis etc.) as a necessary, but not a sufficient  condition. The general opinion is that it is easier  to create an MT system for a pair of related  \u001b[1;32;40m language \u001b[0;0ms. In our contribution we would like to  demonstrate hat this assumption holds only for  really very closely related \u001b[1;32;40m language \u001b[0;0ms.  1. Czech-to-Russian MT system RUSLAN  1.1 History  The first attempt o verify the hypothesis that  related \u001b[1;32;40m language \u001b[0;0ms are easier to translate started in  mid 80s at Charles University in Prague. The  project was called RUSLAN and aimed at the  translation of documentation i the domain of  operating systems for mainframe computers. It  was developed in cooperation with the Research  Institute of Mathematical Machines in Prague. At  that time in former COMECON countries it was  obligatory to translate any kind of documentation  to such systems into Russian. The work on the  Czech-to-Russian MT system RUSLAN (cf. Oliva  (1989)) started in 1985. It was terminated in 1990  (with COMECON gone) for the lack of funding.  1.2 System description  The system was rule-based, implemented in  Colmerauer's Q-systems. It contained a full-  fledged morphological and syntactic analysis of  Czech, a transfer and a syntactic and  morphological generation of Russian. There was  almost no transfer at the beginning of the project  due to the assumption that both \u001b[1;32;40m language \u001b[0;0ms are  similar to the extent that does not require any  transfer phase at all. This assumption turned to be  wrong and several phenomena were covered by  the transfer in the later stage of the project (for  example the translation of the Czech verb \"b~\"  \\[to be\\] into one of the three possible Russian  equivalents: empty form, the form \"byt6\" in future  7  tense and the verb \"javljat6sja\"; or the translation  of verbal negation).  At the time when the work was terminated in  1990, the system had a main translation  dictionary of about 8000 words, accompanied by  so called transducing dictionary covering another  2000 words. The transducing dictionary was  based on the original idea described in Kirschner  (1987). It aimed at the exploitation of the fact  that technical terms are based (in a majority of  European \u001b[1;32;40m language \u001b[0;0ms) on Greek or Latin stems,  adopted according to the particular derivational  rules of the given \u001b[1;32;40m language \u001b[0;0ms. This fact allows for  the \"translation\" of technical terms by means of a  direct transcription of productive ndings and a  slight (regular) adjustment of the spelling of the  stem. For example, the English words  localization and discrimination can be  transcribed into Czech as \"lokalizace\" and  \"diskriminace\" with a productive nding -ation  being transcribed to -ace. It was generally  assumed that for the pair Czech/Russian the  transducing dictionary would be able to profit  from a substantially greater number of productive  rules. This hypothesis proved to be wrong, too  (see B6mov~, Kubofi (1990)). The set of  productive ndings for both pairs (English/Czech,  as developed for an earlier MT system from  English to Czech, and Czech/Russian) was very  similar.  The evaluation of results of RUSLAN showed  that roughly 40% of input sentences were  translated correctly, about 40% with minor errors  correctable by a human post-editor and about  20% of the input required substantial editing or  re-translation. There were two main factors that  caused a deterioration of the translation. The first  factor was the incompleteness of the main  dictionary of the system. Even though the system  contained a set of so-called fail-soft rules, whose  task was to handle such situations, an unknown  word typically caused a failure of the module of   syntactic analysis, because the dictionary entries  contained - besides the translation equivalents  and morphological information - very important  syntactic information.  The second factor was the module of syntactic  analysis of Czech. There were several reasons of  parsing failures. Apart from the common inability  of most rule-based formal grammars to cover a  particular \u001b[1;32;40m natural \u001b[0;0m anguage to the finest detail of  its syntax there were other problems. One of  them  was the existence of non-projective constructions,  which are quite common in Czech even in  relatively short sentences. Even though they  account only for 1.7/'o f syntactic dependencies,  every third Czech sentence contains at least one,  and in a news corpus, we discovered as much as  15 non-projective dependencies; see also Haji6 et  al. (1998). An example of a non-projective  construction is \"Soubor se nepodafilo otev~it.\"  \\[lit.: File Refl. was_not._possible to_open. - It was  not possible to open the file\\]. The formalism used  for the implementation (Q-systems) was not meant  to handle non-projective constructions. Another  source of trouble was the use of so-called  semantic features. These features were based on  lexical semantics of individual words. Their main  task was to support a semantically plausible  analysis and to block the implausible ones. It  turned out that the question of implausible  combinations of  semantic features is also more  complex than it was supposed to be. The practical  outcome of the use of semantic features was a  higher atio of parsing failures - semantic features  often blocked a plausible analysis. For example,  human lexicographers a signed the verb 'to run' a  semantic feature stating that only a noun with  semantic features of a human or other living being  may be assigned the role of subject of this verb.  The input text was however full of sentences with  'programs' or 'systems' running etc. It was of  course very easy to correct he semantic feature in  the dictionary, but the problem was that there  were far too many corrections required.  On the other hand, the fact that both \u001b[1;32;40m language \u001b[0;0ms  allow a high degree of word-order freedom  accounted for a certain simplification of  the  translation process. The grammar elied on the  fact that there are only minor word-order  differences between Czech and Russian.  1.3 Lessons learned  f rom RUSLAN  We have learned several lessons regarding the MT  of closely related \u001b[1;32;40m language \u001b[0;0ms:   The transfer-based approach provides a  similar quality of translation both for closely  related and typologically different \u001b[1;32;40m language \u001b[0;0ms   Two main bottlenecks of full-fledged  transfer-based systems are:  8  - complexity of the syntactic dictionary  - relative unreliability of the syntactic  analysis of the source \u001b[1;32;40m language \u001b[0;0m  Even a relatively simple component  (transducing dictionary) was equally complex  for English-to-Czech and Czech-to-Russian  translation  Limited text domains do not exist in real life,  it is necessary to work with a high coverage  dictionary at least for the source \u001b[1;32;40m language \u001b[0;0m.  2. Translation and localization  2.1 A pivot \u001b[1;32;40m language \u001b[0;0m  Localization of products and their documentation  is a great problem for any company, which wants  to strengthen its position on foreign \u001b[1;32;40m language \u001b[0;0m  market, especially for companies producing  various kinds of  software. The amounts of texts  being localized are huge and the localization  costs are huge as well.  It is quite clear that the localization from one  source \u001b[1;32;40m language \u001b[0;0m to several target \u001b[1;32;40m language \u001b[0;0ms,  which are typologically similar, but different  from the source \u001b[1;32;40m language \u001b[0;0m, is a waste of money  and effort. It is of course much easier to translate  texts from Czech to Polish or from Russian to  Bulgarian than from English or German to any of  these \u001b[1;32;40m language \u001b[0;0ms. There are several reasons, why  localization and translation is not being  performed through some pivot \u001b[1;32;40m language \u001b[0;0m,  representing a certain group of closely related  \u001b[1;32;40m language \u001b[0;0ms. Apart from political reasons the  translation through a pivot \u001b[1;32;40m language \u001b[0;0m has several  drawbacks. The most important one is the  problem of the loss of translation quality. Each  translation may to a certain extent shift the  meaning of the translated text and thus each  subsequent translation provides results more and  more different from the original. The second  most important reason is the lack of translators  from the pivot to the target \u001b[1;32;40m language \u001b[0;0m, while this is  usually no problem for the translation from the  source directly to the target \u001b[1;32;40m language \u001b[0;0m.  2.2 Translation memory is the key  The main goal of this paper is to suggest how to  overcome these obstacles by means of a  combination of an MT system with commercial  MAHT (Machine-aided human translation)  systems. We have chosen the TRADOS  Translator's Workbench as a representative  system of a class of these products, which can be  characterized as an example-based translation  tools. IBM's Translation Manager and other  products also belong to this class. Such systems  uses so-called translation memory, which contains  pairs of previously translated sentences from a  source to a target \u001b[1;32;40m language \u001b[0;0m. When a human  translator starts translating a new sentence, the  system tries to match the source with sentences  already stored in the translation memory. If it is  successful, it suggests the translation and the  human translator decides whether to use it, to  modify it or to reject it.  The segmentation f a translation memory is a key  feature for our system. The translation memory  may be exported into a text file and thus allows  easy manipulation with its content. Let us suppose  that we have at our disposal two translation  memories - one human made for the source/pivot  \u001b[1;32;40m language \u001b[0;0m pair and the other created by an MT  system for the pivot/target \u001b[1;32;40m language \u001b[0;0m pair. The  substitution of segments of a pivot \u001b[1;32;40m language \u001b[0;0m by  the segments of a target \u001b[1;32;40m language \u001b[0;0m is then only a  routine procedure. The human translator  translating from the source \u001b[1;32;40m language \u001b[0;0m to the target  \u001b[1;32;40m language \u001b[0;0m then gets a translation memory for the  required pair (source/target). The system of  penalties applied in TRADOS Translator's  Workbench (or a similar system) guarantees that if  there is already a human-made translation present,  then it gets higher priority than the translation  obtained as a result of the automatic MT. This  system solves both problems mentioned above -  the human translators from the pivot to the target  \u001b[1;32;40m language \u001b[0;0m are not needed at all and the machine-  made translation memory serves only as a  resource supporting the direct human translation  from the source to the target \u001b[1;32;40m language \u001b[0;0m.  3. Mach ine  t rans lat ion of  (very) closely  related Slavic \u001b[1;32;40m language \u001b[0;0ms  In the group of Slavic \u001b[1;32;40m language \u001b[0;0ms, there are more  closely related \u001b[1;32;40m language \u001b[0;0ms than Czech and Russian.  Apart from the pair of Serbian and Croatian  \u001b[1;32;40m language \u001b[0;0ms, which are almost identical and were  9  considered one \u001b[1;32;40m language \u001b[0;0m just a few years ago, the  most closely related \u001b[1;32;40m language \u001b[0;0ms in this group are  Czech and Slovak.  This fact has led us to an experiment with  automatic translation between Czech and Slovak.  It was clear that application of a similar method  to that one used in the system RUSLAN would  lead to similar results. Due to the closeness of  both \u001b[1;32;40m language \u001b[0;0ms we have decided to apply a  simpler method. Our new system, (~ESILKO,  aims at a maximal exploitation of the similarity  of both \u001b[1;32;40m language \u001b[0;0ms. The system uses the method of  direct word-for-word translation, justified by the  similarity of syntactic constructions of both  \u001b[1;32;40m language \u001b[0;0ms.  Although the system is currently being tested on  texts from the domain of documentation to  corporate information systems, it is not limited to  any specific domain. Its primary task is, however,  to provide support for translation and localization  of various technical texts.  3.1 System (~ESiLKO  The greatest problem of the word-for-word  translation approach (for \u001b[1;32;40m language \u001b[0;0ms with very  similar syntax and word order, but different  morphological system) is the problem of  morphological ambiguity of individual word  forms. The type of ambiguity is slightly different  in \u001b[1;32;40m language \u001b[0;0ms with a rich inflection (majority of  Slavic \u001b[1;32;40m language \u001b[0;0ms) and in \u001b[1;32;40m language \u001b[0;0ms which do not  have such a wide variety of forms derived from a  single lemma. For example, in Czech there are  only rare cases of part-of-speech ambiguities ( t~t  \\[to stay/the state\\], zena \\[woman/chasing\\] or tri  \\[three/rub(imperative)\\]), much more frequent is  the ambiguity of gender, number and case (for  example, the form of the adjective jam\\[ \\[spring\\]  is 27-times ambiguous). The main problem is that  even though several Slavic \u001b[1;32;40m language \u001b[0;0ms have the  same property as Czech, the ambiguity is not  preserved. It is distributed in a different manner  and the \"form-for-form\" translation is not  applicable.  Without he analysis of at least nominal groups it  is often very difficult to solve this problem,  because for example the actual morphemic  categories of adjectives are in Czech  distinguishable only on the basis of gender,  number and case agreement between an adjective  and its governing noun. An alternative way to the  solution of this problem was the application of a  stochastically based morphological disambiguator  (morphological tagger) for Czech whose success  rate is close to 92/'0. Our system therefore consists  of the following modules:  1. Import of the input from so-called 'empty'  translation memory  2. Morphological analysis of Czech  3. Morphological disambiguation  4. Domain-related bilingual glossaries (incl.  single- and multiword terminology)  5. General bilingual dictionary  6. Morphological synthesis of Slovak  7. Export of the output o the original translation  memory  Letus now look in a more detail at the individual  modules of the system:  ad 1. The input text is extracted out of a  translation memory previously exported into an  ASCII file. The exported translation memory (of  TRADOS) has a SGML-Iike notation with a  relatively simple structure (cf. the following  example):  Example 1. - A sample of the exported translation  memory  <RTF Preamble>...</RTF Preamble>  <TrU>  <CrD>23051999  <CrU>VK  <Seg L=CS_01>Pomoci v~kazu ad-hoc m65ete  rychle a jednoduge vytv~i~et regerge.  <Seg L=SK_01 >n/a  </TrU>  Our system uses only the segments marked by  <Seg L=CS_01>, which contain one source  \u001b[1;32;40m language \u001b[0;0m sentence ach, and <Seg L=SK_01>,  which is empty and which will later contain the  same sentence translated into the target \u001b[1;32;40m language \u001b[0;0m  by CESiLKO.  ad 2. The morphological analysis of Czech is  based on the morphological dictionary developed  by Jan Haji6 and Hana Skoumalov~i in 1988-99  (for latest description, see Haji~ (1998)). The  dictionary contains over 700 000 dictionary  entries and its typical coverage varies between  10  99% (novels) to 95% (technical texts). The  morphological analysis uses the system of  positional tags with 15 positions (each  morphological .category, such as Part-of-speech,  Number, Gender, Case, etc. has a fixed, single-  symbol place in the tag).  Example 2 - tags assigned to the word-form  \"pomoci\" (help/by means of)  pomoci:  NFP2 .... . .  A .... \\]NFS7 ...... A .... I R--2 . . . . . . . . . . .   where :  N - noun; R - preposition  F - feminine gender  S - singular, P - plural  7, 2 - case (7 - instrumental, 2 - genitive)  A - affirmative (non negative)  ad 3. The module of morphological  disambiguation is a key to the success of  the  translation. It gets an average number of 3.58  tags per token (word form in text) as an input.  The tagging system is purely statistical, and it  uses a log-linear model of probability distribution  - see Haji~, Hladkfi (1998). The learning is based  on a manually tagged corpus of Czech texts  (mostly from the general newspaper domain).  The system learns contextual rules (features)  automatically and also automatically determines  feature weights. The average accuracy of tagging  is between 91 and 93% and remains the same  even for technical texts (if we disregard the  unknown names and foreign-\u001b[1;32;40m language \u001b[0;0m t rms that  are not ambiguous anyway).  The lemmatization immediately follows tagging;  it chooses the first lemma with a possible tag  corresponding to the tag selected. Despite this  simple lemmatization method, and also thanks to  the fact that Czech words are rarely ambiguous in  their Part-of-speech, it works with an accuracy  exceeding 98%.  ad 4. The domain-related bilingual glossaries  contain pairs of individual words and pairs of  multiple-word terms. The glossaries are  organized into a hierarchy specified by the user;  typically, the glossaries for the most specific  domain are applied first. There is one general  matching rule for all levels of glossaries - the  longest match wins.  The multiple-word terms are sequences of lemmas  (not word forms). This structure has several  advantages, among others it allows to minimize  the size of the dictionary and also, due to the  simplicity of the structure, it allows modifications  of the glossaries by the linguistically naive user.  The necessary morphological information is  introduced into the domain-related glossary in an  off-line preprocessing stage, which does not  require user intervention. This makes a big  difference when compared to the RUSLAN  Czech-to-Russian MT system, when each  multiword dictionary entry cost about 30 minutes  of linguistic expert's time on average.  ad 5. The main bilingual dictionary contains data  necessary for the translation of  both lemmas and  tags. The translation of tags (from the Czech into  the Slovak morphological system) is necessary,  because due to the morphological differences both  systems use close, but slightly different tagsets.  Currently the system handles the 1:1 translation of  tags (and 2:2, 3:3, etc.). Different ratio of  translation is very rare between Czech and Siovak,  but nevertheless an advanced system of dictionary  items is under construction (for the translation 1:2,  2:1 etc.). It is quite interesting that the lexically  homonymous words often preserve their  homonymy even after the translation, so no  special treatment of homonyms is deemed  necessary.  ad 6. The morphological synthesis of Slovak is  based on a monolingual dictionary of SIovak,  developed by J.Hric (1991-99), covering more  than \\]00,000 dictionary entries. The coverage of  the dictionary is not as high as of  the Czech one,  but it is still growing. It aims at a similar coverage  of Slovak as we enjoy for Czech.  ad 7. The export of  the output of the system  (~ESILKO into the translation memory (of  TRADOS Translator's Workbench) amounts  mainly to cleaning of all irrelevant SGML  markers. The whole resulting Slovak sentence is  inserted into the appropriate location in the  original translation memory file. The following  example also shows that the marker <CrU>  contains an information that the target \u001b[1;32;40m language \u001b[0;0m  sentence was created by an MT system.  11  Example 3. -A  sample of the translation memory  containing the results of MT  <RTF Preamble>...</RTF Preamble>  <TrU>  <CRD>23051999  <CrU>MT!  <Seg L=CS_01>Pomoci v~kazu ad-hoc mfi~ete  rychle a jednodu~e vytv~i~et re,erie.  <Seg L=SK_01>Pomoci v~kazov ad-hoc m6~ete  r~chio a jednoducho vytvhrat' re,erie.  </TrU>  3.2 Evaluation of results  The problem how to evaluate results of automatic  translation is very difficult. For the evaluation of  our system we have exploited the close  connection between our system and the  TRADOS Translator's Workbench. The method  is simple - the human translator eceives the  translation memory created by our system and  translates the text using this memory. The  translator is free to make any changes to the text  proposed by the translation memory. The target  text created by a human translator is then  compared with the text created by the mechanical  application of translation memory to the source  text. TRADOS then evaluates the percentage of  matching in the same manner as it normally  evaluates the percentage of matching of source  text with sentences in translation memory. Our  system achieved about 90% match (as defined by  the TRADOS match module) with the results of  human translation, based on a relatively large  (more than 10,000 words) test sample.  4. Conclusions  The accuracy of the translation achieved by our  system justifies the hypothesis that word-for-  word translation might be a solution for MT of  really closely related \u001b[1;32;40m language \u001b[0;0ms. The remaining  problems to be solved are problems with the one-  to many or many-to-many translation, where the  lack of information in glossaries and dictionaries  sometimes causes an unnecessary translation  error.  The success of the system CESILKO has  encouraged the investigation of the possibility to  use the same method for other pairs of Slavic  \u001b[1;32;40m language \u001b[0;0ms, namely for Czech-to-Polish translation.  Although these \u001b[1;32;40m language \u001b[0;0ms are not so similar as  Czech and Slovak, we hope that an addition of a  simple partial noun phrase parsing might provide  results with the quality comparable to the full-  fledged syntactic analysis based system RUSLAN  (this is of course true also for the Czechoto-Slovak  translation). The first results of Czech-to Polish  translation are quite encouraging in this respect,  even though we could not perform as rigorous  testing as we did for Slovak.  Acknowledgements  This project was supported by the grant GAt~R  405/96/K214 and partially by the grant GA(~R  201/99/0236 and project of the Ministry of  Education No. VS96151.  References  B6movfi, Alevtina and Kubofi, Vladislav (1990). Czech-  to-Russian Transducing Dictionary; In: Proceedings  of the Xlllth COLING conference, Helsinki 1990  Haji~, Jan (1998). Building and Using a Syntactially  Annotated Coprus: The Prague Dependency  Treebank. In: Festschrifi for Jarmila Panevov~i,  Karolinum Press, Charles Universitz, Prague. pp.  106---132.  Haji~, Jan and Barbora Hladk~t (1998). Tagging  Inflective Languages. Prediction of Morphological  Categories for a Rich, Structured Tagset. ACL-  Coling'98, Montreal, Canada, August 1998, pp. 483-  490.  Haji~, Jan; Brill, Eric; Collins, Michael; Hladk~t  Barbora; Jones, Douglas; Kuo, Cynthia; Ramshaw,  Lance; Schwartz, Oren; Tillman, Christoph; and  Zeman, Daniel: Core Natural Language Processing  Technology Applicable to Multiple Languages. The  Workshop'98 Final Report. CLSP JHU. Also at:  http:llwww.clsp.jhu.edulws981projectslnlplreport.  Kirschner, Zden~k (1987). APAC3-2: An English-to-  Czech Machine Translation System; Explizite  Beschreibung der Sprache und automatische  Textbearbeitung XII1, MFF UK Prague  Oliva, Karel (1989). A Parser for Czech Implemented  in Systems Q; Explizite Beschreibung der Sprache  und automatische Textbearbeitung XVI, MFF UK  Prague  12  \n",
            "-----------------------------\n",
            "--- document 4: --- document 4: A Compact Architecture for Dialogue Management Based on  Scripts ond Meta-Outputs  MAnny Rayner ,  Beth  Ann Hockey ,  F rAnk ie  J~mes   Research Inst i tute for Advanced Computer  Science  Mail  Stop 19-39, NASA Ames Research Center   Moffett  Field, CA  94035-1000  { mauny, bahockey, l~ ames} ~r iacs.edu  Abst rac t   We describe an architecture for spoken dialogue  interfaces to semi-autonomous systems that trans-  forms speech signals through successive r presenta-  tions of linguistic, dialogue, and domain knowledge.  Each step produces an output, and a meta-output  describing the transformation, with an executable  program in a simple scripting \u001b[1;32;40m language \u001b[0;0m as the fi-  nal result. The output/meta-output distinction per-  mits perspicuous treatment of diverse tasks such as  resolving pronouns, correcting user misconceptions,  and optimizing scripts.  1 In t roduct ion   The basic task we consider in this paper is that of  using spoken \u001b[1;32;40m language \u001b[0;0m to give commands to a semi-  autonomous robot or other similar system. As ev-  idence of the importance of this task in the NLP  community note that the early, influential system  SHRDLU (Winograd, 1973) was intended to address  just this type of problem. More recent work on spo-  ken \u001b[1;32;40m language \u001b[0;0m interfaces to semi-antonomous robots  include SRrs Flakey robot (Konolige et al., 1993)  and NCARArs InterBOT project (Perzanowski et  al., 1998; Perzanowski et al., 1999). A number of  other systems have addressed part of the task. Com-  mandTalk (Moore et al., 1997), Circuit Fix-It Shop  (Smith, 1997) and TRAINS-96 (Traum and Allen,  1994; Tranm and Andersen, 1999) are spoken lan-  guage systems but they interface to simulation or  help facilities rather than semi-autonomous agents.  Jack's MOOse Lodge (Badler et al., 1999) takes text  rather than speech as \u001b[1;32;40m natural \u001b[0;0m \u001b[1;32;40m language \u001b[0;0m input and the  avatars being controlled are not semi-autonomous.  Other researchers have considered particular aspects  of the problem such as accounting for various aspects  of actions (Webber, 1995; Pyre et al., 1995). In most  of this and other related work the treatment is some  variant of the following. If there is a speech inter-  face, the input speech signal is converted into text.  Text either from the recognizer or directly input by  the user is then converted into some kind of logi-  cal formula, which abstractly represents the user's  intended command; this formula is then fed into a  command interpreter, which executes the command.  We do not think the standard treatment outlined  above is in essence incorrect, but we do believe that,  as it stands, it is in need of some modification. This  paper will in particular make three points. First, we  suggest that the output representation should not be  regarded as a logical expression, but rather as a pro-  gram in some kind of scripting \u001b[1;32;40m language \u001b[0;0m. Second, we  argue that it is not merely the case that the process  of converting the input signal to the final represen-  tation can sometimes go wrong; rather, this is the  normal course of events, and the interpretation pro-  cess should be organized with that assumption in  mind. Third, we claim, perhaps urprisingly, that  the first and second points are related. These claims  are elaborated in Section 2.  The remainder of the paper describes an archi-  tecture which addresses the issues outlined above,  and which has been used to implement a prototype  speech interface to a simulated semi-autonomous  robot intended for deployment on the International  Space Station. Sections 3 and 4 present an overview  of the implemented interface, focussing on represen-  tational issues relevant to dialogue management. Il- lustrative xamples of interactions with the system  are provided in Section 5. Section 6 concludes.  2 Theoret i ca l  Ideas   2.1 Scripts vs Logical Forms  Let's first look in a little more detail at the question  of what the output representation f a spoken lan-  guage interface to a semi-autonomous robot/agent  should be. In practice, there seem to be two main  choices: atheoreticai representations, or some kind  of logic.  Logic is indeed an excellent way to think  about representing static relationships like database  queries, but it is much less clear that it is a good way  to represent commands. In real life, when people  wish to give a command to a computer, they usu-  ally do so via its operating system; a complex com-  mand is an expression in a scripting \u001b[1;32;40m language \u001b[0;0m like  CSHELL, Perl, or VBScript. These \u001b[1;32;40m language \u001b[0;0ms are  related to logical formalisms, but cannot be mapped  112  onto them in a simple way. Here are some of the  obvious differences:   A scripting \u001b[1;32;40m language \u001b[0;0m is essentially imperative,  rather than relational.   The notion of temporal sequence is fundamental  to the \u001b[1;32;40m language \u001b[0;0m. \"Do P and then Q\" is not the  same as \"Make the goals P and Q true\"; it is  explicitly stated that P is to be done first. Simi-  larly, \"For each X in the list (A B C), do P(X)\"  is not the same as \"For all X, make P(X) true\";  once again, the scripting \u001b[1;32;40m language \u001b[0;0m defines an or:  der, but not the logical anguage 1.   Scripting \u001b[1;32;40m language \u001b[0;0ms assume that commands do  not always succeed. For example, UNIX-based  scripting \u001b[1;32;40m language \u001b[0;0ms like CSHELL provide each  script with the three predefined streams tdin,  stdout and sl;derr. Input is read from std in  and written to sCdout; error messages, warn-  ings and other comments are sent to stderr .   We do not think that these properties of scripting  \u001b[1;32;40m language \u001b[0;0m are accidental. They have evolved as the  result of strong selectional pressure from real users  with real-world tasks that need to be carried out,  and represent a competitive way to meet said users'  needs. We consequently hink it is worth taking seri-  ously the idea that a target representation produced  by a spoken \u001b[1;32;40m language \u001b[0;0m interface should share many of  these properties.  2.2 Fall|ble Interpretat ion:  Outputs  and  Meta-outputs   We now move on to the question of modelling the in-  terpretation process, that is to say the process that  converts the input (speech) signal to the output (ex-  ecutable) representation. As already indicated, we  think it is important to realize that interpretation  is a process which, like any other process, may suc-  ceed more or less well in achieving its intended goals.  Users may express themselves unclearly or incom-  pletely, or the system may more or less seriously  fail to understand exactly what they mean. A good  interpretation architecture will keep these consider-  ations in mind.  Taking our lead from the description of scripting  \u001b[1;32;40m language \u001b[0;0ms sketched above, we adapt the notion of  the \"error stream\" to the interpretation process. In  the course of interpreting an utterance, the system  translates it into successively \"deeper\" levels of rep-  resentation. Each translation step has not only an  input (the representation consumed) and an output  1In cases like these, the theorem prover or logic program-  ruing interpreter used to evaluate he logical formula typically  assigns a conventional order to the conjuncts; note however  that this is part of the procedural semantics ofthe theorem  prover/interpreter, and does not follow from the declarative  semantics of the logical formalism.  (the representation produced), but also something  we will refer to as a \"meta-output\": his provides in-  formation about how the translation was performed.  At a high level of abstraction, our architecture will  be as follows. Interpretation proceeds as a series  of non-deterministic translation steps, each produc-  ing a set of possible outputs and associated meta-  outputs. The final translation step produces an ex-  ecutable script. The interface attempts to simulate  execution of each possible script produced, in or-  der to determine what would happen if that script  were selected; simulated execution can itself produce  further meta-outputs. Finally, the system uses the  meta-output information to decide what to do with  the various possible interpretations it has produced.  Possible actions include selection and execution of  an output script, paraphrasing meta-output infor-  mation back to the user, or some combination ofthe  two.  In the following section, we present a more de-  tailed description showing how the output/meta-  output distinction works in a practical system.  3 A P ro to type  Imp lementat ion   The ideas sketched out above have been realized as  a prototype spoken \u001b[1;32;40m language \u001b[0;0m dialogue interface to a  simulated version of the Personal Satellite Assistant  (PSA; (PSA, 2000)). This section gives an overview  of the implementation; i  the following section, we  focus on the specific aspects of dialogue management  which are facilitated by the output/meta-output ar-  chitecture.  3.1 Leve ls  o f  Representat ion   The real PSA is a miniature robot currently being  developed at NASA Ames Research Center, which  is intended for deployment on the Space Shuttle  and/or International Space Station. It will be ca-  pable of free navigation in an indoor micro-gravity  environment, and will provide mobile sensory capac-  ity as a backup to a network of fixed sensors. The  PSA will primarily be controlled by voice commands  through a hand-held or head-mounted microphone,  with speech and \u001b[1;32;40m language \u001b[0;0m processing being handled  by an offboard processor. Since the speech process-  ing units are not in fact physically connected to the  PSA we envisage that they could also be used to con-  trol or monitor other environmental functions. In  particular, our simulation allows voice access to the  current and past values of the fixed sensor eadings.  The initial PSA speech interface demo consists of  a simple simulation of the Shuttle. State parame-  ters include the PSA's current position, some envi-  ronmental variables uch as local temperature, pres-  sure and carbon dioxide levels, and the status of the  Shuttle's doors (open/closed). A visual display gives  direct feedback on some of these parameters.  113  The speech and \u001b[1;32;40m language \u001b[0;0m processing architecture  is based on that of the SRI CommandTalk sys-  tem (Moore et al., 1997; Stent et al., 1999). The sys-  tem comprises a suite of about 20 agents, connected  together using the SPd Open Agent Architecture  (OAA; (Martin et al., 1998)). Speech recognition  is performed using a version of the Nuance recog-  nizer (Nuance, 2000). Initial \u001b[1;32;40m language \u001b[0;0m processing is carried out using the SRI Gemini system (Dowding  et al., 1993), using a domain~independent unification  grammar and a domain-specific lexicon. The lan-  guage processing rammar is compiled into a recog-  nition grarnm~kr using the methods of (Moore et al.,  1997); the net result is that only grammatically well-  formed utterances can be recognized. Output from  the initial \u001b[1;32;40m language \u001b[0;0m-processing step is represented  in a version of Quasi Logical Form (van Eijck and  Moore, 1992), and passed in that form to the dia-  logue manager. We refer to these as linguistic level  representations.  The aspects of the system which are of primary in-  terest here concern the dialogue manager (DM) and  related modules. Once a linguistic level represen-  tation has been produced, the following processing  steps occur:   The linguistic level representation is converted  into a discourse level representation. This pri-  marily involves regularizing differences in sur-  face form: so, for example, \"measure the pres-  sure\" and '~hat is the pressure?\" have differ-  ent representations at the linguistic level, but  the same representation at the discourse level.   If necessary, the system attempts to resolve in-  stances of ellipsis and anaph*oric reference. For  example, if the previous command was \"mea-  sure temperature at flight deck\", then the new  command \"lower deck\" will be resolved to an  expression meaning \"measure temperature at  lower deck\". Similarly, if the previous command  was \"move to the crew hatch\", then the com-  mand \"open it\" will be resolved to \"open the  crew hatch\". We call the output of this step a  resolved iscourse level representation.   The resolved discourse level representation is  converted into an executable script in a lan-  guage essentially equivalent o a subset of  CSHELL. This involves two sub-steps. First,  quantified variables are given scope: for exam-  ple, \"go to the flight deck and lower deck and  measure pressure\" becomes omething approxi-  mately equivalent to the script  foreach x ( f l ight_deck lower_deck)  go_to $x  measure  pressure  end  The point to note here is that the foreach has  scope over both the go_to and the meeusmre ac-  tions; an alternate (incorrect) scoping would be  fo reachx  ( f l ight_deck lower_deck)  go_to $x  end  measure  pressure  The second sub-step is to attempt o optimize  the plan. In the current example, this can  be done by reordering the list ( f l ight .deck  louer_deck). For instance, if the PSA is al-  ready at the lower deck, reversing the list will  mean that the robot only makes one trip, in-  stead of two.  The final step in the interpretation process is  plan evaluation: the system tries to work out  what will happen if it actually executes the  plan. (The relationship between plan evaluation  and plan execution is described in more detail  in Section 4.1). Among other things, this gives  the dialogue manager the possibility of compar-  ing different interpretations of the original com-  mand, and picking the one which is most effi-  cient.  3.2 How Meta-outputs Participate in the  Tr---qlation ,  The above sketch shows how context-dependent  interpretation is arranged as a series of non-  deterministic translation steps; in each case, we have  described the input and the output for the step in  question. We now go back to the concerns of Sec-  tion 2. First, note that each translation step is in  general fallible. We give several examples:  One of the most obvious cases arises when the  user simply issues an invalid command, such as  requesting the PSA to open a door D which is  already open. Here, one of the meta-outputs  issued by the plan evaluation step will be the  term  presupposition_failure(already_open(D));  the DM can decide to paraphrase this back to  the user as a surface string of the form \"D is  already open\". Note that plan evaluation does  not involve actually executing the final script,  which can be important. For instance, if the  command is \"go to the crew hatch and open it\"  and the crew hatch is already open, the interface  has the option of informing the user that there  is a problem without first carrying out the \"go  to\" action.  The resolution step can give rise to similar kinds  of metaooutput. For example, a command may  114  include a referring expression that has no deno-  tation, or an ambiguous denotation; for exam-  ple, the user might say \"both decks\", presum-  ably being unaware that there are in fact three  of them. This time, the meta-output produced  is  presupposition_failure (  incorrect_size_of_set (2,3))  representing the user's incorrect belief about  the number of decks. The DM then has the pos-  sibility of informingthe user of this misconcelfi  tion by realizing the meta-output term as the  surface string \"in fact there are three of them\".  Ambiguous denotation occurs when a descrip-  tion is under-specified. For instance, the user  might say \"the deck\" in a situation where there  is no clearly salient deck, either in the discourse  situation or in the simulated world: here, the  meta-output will be  presupposition_failure (  underspecif ied_def inite (deck))  which can be realized as the clarification ques-  tion \"which deck do you mean?\"   A slightly more complex case involves plan  costs. During plan evaluation, the system simu-  lates execution of the output script while keep-  ing track of execution cost. (Currently, the cost  is just an estimate of the time required to exe-  cute the script). Execution costs are treated as  meta-outputs of the form  cost (C)  and passed back through the interpreter so that  the plan optimization step can make use of  them.   Finally, we consider what happens when the  system receives incorrect input from the speech  recognizer. Although the recognizer's \u001b[1;32;40m language \u001b[0;0m  model is constrained so that it can only pro-  duce grammatical utterances, it can still misrec-  ognize one grammatical string as another one.  Many of these cases fall into one of a small  number of syntactic patterns, which function as  fairly reliable indicators of bad recognition. A  typical example is conjunction involving a pro-  noun: if the system hears \"it and flight deck\",  this is most likely a misrecognition fsomething  like \"go to flight deck\".  During the processing phase which translates  linguistic level representations into discourse  level representations, the system attempts to  match each misrecognition pattern against he  input linguistic form, and if successful produces  a meta-output of the form  presupposi t ion_fa i lure (  dubious_If (<Type>))  These meta-outputs are passed down to the  DM, which in the absence of sufficiently com-  pelling contrary evidence will normally issue a  response of the form \"I'm sorry, I think I mis-  heard you\".  4 A Compact  Arch i tec ture  for   D ia logue  Management  Based  on   Scr ip ts  and  Meta -Outputs   None of the individual functionalities outlined above  are particularly novel in themselves. What we find  new and interesting is the fact that they can all  be expressed in a uniform way in terms of the  script output/meta-output architecture. This sec-  tion presents three examples illustrating how the ar-  chitecture can be used to simplify the overall orga-  nization of the system.  4.1 Integration of plan evaluation, plan  execution and dialogue management  Recall that the DM simulates evaluation of the plan  before running it, in order to obtain relevant meta-  information. At plan execution time, plan actions  result in changes to the world; at plan evaluation  time, they result in simulated changes to the world  and/or produce meta-outputs.  Conceptualizing plans as scripts rather than log-  icai formulas permits an elegant reatment of the  execution/evaluation dichotomy. There is one script  interpreter, which functions both as a script exec-  utive and a script evaluator, and one set of rules  which defines the procedural semantics of script ac-  tions. Rules are parameterized by execution type  which is either \"execute\" or \"evaluate\". In \"evalu-  ate\" mode, primitive actions modify a state vector  which is threaded through the interpreter; in \"ex-  ecute\" mode, they result in commands being sent  to (real or simulated) effector agents. Conversely,  \"meta-information\" actions, such as presupposition  failures, result in output being sent to the meta-  output stream in \"evaluate\" mode, and in a null ac-  tion in \"execute\" mode. The upshot is that a simple  semantics can be assigned to rules like the following  one, which defines the action of attempting to open  a door which may already be open:  procedure (  open_door (D),  i f_then_else (status (D, open_closed, open),  presupposi t ion_fa i lure (already_open(D)),  change_status (D, open_closed, open) ) )  4.2 Using meta-outputs to choose between  interpretations  As described in the preceding section, the resolution  step is in general non-deterministic and gives rise to  115  meta-outputs which describe the type of resolution  carried out. For example, consider a command in-  volving a definite description, like \"open the door\".  Depending on the preceding context, resolution will  produce a number of possible interpretations; \"the  door\" may be resolved to one or more contextually  available doors, or the expression may be left un-  resolved. In each case, the type of resolution used  appears as a meta-output, and is available to the di-  alogue manager when it decides which interpretation  is most felicitous. By default, the DM's strategy isto  attempt to supply antecedents for referring expre~..  sious, preferring the most recently occurring sortally  appropriate candidate. In some cases, however, it is  desirable to allow the default strategy to be over-  ridden: for instance, it may result in a script which  produces a presupposition failure during plan eval-  uation. Treating resolution choices and plan evalu-  ation problems as similar types of objects makes it  easy to implement this kind of idea.  4.3 Us ing meta-outputs  to choose between  dialogue management  moves  Perhaps the key advantage ofour architecture is that  collecting together several types of information as a  bag of meta-outputs simplifies the top-level struc-  ture of the dialogue manager. In our application,  the critical choice of dialogue move comes after the  dialogue manager has selected the most plausible in-  terpretation. It now has to make two choices. First,  it must decide whether or not to paraphrase any of  the meta-outputs back to the user; for example, if resolution was unable to fill some argument posi-  tion or find an antecedent for a pronoun, it may be  appropriate to paraphrase the corresponding meta-  output as a question, e.g. \"where do you mean?\", or  \"what do you mean by 'it' ?' .  Having all the meta-  outputs available together means that the DM is  able to plan a coherent response: so if there are sev-  eral recta-outputs which could potentially be worth  paraphrasing, it typically realizes only the most im-  portant one.  Second, if interpretation was able to produce a  well-formed plan, the DM currently has the three  options of executing it, paraphrasing it back to the  user as a confirmation question, or doing nothing.  Once again, this decision often requires global infor-  mation about what has happened uring the inter-  pretation process. For example, knowing that plan  evaluation showed that the plan would take signif-  icant time to execute makes it more plausible that  the user would prefer to receive a confirmation.  5 Examples   This section presents a commented sample dialogue  with the implemented PSA interface demo chosen  to illustrate the points discussed in the previous ec-  tions. We focus in particular on the role that meta-  outputs play in interpretation. Text preceded by  USER represents spoken utterances from the user.  Text preceded by PSA represents synthesized spoken  responses from the PSA. Text inside square brackets  describes the actions carried out by the PSA.  1. The PSA can move to various locations and  measure nvironmental f ctors. In the first ex-  change, reference resolution is used to interpret  the phrase \"all three decks\" as referring to the  lower deck, mid deck and flight deck. Plan op-  timiT.ation finds the most efficient way to or-  der the locations, and paraphrases the resulting  plan back to the user.  USER: Go to all three decks and measure car-  bon dioxide.  PSA: I will move to flight deck, mid deck and  then lower deck and I will measure carbon  dioxide level, okay?  USER: Okay. \\[PSA moves to flight deck\\]  PSA: The carbon dioxide level at the flight  deck is one percent. \\[PSA moves to mid  deck\\]  PSA: The carbon dioxide level at the mid deck  is one percent. \\[PSA moves to lower deck\\]  PSA: The carbon dioxide level at the lower  deck is one percent.  2. The next example involves resolution of ellip-  sis. The relevant context is recovered from the  preceding dialogue.  USER: Do the same for the pilot's seat.  PSA: I will move to the pilot's seat and I will  measure carbon dioxide level, okay?  USER: Right. \\[PSA moves to pilot's seat\\]  PSA: The carbon dioxide level at the pilot's  seat is one percent.  3. Resolution of ellipsis can involve filling in un-  specified semantic parameters toa syntactically  complete command. It is not meaningful to talk  about a sensor eading without reference to the  place at which that reading was carried out; in  the following example, resolution assumes that  the location in question is the one most recently  mentioned in the dialogue.  Note also that since the PSA does not have to  move in order to carry out the command, and  hence can execute it immediately, the DM rea-  sonably decides to skip the confirmation ques-  tion. This decision is made based on the meta-  output encoding the estimated cost of executing  the script.  116  USER: What was the carbon dioxide level at  fifteen oh five according to the fixed sen-  sors?  PSA: According to the fixed sensors, at fifteen  oh five the carbon dioxide level at the pi-  lot's seat was one percent.  4. The system encounters a user misconception,  which results in a \"presupposition failure\"  meta-output being produced uring reference  resolution. It consequently decides to alert the  user to this, rather than attempt to carry out  the command.  USER: Close both doors.  PSA: There are in fact three of them.  5. The user utterance includes a pronoun, \"it\".  The system resolves \"it\" to \"the crew hatch\",  and issues an appropriate paraphrase in order  to confirm that this is what the user meant.  USER: Go to crew hatch and close it.  PSA: I will go to crew hatch and I will close  crew hatch, okay?  USER: Yeah. \\[PSA moves to crew hatch and  closes it\\]  6. The user asks the system to close the door, but  it is unclear which door they are referring to.  An \"ambiguous reference\" meta-output is pro-  duced by reference resolution, resulting in the  DM issuing a clarification question.  USER: Close the door.  PSA: Which door do you mean?  7. The user response is successfully interpreted by  reference resolution as an instance of ellipsis.  Trying to close the crew hatch a second time  however causes a presupposition failure meta-  output o be generated during plan evaluation.  The system informs the user of this and takes  no action.  USER: The crew hatch.  PSA: The crew hatch is already closed.  8. The user presents three locations in a subopti-  mal order. Plan evaluation finds a more efficient  ordering.  USER: Move to storage lockers, commander's  seat and flight deck and measure tempera-  ture.  PSA: I will move to flight deck, commander's  seat and then storage lockers and I will  measure temperature, okay?  USER: Sure.  \\[PSA moves to flight deck, commander's  seat and storage lockers in that order and  reports temperature at each location\\]  9. The user asks the system to repeat he previous  command. Since the PSA is now starting from  a different position, its most efficient plan is no  longer the same, and it informs the user of this.  USER: Do that again.  PSA: I will move to storage lockers, comman-  der's seat and then flight deck and I will  measure temperature, okay?  USER: Yes. \\[PSA moves to storage lockers\\]  PSA: The temperature at the storage lockers  is 19.9 degrees Celsius. \\[PSA starts moving  to commander's seat\\]  10. The user interrupts the PSA in the middle of the  plan, and asks it to go back to its last stopping  place. Since \"stop\" may involve a potentially  hazardous condition, the interface obeys imme-  diately, without attempting to confirm first.  USER: Stop. \\[PSA stops\\]  USER: Go back. \\[PSA returns to storage lock-  ers/  6 Summary   We have described a generic architecture for spoken  \u001b[1;32;40m language \u001b[0;0m dialogue interfaces to semi-autonomous  agents, based on the standard notion of translating  to successively deeper levels of representation. The  novel aspects of our scheme center around two ideas:  first, that the final output representations are best  conceptualized not as logical expressions but rather  as programs in a scripting \u001b[1;32;40m language \u001b[0;0m; second, that  steps in the translation process hould produce not  only a simple output, but also meta-information de- scribing how the output was produced. We have pre-  sented examples suggesting how several apparently  diverse types of dialogue behavior can be captured  simply within our framework, and outlined a proto-  type implementation f the scheme.  References   N. Badler, R. Bindiganavale, J. Bourne, J. Allbeck,  J. Shi, and M. Palmer. 1999. Real time virtual  humans. In International Conference on Digital  Media Futures.  J. Dowding, M. Gawron, D. Appelt, L. Cherny,  R. Moore, and D. Moran. 1993. Gemini: A nat-  ural \u001b[1;32;40m language \u001b[0;0m system for spoken \u001b[1;32;40m language \u001b[0;0m un-  derstanding. In Proceedings of the Thirty-First  Annual Meeting of the Association for Computa-  tional Linguistics.  117  K. Konolige, K. Myers, E. Ruspini, and A. Saf-  fiotti. 1993. Flakey in action: The 1992 AAAI   robot competition. Technical Report SRI Techni-  cal Note 528, SKI, AI Center, SKI International,  333 Ravenswood Ave., Menlo Park, CA  94025.  D. Martin, A. Cheyer, and D. Moran. 1998. Build-  ing distributed software systems with the open  agent architecture. In Proceedings of the Third  International Conference on the Practical Appli-  cation of Intelligent Agenta nd Multi-Agent Tech-  nalogy.  R. Moore, J. Dowding, H. Bratt, J. Gawron~-  Y. Gorfu, and A. Cheyer. 1997. CommandTalk:  A spoken-\u001b[1;32;40m language \u001b[0;0m interface for battlefield simu-  lations. In Proceedings ofthe Fifth Conference on  Applied Natural Language Processing, pages 1-7.  Nuance, 2000. Nuance Communications, Inc.  http://www.nuance.com. As of 9 March 2000.  D. Perzanowski, A. Schnltz, and W. Adams. 1998.  Integrating \u001b[1;32;40m natural \u001b[0;0m \u001b[1;32;40m language \u001b[0;0m and gesture in a  robotics domain. In IEEE International Sympo-  sium on Intelligent Control.\" ISIC/CIRA/ISAS  Joint Conference, pages 247-252, Gaithersburg,  MD: National Institute of Standards and Tech-  nology.  D. Perzanowski, A. Schnltz, W. Adams, and  E. Marsh. 1999. Goal tracking in s \u001b[1;32;40m natural \u001b[0;0m lan-  guage interface: Towards achieving adjustable au-  tonomy. In ISIS/CIRA99 Conference, Monterey,  CA. IEEE.  PSA, 2000. Personal Satellite Assistant (PSA)  Project. http://ic.arc.nasa.gov/ic/psa/. As of 9  March 2000.  D. Pyre, L. Pryor, and D. Murphy. 1995. Actions  as processes: a position on planning. In Working  Notes, AAAI Symposium on Eztending Theories  of Action, pages 169-173.  R. W. Smith. 1997. An evaluation of strategies for  selective utterance verification for spoken atural  \u001b[1;32;40m language \u001b[0;0m dialog. In Proceedings of the Fifth Con-  \\]erence on Applied Natural Language Processing,  pages 41-48.  A. Stent, J. Dowding, J. Gawron, E. Bratt, and  R. Moore. 1999. The CommandTalk spoken di-  alogue system. In Proceedings of the Thirty-  Seventh Annual Meeting of the Association for  Computational Linguistics, pages 183-190.  D. R. Tranm and J. Allen. 1994. Discourse obliga-  tions in dialogue processing. In Proceedings ofthe  Thirty-Second Annual Meetiitg of the Association  for Computational Linguistics, pages 1-8.  D. R. Traum and C. F. Andersen. 1999. Represen-  tations of dialogue state for domain and task inde-  pendent meta-dialogue. In Proceedings of the IJ-  CAI'gg Workshop on Knowledge and Reasoning  in Practical Dialogue Systems, pages 113-120.  J. van Eijck and R. Moore. 1992. Semantic rules  for English. In H. Alshawi, editor, The Core Lan-  guage Engine. MIT Press.  B. Webber. 1995. Instructing animated agents:  Viewing \u001b[1;32;40m language \u001b[0;0m inbehavioral terms. In Proceed-  ings of the International Conference on Coopera-  tive Multi-modal Communication.  T. A. Winograd. 1973. A procedural model of lan-  guage understanding. In R. C. Shank and K. M.  Colby, editors, Computer Models of Thought and  Language. Freeman, San Francisco, CA.  118  \n",
            "-----------------------------\n",
            "--- document 5: --- document 5: Multilingual Coreference Resolution  Sanda M.  Harabag iu   Southern Methodist  University  Dallas, TX  75275-0122  sanda@seas, smu. edu  Steven J .  Ma iorano   IPO  Washington,  D.C. 20505  maiorano@cais, com  Abst rac t   In this paper we present a new, multi-  lingual data-driven method for coreference  resolution as implemented in the SWIZZLE  system. The results obtained after training  this system on a bilingual corpus of English  and Romanian tagged texts, outperformed  coreference r solution in each of the indi-  vidual anguages.  1 I n t roduct ion   The recent availability of large bilingual corpora has  spawned interest in several areas of multilingual text  processing. Most of the research as focused on  bilingual terminology identification, either as par-  allel multiwords forms (e.g. the ChampoUion sys-  tem (Smadja et a1.1996)), technical terminology (e.g.  the Termight system (Dagan and Church, 1994) or  broad-coverage translation lexicons (e.g. the SABLE  system (Resnik and Melamed, 1997)). In addition,  the Multilingual Entity Task (MET) from the TIP-  STER program 1 (http://www-nlpir.nist.gov/related-  projeets/tipster/met.htm) challenged the partici-  pants in the Message Understanding Conference  (MUC) to extract named entities across everal for-  eign \u001b[1;32;40m language \u001b[0;0m corpora, such as Chinese, Japanese  and Spanish.  In this paper we present a new application of  aligned multilinguai texts. Since coreference r so-  lution is a pervasive discourse phenomenon causing  performance impediments in current IE systems, we  considered a corpus of aligned English and Roma-  nian texts to identify coreferring expressions. Our  task focused on the same kind of coreference as  considered in the past MUC competitions, namely  1The TIPSTER Text Program was a DARPA-Ied  government effort o advance the state of the art in text  processing technologies.  the identity coreference. Identity coreference links  nouns, pronouns and noun phrases (including proper  names) to their corresponding antecedents.  We created our bilingual collection by translating  the MUC-6 and MUC-7 coreference training texts  into Romanian using native speakers. The train-  ing data set for Romanian coreference used, wher-  ever possible, the same coreference identifiers as the  English data and incorporated additional tags as  needed. Our claim is that by adding the wealth  of coreferential features provided by multilingual  data, new powerful heuristics for coreference r solu-  tion can be developed that outperform onolingual  coreference r solution systems.  For both \u001b[1;32;40m language \u001b[0;0ms, we resolved coreference by  using SWIZZLE, our implementation f a bilingual  coreference r solver. SWIZZLE is a multilingual en-  hancement of COCKTAIL (Harabagiu and Maiorano,  1999), a coreference r solution system that operates  on a mixture of heuristics that combine semantic  and textual cohesive information 2. When COCKTAIL  was applied separately on the English and the Ro-  manian texts, coreferring links were identified for  each English and Romanian document respectively.  When aligned referential expressions corefer with  non-aligned anaphors, SWIZZLE derived new heuris-  tics for coreference. Our experiments show that  SWIZZLE outperformed COCKTAIL on both English  and Romanian test documents.  The rest of the paper is organized as follows. Sec-  tion 2 presents COCKTAIL, a monolingnai coreference  resolution system used separately on both the En-  glish and Romanian texts. Section 3 details the  data-driven approach used in SWIZZLE and presents  some of its resources. Section 4 reports and discusses  the experimental results. Section 5 summarizes the  2The name of COCKTAIL is a pun on CogNIAC be-  cause COCKTAIL combines a larger number of heuristics  than those reported in (Baldwin, 1997). SWIZZLE, more-  over, adds new heuristics, discovered from the bilingual  aligned corpus.  142  conclusions.  2 COCKTAIL   Currently, some of the best-performing and  most robust coreference r solution systems employ  knowledge-based techniques. Traditionally, these  techniques have combined extensive syntactic, se-  mantic, and discourse knowledge. The acquisition  of such knowledge is time-consuming, difficult, and  error-prone. Nevertheless, recent results show that  knowledge-poor methods perform with amazing ac-  curacy (cf. (Mitkov, 1998), (Kennedy and Boguraev,  1996) (Kameyama, 1997)). For example, CogNIAC  (Baldwin, 1997), a system based on seven ordered  heuristics, generates high-precision resolution (over  90%) for some cases of pronominal reference. For  this research, we used a coreference r solution sys-  tem ((Harabagiu and Malorano, 1999)) that imple-  ments different sets of heuristics corresponding to  various forms of coreference. This system, called  COCKTAIL, resolves coreference by exploiting several  textual cohesion constraints (e.g. term repetition)  combined with lexical and textual coherence cues  (e.g. subjects of communication verbs are more  likely to refer to the last person mentioned in the  text). These constraints are implemented as a set of  heuristics ordered by their priority. Moreover, the  COCKTAIL framework uniformly addresses the prob-  lem of interaction between different forms of coref-  erence, thus making the extension t  multilingual  coreference very \u001b[1;32;40m natural \u001b[0;0m.  2.1 Data -Dr iven  Coreference Reso lu t ion   In general, we define a data-driven methodology as  a sequence of actions that captures the data pat-  terns capable of resolving a problem with both a  high degree of precision and recall. Our data-driven  methodology reported here generated sets of heuris-  tics for the coreference r solution problem. Precision  is the number of correct references out of the total  number of coreferences resolved, whereas the recall  measures the number of resolved references out of  the total number of keys, i.e., the annotated coref-  erence data.  The data-driven methodology used in COCKTAIL is  centered around the notion of a coreference chain.  Due to the transitivity of coreference relations, k  coreference r lations having at least one common ar-  gument generate k + 1 core/erring expressions. The  text position induces an order among coreferring ex-  pressions. A coreference structure is created when  a set of coreferring expressions are connected in an  oriented graph such that each node is related only  to one of its preceding nodes. In turn, a corefer-  ence chain is the coreference structure in which ev-  ery node is connected to its immediately preceding  node. Clearly, multiple coreference structures for the  same set of coreferring expressions can be mapped  to a single coreference chain. As an example, both  coreference structures illustrated in Figure l(a) and  (c) are cast into the coreference chain illustrated in  Figure l(b).  TEXT TEXT TEXT  i [ ]   (a) (b) (c)  Figure 1: Three coreference structures.  Given a corpus annotated with coreference data,  the data-driven methodology first generates all  coreference chains in the data set and then con-  siders all possible combinations of coreference re-  lations that would generate the same coreference  chains. For a coreference chain of length l with  nodes nl, n2, ... nt+l, each node nk ( l<k~/)  can  be connected to any of the l - k nodes preceding  it. From this observation, we find that a number  of 1 x 2 x ... x (l - k)... x I = l! coreference struc-  tures can generate the same coreference chain. This  result is very important, since it allows for the auto-  matic generation of coreference data. For each coref-  erence relation T~ from an annotated corpus we cre-  ated a median of (l - 1)! new coreference r lations,  where l is the length of the coreference hain contain-  ing relation 7~. This observation gave us the possi-  bility of expanding the test data provided by the  coreference keys available in the MUC-6 and MUC-  7 competitions (MUC-6 1996), (MUC-7 1998). The  MUC-6 coreference annotated corpus contains 1626  coreference r lations, while the MUC-7 corpus has  2245 relations. The average length of a coreference  chain is 7.21 for the MUC-6 data, and 8.57 for the  MUC-7 data. We were able to expand the number  of annotated coreference relations to 6,095,142 for  the MUC-6 corpus and to 8,269,403 relations for the  MUC-7 corpus; this represents an expansion factor  of 3,710. We are not aware of any other automated  way of creating coreference annotated ata, and we  believe that much of the COCKTAIL's impressive per-  formance is due to the plethora of data provided by  this method.  143  Heuristics for 3rd person pronouns  oHeuristie 1-Pronoun(H1Pron)  Search in the same sentence for the same  3rd person pronoun Pros'  if (Pron' belongs to coreference chain CC)  and there is an element from CC which is  closest o Pron in Text, Pick that element.  else Pick Pron'.  oHeuristic 2-Pronoun(H2Pron)  Search for PN, the closest proper name from Pron  if (PN agrees in number and gender with Pros)  if (PN belongs to coreference chain CC)  then Pick the element from CC which is  closest o Pros in Text.  else Pick PN.  o Heuristic 3- Pronoun( H3Pron ) Search for Noun, the closest noun from Pros  if (Noun agrees in number and gender with Pros)  i f  (Noun belongs to coreference chain CC)  and there is an element from CC which is  closest o Pros in Text, Pick that element.  else Pick Noun  Heuristics for nominal reference  o Heuristic 1-Nominal(HINom )  if (Noun is the head of an appositive)  then Pick the preceding NP.  oHeuristic 2-Nominal(H2Nom)  if (Noun belongs to an NP, Search for NP'  such that Noun'=same_name(head(NP),head(NP'))  or   Noun'--same_name(adjunct(NP), adjunct(NP')))  then if (Noun' belongs to coreference chain CC)  then Pick the element from CC which is  closest o Noun in Text.  else Pick Noun'.  oHeuristic 3-Nominal(H3Nom)  if Noun is the head of an NP  then Search for proper name PN  such that head(PN)=Noun  if (PN belongs to coreference chain CC)  and there is an element from CC which is  closest o Noun in Text, Pick that element.  else Pick PN.  Table 1: Best performing heuristics implemented in COCKTAIL  2.2 Knowledge-Poor  Core ference   Reso lu t ion   The result of our data-driven methodology is the  set of heuristics implemented in COCKTAIL which  cover both nominal and pronoun coreference. Each  heuristic represents a pattern of coreference that  was mined from the large set of coreference data.  COCKTAIL uses knowledge-poor methods because (a)  it is based only on a limited number of heuristics  and (b) text processing is limited to part-of-speech  tagging, named-entity recognition, and approximate  phrasal parsing. The heuristics from COCKTAIL can  be classified along two directions. First of all, they  can be grouped according to the type of corefer-  ence they resolve, e.g., heuristics that resolve the  anaphors of reflexive pronouns operate differently  than those resolving bare nominals. Currently, in  COCKTAIL there are heuristics that resolve five types  of pronouns (personal, possessive, reflexive, demon-  strative and relative) and three forms of nominals  (definite, bare and indefinite).  Secondly, for each type of coreference, there are  three classes of heuristics categorized according to  their suitability to resolve coreference. The first  class is comprised of strong indicators of coreference.  This class resulted from the analysis of the distribu-  tion of the antecedents in the MUC annotated ata.  For example, repetitions of named entities and ap-  positives account for the majority of the nominal  coreferences, and, therefore, represent anchors for  the first class of heuristics.  The second class of coreference covers cases in  which the arguments are recognized to be seman-  tically consistent. COCKTAIL's test of semantic on-  sistency blends together information available from  WordNet and statistics gathered from Treebank.  Different consistency checks are modeled for each of  the heuristics.  Example of the application of heuristic H2Pron  Mr. Adams1, 69 years old, is the retired chairman  of Canadian-based Emco Ltd., a maker of plumbing  and petroleum equipment; he1 has served on the  Woolworth board since 1981.  Example of the application of heuristic H3Pron  \"We have got to stop pointing our fingers at these  kids2 who have no future,\" he said, \"and reach our  hands out to them2.  Example of the application of heuristic H2Nom  The chairman and the chief executive officer3  of Woolworth Corp. have temporarily relinquished  their posts while the retailer conducts its investi-  gation into alleged accounting irregularities4.  Woolworth's board named John W. Adams, an  outsider, to serve as interim chairman and executive  officer3, while a special committee, appointed by  the board last week and led by Mr. Adams,  investigates the alleged irregularities4.  Table 2: Examples of coreference resolution. The  same annotated index indicates coreference.  The third class of heuristics resolves coreference  by coercing nominals. Sometimes coercions involve  only derivational morphology - linking verbs with  their nominalizations. On other occasions, coercions  are obtained as paths of meronyms (e.g. is-part re-  lations) and hypernyms (e.g. is-a relations). Con-  144.  sistency checks implemented for this class of coref-  erence are conservative: ither the adjuncts must be  identical or the adjunct of the referent must be less  specific than the antecedent. Table 1 lists the top  performing heuristics of COCKTAIL for pronominal  and nominal coreference. Examples of the heuristics  operation on the MUC data are presented presented  in Table 2. Details of the top performing heuris-  tics of COCKTAIL were reported in (Harabagiu and  Maiorano, 1999).  2.3 Bootstrapping for Coreferenee  Resolution  One of the major drawbacks of existing corefer-  ence resolution systems is their inability to recog-  nize many forms of coreference displayed by many  real-world texts. Recall measures of current systems  range between 36% and 59% for both knowledge-  based and statistical techniques. Knowledge based-  systems would perform better if more coreference  constraints were available whereas tatistical meth-  ods would be improved if more annotated data were  available. Since knowledge-based techniques out-  perform inductive methods, we used high-precision  coreference heuristics as knowledge seeds for ma-  chine learning techniques that operate on large  amounts of unlabeled ata. One such technique  is bootstrapping, which was recently presented in  (Riloff and Jones 1999), (Jones et a1.1999) as an  ideal framework for text learning tasks that have  knowledge seeds. The method oes not require large  training sets. We extended COCKTAIL by using meta-  bootstrapping of both new heuristics and clusters of  nouns that display semantic onsistency for corefer-  ence.  The coreference heuristics are the seeds of our  bootstrapping framework for coreference r solution.  When applied to large collections of texts, the  heuristics determine classes of coreferring expres-  sions. By generating coreference chains out of all  these coreferring expressions, often new heuristics  are uncovered. For example, Figure 2 illustrates the  application of three heuristics and the generation of  data for a new heuristic rule. In COCKTAIL, after a  heuristic is applied, a new coreference chain is cal-  culated. For the example illustrated in Figure 2, if  the reference of expression A is sought, heuristic H1  indicates expression B to be the antecedent. When  the coreference chain is built, expression A is di-  rectly linked to expression D, thus uncovering a new  heuristic H0.  As a rule of thumb, we do not consider a new  heuristic unless there is massive vidence of its cov-  erage in the data. To measure the coverage we use  the FOIL_Gain measure, as introduced by the FOIL  inductive algorithm (Cameron-Jones and Quinlan  1993). Let Ho be the new heuristic and/-/1 a heuris-  tic that is already in the seed set. Let P0 be the num-  ber of positive coreference examples of Hn~w (i.e.  the number of coreference r lations produced by the  heuristic that can be found in the test data) and no  the number of negative xamples of/-/new (i.e. the  number of relations generated by the heuristic which  cannot be found in the test data). Similarly, Pl and  nl are the positive and negative xamples of Ha.  The new heuristics are scored by their FOIL_Gain  distance to the existing set of heuristics, and the best  scoring one is added to the COCKTAIL system. The  FOIL_Gain formula is:  l og2- - -~ ) FOIL_Gain(H1, Ho) = k(log2 Pl nl  Po -k no  where k is the number of positive examples cov-  ered by both//1 and Ho. Heuristic Ho is added to  the seed set if there is no other heuristic providing  larger FOIL_Gain to any of the seed heuristics.  H3 . j . . .~ IB  B  \\ [~  HO - New Heuristic  Figure 2: Bootstrapping new heuristics.  Since in COCKTAIL, semantic onsistency of core-  ferring expressions is checked by comparing the sim-  ilarity of noun classes, each new heuristic deter-  mines the adjustment of the similarity threshold of  all known coreferring noun classes. The steps of  the bootstrapping algorithm that learns both new  heuristics and adjusts the similarity threshold of  coreferential expressions i :  MUTUAL BOOTSTRAPPING LOOP  1. Score all candidate heuristics with FOIL_Gain  2. Best_h--closest candidate to heuristics(COCKTAIL)  3. Add Best_h to heuristics(COCKTAIL)  ,f. Adjust semantic similarity threshold for semantic  consistency o\\[ coreferring nouns  5. Goto step 1 if the precision and recall did not  degrade under minimal performance.  (Riloff and Jones 1999) note that the bootstrap-  ping algorithm works well but its performance can  deteriorate rapidly when non-coreferring data enter  as candidate heuristics. To make the algorithm ore  robust, a second level of bootstrapping can be intro-  duced. The outer bootstrapping mechanism, called  145  recta-bootstrapping compiles the results of the inner  (mutual) bootstrapping process and identifies the k  most reliable heuristics, where k is a number de-  termined experimentally. These k heuristics are re-  tained and the rest of them are discarded.  3 SWIZZLE   3.1 Mul t iHngua l  Core ference  Data   To study the performance of a data-driven multi-  lingual coreference r solution system, we prepared a corpus of Romanian texts by translating the MUC-6  and MUC-7 coreference training texts. The transla-  tions were performed by a group of four Romanian  native speakers, and were checked for style by a cer-  tified translator from Romania. In addition, the Ro-  manian texts were annotated with coreference keys.  Two rules were followed when the annotations were  done:  o 1: Whenever an expression ER represents a trans-  lation of an expression EE from the corresponding  English text, if Es  is tagged as a coreference key  with identification umber ID, then the Romanian  expression ER is also tagged with the same ID num-  ber. This rule allows for translations in which the  textual position of the referent and the antecedent  have been swapped.  o2: Since the translations often introduce new  coreferring expressions in the same chain, the new  expressions are given new, unused ID numbers.  For example, Table 3 lists corresponding English  and Romanian fragments of coreference chains from  the original MUC-6 Wall Street Journal document  DOCNO: 930729-0143.  Table 3 also shows the original MUC coreference  SGML annotations. Whenever present, the REF tag  indicates the ID of the antecedent, whereas the MIN  tag indicates the minimal reference xpression.  3.2 Lex ica l  Resources   The multilingual coreference resolution method im-  plemented in SWIZZLE incorporates the heuristics de-  rived from COKCTAIL's monolingual coreference res-  olution processing in both \u001b[1;32;40m language \u001b[0;0ms. To this end,  COCKTAIL required both sets of texts to be tagged  for part-of-speech and to recognize the noun phrases.  The English texts were parsed with Brill's part-of-  speech tagger (Brill 1992) and the noun phrases were  identified by the grammar ules implemented in the  phrasal parser of FASTUS (Appelt et al., 1993). Cor-  responding resources are not available in Romanian.  To minimize COCKTAIL's configuration for process-  ing Romanian texts, we implemented a Romanian  part-of-speech rule-based tagger that used the same  Economic adviser Gene Sperling described  <COREF ID=\"29\" TYPE=' IDENT\" REF-\"30\">  i t</COREF> as \"a true full-court press\" to pass  <COREF ID=\"31\" TYPE=\"IDENT\" REF=\"26\"  MIN='bilr '  >the <COREF ID=\"32\"  TYPE=\"IDENT\" REF-----\"10\" MIN=\"reduction\">  <COREF ID=\"33\" TYPE=\"IDENT\" REF=\"12\">  def i c i t</COREF>-reduct ion</COREF>  bill, the final version of which is now being  hammered out by <COREF ID=\" 43\" >House  </COREF> and <COREF ID=\"41\" >Senate  </COREF>negot ia tors</COREF>.   <COREF ID=\" 34\" TYPE=\" IDENT\" REF-\"  2\" >  The executives</COREF>' backing - however tepid  - gives the administration a way to counter  <COREF ID=\"35\" TYPE=\"IDENT\" REF=\"36\">  bus iness</COREF:> critics of <COREF ID=\"500\"  TYPE=\"IDENT\" REF=\"31\" MIN=\"package\"  STATUS=\" OPT\" >the  overall package  </COREF>,. . .   Consilierul cu probleme conomice Gene Sperling a  descris-<COREF ID=\" 29\" TYPE=\"IDENT\"  REF=\"30\" >o</COREF> ca pe un efort de  avengur~ menit s~ promoveze <COREF ID=\" 1125\"  TYPE=\"IDENT\" REF=\"26\" MIN=\"legea\">legea  </COREF> pentru <COREF TYPE=\"IDENT\"  REF=\" 10\" MIN=\"reducerea\" > reducerea  </COREF> <COREF ID=\" 33\" TYPE=\" IDENT\"  REF=\" 12\"> deficitului n bugetul SUA</COREF>.  Versiunea finals a acestei <COREF ID=\"1126\"  TYPE=\"IDENT\" REF=\"l125\" MIN=\"legi\">legi  </COI~EF> este desfiin~at~ chiax in aceste  zile in cadrul dezbaterilor ce au loc in  <COREF ID=\"43\" >Camera  Reprezentat lv i lor   </CORJ~F> i in <COREF ID=\"41\">  Senat</COREF></COREF>.   Sprijinirea <COREF ID=\"127\" TYPE=\"IDENT\"  REF=\" 1126\" MIN=\"legii\" >leg i i>/COREF>  de c~tre speciali~ti in economic - dei  in manier~ moderat~ - ofer~ administra~iei o  modalitate de a contrabalansa criticile aduse  <COREF ID=\"500\" TYPE=\"IDENT\" REF=\"31\"  MIN=\" legii\" STATUS=\" OPT\" >leg i i</COREF>  de c~tre companiile americane,...  Table 3: Example of parallel English and Romanian  text annotated for coreference. The elements from a  coreference chain in the respective texts are under-  lined. The English text has only two elements in the  coreference chain, whereas the Romanian text con-  tains four different elements. The two additional ele-  ments of the Romanian coreference chain are derived  due to (1) the need to translate the relative clause  from the English fragment into a separate sentence  in Romanian; and (2) the reordering of words in the  second sentence.  146  tags as generated by the Brill tagger. In addition,  we implemented rules that identify noun phrases in  Romanian.  To take advantage of the aligned corpus, SWIZZLE  also relied on bilingual exical resources that help  translate the referential expressions. For this  purpose, we used a core Romanian WordNet  (Harabagiu, 1999) which encoded, wherever possi-  ble, links between the English synsets and their Ro-  manian counterparts. This resource also incorpo-  rated knowledge derived from several bilingual dic-  tionaries (e.g. (Bantam, 1969)).  Having the parallel coreference annotations, we  can easily identify their translations because they  have the same identification coreference k y. Look-  ing at the example given in Table 3, the expres-  sion \"legii', with ID=500 is the translation of the  expression \"package\", having the same ID in the  English text. However, in the test set, the REF  fields are intentionally voided, entrusting COCKTAIL  to identify the antecedents. The bilingual corefer-  ence resolution performed in SWIZZLE, however, re-  quires the translations ofthe English and Romanian  antecedents. The principles guiding the translations  of the English and Romanian antecedents (AE-R  and A R-E, respectively) are:   Circularity: Given an English antecedent, due to  semantic ambiguity, it can belong to several English  WordNet sysnsets. For each such sysnset S/~ we con-  sider the Romanian corresponding sysnet(s) Sff. We  filter out all Sff that do not contain A E-R. If only  one Romanian sysnset is left, then we identified a translation. Otherwise, we start from the Roma-  nian antecedent, find all synsets SR to which it be-  longs, and obtain the corresponding English sysnets  S F. Similarly, all English synsets not containing  the English antecedent are filtered out. If only one  synset remains, we have again identified a transla-  tion. Finally, in the last case, the intersection of  the multiple synsets in either \u001b[1;32;40m language \u001b[0;0m generates a  legal translation. For example, the English synset  S E ={bill, measure} translates into the Romanian  synset S R ={lege}. First, none of the dictionary  translations of bill into Romanian (e.g. politE, bac-  notE, afi~) translate back into any of the elements  of S E. However the translation of measure into the  Romanian lege translates back into bill, its synonym.   Semantic density: Given an English and a Roma-  nian antecedent, to establish whether they are trans-  lations of one another, we disambiguate them by first  collapsing all sysnsets that have common elements.  Then we apply the circularity principle, relying on  the semantic alignment encoded in the Romanian  WordNet. When this core lexical database was first  implemented, several other principles were applied.  In our experiment, we were satisfied with the qual-  ity of the translations recognized by following only  these two principles.  3.3 Mult i l ingual Coreference Resolution  The SWIZZLE system was run on a corpus of 2335  referential expressions in English (927 from MUC-  6 and 1408 from MUC-7) and 2851 Romanian ex-  pressions (1219 from MUC-6 and 1632 from MUC-  7). Initially, the heuristics implemented in COCKTAIL  were applied separately to the two textual collec-  tions. Several special cases arose.  English Text  . . . .  :rr-Z, la ,on . . . . . . .   Translation  Romanian Text  \"~eference   Figure 3: Case 1 of multilingual coreference  Case 1, which is the ideal case, is shown in Fig-  ure 3. It occurs when two referential expressions  have antecedents hat are translations of one an-  other. This situation occurred in 63.3% of the refer-  ential expressions from MUC-6 and in 58.7% of the  MUC-7 references. Over 50% of these are pronouns  or named entities. However, all the non-ideal cases  are more interesting for SWIZZLE, since they port  knowledge that enhances system performance.  Coref. English Text chains  E . . . . . . . . . . . . . . . . . .   H 4 ~  .................  Translation  ER ~ .. . . . . . . . . . . . . . . . . . . . . . . . . . .   Translation  Romanian Text   ~ ......... RA  '~  (R)RR  ER: English reference RR: Romanian reference  EA: English antecedent RA: Romanian antecedent  ET: English translation RT: Romanian translation  of Romanian antecedent of English antecedent  Figure 4: Case 2 of multilingual coreference  Case 2 occurs when the antecedents are not trans-  lations, but belong to or corefer with elements of  some coreference chains that were already estab-  lished. Moreover, one of the antecedents is textually  147  closer to its referent. Figure 4 illustrates the case  when the English antecedent is closer to the referent  than the Romanian one.  SWIZZLE Solutions: (1) If the heuristic H(E) used  to resolve the reference inthe English text has higher  priority than H(R), which was used to resolve the  reference from the Romanian text, then we first  search for RT, the Romanian translation of EA, the  English antecedent. In the next step, we add heuris-  tic H1 that resolves RR into RT, and give it a higher  priority than H(R). Finally, we also add heuristic H2  that links RTto RA when there is at least one trans-  lation between the elements of the coreference hains  containing EA and ET respectively.  (2) If H(R) has higher priority than H(E), heuris-  tic H3 is added while H(E) is removed. We also add  //4 that relates ER to ET, the English translation of  RA.  Case 3 occurs when at least one of the antecedents  starts a new coreference chain (i.e., no coreferring  antecedent can be found in the current chains).  SWIZZLE Solution: If one of the antecedents  corefers with an element from a coreference chain,  then the antecedent in the opposite \u001b[1;32;40m language \u001b[0;0m is its  translation. Otherwise, SNIZZLE chooses the an-  tecedent returned by the heuristic with highest pri-  ority.  4 Resu l ts   The foremost contribution of SWIZZLE was that it  improved coreference r solution over both English  and Romanian texts when compared to monolingual  coreference r solution performance in terms of preci-  sion and recall. Also relevant was the contribution of  SNIZZLE to the process of understanding the cultural  differences expressed in \u001b[1;32;40m language \u001b[0;0m and the way these  differences influence coreference r solution. Because  we do not have sufficient space to discuss this issue  in detail here, let us state, in short, that English is  more economical than Romanian in terms of referen-  tial expressions. However the referential expressions  in Romanian contribute to the resolution of some of  the most difficult forms of coreference in English.  4.1 Precis ion and Recall  Table 4 summarizes the precision results for both  English and Romanian coreference. The results in-  dicate that the English coreference is more pre-  cise than the Romanian coreference, but SNIZZLE  improves coreference r solution in both \u001b[1;32;40m language \u001b[0;0ms.  There were 64% cases when the English coreference  was resolved by a heuristic with higher priority than  the corresponding heuristic for the Romanian coun-  terpart. This result explains why there is better pre-  cision enhancement for the English coreference.  English  Romanian  SWIZZLE on  English  SWIZZLE on  Romanian  Nominal Pronominal  73% 89%  66% 78%  76% 93%  71/o 82%  Table 4: Coreference precision  Total  84%  72%  87%  76%  English  Romanian  SWIZZLE on  English  SWIZZLE on  Romanian  Nominal  69%  63%  66%  61%  Pronominal Total  89% 78%  83% 72%  87% 77%  80% 70%  Table 5: Coreference r call  Table 5 also illustrates the recall results. The  advantage of the data-driven coreference r solution  over other methods is based on its better ecall per-  formance. This is explained by the fact that this  method captures a larger variety of coreference pat-  terns. Even though other coreference r solution sys-  tems perform better for some specific forms of refer-  ence, their recall results are surpassed by the data-  driven approach. Multilingual coreference in turn  improves more the precision than the recall of the  monolingual data-driven coreference systems.  In addition, Table 5 shows that the English coref-  erence results in better ecall than Romanian coref-  erence. However, the recall shows a decrease for both  \u001b[1;32;40m language \u001b[0;0ms for SNIZZLE because imprecise coreference  links are deleted. As is usually the case, deleting  data lowers the recall. All results were obtained by  using the automatic scorer program developed for  the MUC evaluations.  5 Conc lus ions   We have introduced a new data-driven method for  multilingual coreference r solution, implemented in the SWIZZLE system. The results of this method  are encouraging since they show clear improvements  over monolingual coreference r solution. Currently,  we are also considering the effects of a bootstrap-  ping algorithm for multilingual coreference r solu-  tion. Through this procedure we would learn con-  currently semantic onsistency knowledge and bet-  ter performing heuristic rules. To be able to de-  velop such a learning approach, we must first develop  a method for automatic recognition of multilingual  referential expressions.  148  We also believe that a better performance valu-  ation of SidIZZLE can be achieved by measuring its  impact on several complex applications. We intend  to analyze the performance of SIdIZZLE when it is  used as a module in an IE system, and separately in  a Question/Answering system.  Acknowledgements  This paper is dedicated to the  memory of our friend Megumi Kameyama, who in-  spired this work.  Re ferences   Douglas E. Appelt, Jerry R. Hobbs, John Bear, David  Israel, Megumi Kameyama nd Mabry Tyson. 1993.  The SRI MUC-5 JV-FASTUS Information Extraction  System. In Proceedings of the Fifth Message Under-  standing Conference (MUC-5).  Brack Baldwin. 1997. CogNIAC: high precision corefer-  ence with limited knowledge and linguistic resources.  In Proceedings of the ACL '97/EACL '97 Workshop on  Operational factors in practical, robust anaphora res-  olution, pages 38-45, Madrid, Spain.  Andrei Bantam. 1969. Dic~ionar Romn-Englez, Enlgez-  Romfi~. Editura ~tiin~ific~, Bucure~ti.  David Bean and Ellen Riloff. 1999. Corpus-Based I en-  tification of Non-Anaphoric Noun Phrases. In Pro-  ceedings of the 37th Conference of the Assosiation for  Computatioanl Linguistics (A CL-99), pages 373-380.  Eric Brill. A simple rule-based part of speech tagger. In  Proceedings of the Third Conference on Applied Nat-  ural Language Processing, pages 152-155, 1992.  Joseph F. Cameron-Jones and Ross Quinlan. 1993.  Avoiding Pitfalls When Learning Recursive Theories.  In Proceedings of the 13th International Joint Confer-  ence on Artificial Intelligence (IJCAI-93), pages 1050-  1055.  Claire Cardie and Kiri Wagstaff. 1999. Noun phrase  coreference as clustering. In Proceedings of the Joint  Conference on Empirical Methods in NLP and Very  Large Corpora, pages 82-89.  Niyu Ge, John Gale and Eugene Charniak. 1998.  Anaphora Resolution: A Multi-Strategy Approach. In  Proceedings of the 6th Workshop on Very Large Cor-  pora, (COLING/A CL '98).  Ido Dagan and Ken W. Church. 1994. TERMIGHT:  Identifying and translating technical terminology. In  Proceedings of the ~th ACL Conference on Applied  Natural Language Processing (ANLP-94).  Sanda M. Harabagiu. 1999. Lexical Acquisition for a  Romanian WordNet. Proceeding of the 3rd European  Summer School on Computational Linguistics.  Sanda M. Harabagiu and Steve J. Maiorano. 1999.  Knowledge-Lean Coreference Resolution and its Re-  lation to Textual Cohesion and Coherence. In Pro-  ceedings of the Workshop on the Relation of Dis-  course/Dialogue Structure and Reference, ACL'98,  pages 29-38.  Jerry R. Hobbs. Resolving pronoun references. Lingua,  44:311-338.  Andrew Kehler. 1997. Probabilistic Coreference in In-  formation Extraction. In Proceedings of the Second  Conference on Empirical Methods in Natural Lan-  guage Processing (SIGDAT), pages 163-173.  Shalom Lappin and Herbert Leass. 1994. An algorithm  for pronominal anaphora resolution. Computational  Linguistics, 20(4):535-562.  Rosie Jones, Andrew McCallum, Kevin Nigam and Ellen  Riloff. 1999. Bootstrapping for Text Learning Tasks.  In Proceedings of the IJCAI-99 Workshop on Text  Mining: Foundations, Techniques, and Applications.  Megumi Kameyama. 1997. Recognizing Referential  Links: An Information Extraction Perspective. In  Proceedings of the Workshop on Operational Factors  in Practical, Robust Anaphora Resolution for Un-  restricted Texts, (ACL-97/EACL-97), pages 46-53,  Madrid, Spain.  Christopher Kennedy and Branimir Bogureav. 1996.  Anaphora for everyone: Pronominal anaphora reso-  lution without a parser. In Proceedings of the 16th  International Conference on Computational Linguis-  tics (COLING-96).  George A. Miller. 1995. WordNet: A Lexical Database.  Communication of the A CM, 38(11):39-41.  Ruslan Mitkov. 1998. Robust pronoun resolution  with limited knowledge. In Proceedings of COLING-  ACL'98, pages 869-875.  1996. Proceedings of the Sixth Message Understanding  Conference (MUC-6),Morgan Kaufmann, San Mateo,  CA.  1998. Proceedings of the Seventh Message Understand-  ing Conference (MUC-7) ,Morgan Kaufmann, San  Mateo, CA.  Philip Resnik and I. Dan Melamed. 1997. Semi-  Automatic Acquisition of Domain-Specific Translation  Lexicons. In Proceedings of the 5th ACL Conference  on Applied Natural Language Processing (ANLP-97).  Ellen Riloff and Rosie Jones. 1999. Learning Dictionar-  ies for Information Extraction by Multi-Level Boot-  strapping. In Proceedings of the Sixteenth National  Conference on Artificial Intelligence (AAAI-99).  Frank Smadja, Katheleen R. McKeown and Vasileios  Hatzivassiloglou. 1996. Translating collocations for  bilingual exicons: A statistical approach. Computa-  tional Linguistics , 21(1):1-38.  149  \n",
            "-----------------------------\n",
            "--- document 6: --- document 6: MIMIC: An Adaptive Mixed Initiative Spoken Dialogue System for  Information Queries  Jennifer Chu-Carroll  Lucent Technologies Bell Laboratories  600 Mountain Avenue  Murray Hill, NJ 07974, U.S.A.  jencc @research.bell-labs.corn  Abstract  This paper describes MIMIC, an adaptive mixed initia-  tive spoken dialogue system that provides movie show-  time information. MIMIC improves upon previous  dialogue systems in two respects. First, it employs  initiative-oriented strategy adaptation to automatically  adapt response generation strategies based on the cumu-  lative effect of information dynamically extracted from  user utterances during the dialogue. Second, MIMIC's  dialogue management architecture decouples its initia-  tive module from the goal and response strategy selec-  tion processes, providing ageneral framework for devel-  oping spoken dialogue systems with different adaptation  behavior.  1 Introduction  In recent years, speech and \u001b[1;32;40m natural \u001b[0;0m anguage technolo-  gies have matured enough to enable the development of spoken dialogue systems in limited domains. Most ex-  isting systems employ dialogue strategies pre-specified  during the design phase of the dialogue manager with-  out taking into account characteristics of actual dialogue  interactions. More specifically, mixed initiative systems  typically employ rules that specify conditions (generally  based on local dialogue context) under which initiative  may shift from one agent o the other. Previous research,  on the other hand, has shown that changes in initiative  strategies inhuman-human dialogues can be dynamically  modeled in terms of characteristics of the user and of  the on-going dialogue (Chu-Carroll and Brown, 1998)  and that adaptability of initiative strategies in dialogue  systems leads to better system performance (Litman and  Pan, 1999). However, no previous dialogue system takes  into account these dialogue characteristics or allows for  initiative-oriented adaptation of dialogue strategies.  In this paper, we describe MIMIC, a voice-enabled  telephone-based dialogue system that provides movie  showtime information, emphasizing its dialogue man-  agement aspects. MIMIC improves upon previous ys-  tems along two dimensions. First, MIMIC automat-  ically adapts dialogue strategies based on participant  roles, characteristics of the current utterance, and dia-  logue history. This automatic adaptation allows appro-  priate dialogue strategies to be employed based on both  local dialogue context and dialogue history, and has been  shown to result in significantly better performance than  non-adaptive systems. Second, MIMIC employs an ini-  tiative module that is decoupled from the goal selection  process in the dialogue manager, while allowing the out-  come of both components ojointly determine the strate-  gies chosen for response generation. As a result, MIMIC  can exhibit drastically different dialogue behavior with  very minor adjustments o parameters in the initiative  module, allowing for rapid development and comparison  of experimental prototypes and resulting in general and  portable dialogue systems.  2 Adaptive Mixed Initiative Dialogue  Management  2.1 Motivation  In \u001b[1;32;40m natural \u001b[0;0mly occurring human-human dialogues, peakers  often adopt different dialogue strategies based on hearer  characteristics, dialogue history, etc. For instance, the  speaker may provide more guidance if the hearer is hav-  ing difficulty making progress toward task completion,  while taking a more passive approach when the hearer  is an expert in the domain. Our main goal is to enable  a spoken dialogue system to simulate such human be-  havior by dynamically adapting dialogue strategies dur-  ing an interaction based on information that can be au-  tomatically detected from the dialogue. Figure 1 shows  an excerpt from a dialogue between MIMIC and an ac-  tual user where the user is attempting to find the times  at which the movie Analyze This playing at theaters in  Montclair. S and U indicate system and user utterances,  respectively, and the italicized utterances are the output  of our automatic speech recognizer. In addition, each  system turn is annotated with its task and dialogue ini-  tiative holders, where task initiative tracks the lead in the  process toward achieving the dialogue participants' do-  main goal, while dialogue initiative models the lead in  determining the current discourse focus (Chu-Carroll and  Brown, 1998). In our information query application do-  main, the system has task (and thus dialogue) initiative if  its utterances provide helpful guidance toward achieving  the user's domain goal, as in utterances (6) and (7) where  MIMIC provided valid response choices to its query in-  tending to solicit a theater name, while the system has  97  dialogue but not task initiative if its utterances only spec-  ify the current discourse goal, as in utterance (4). i  This dialogue illustrates everal features of our adap-  tive mixed initiative dialogue manager. First, MIMIC au-  tomatically adapted the initiative distribution based on  information extracted from user utterances and dialogue  history. More specifically, MIMIC took over task initia-  tive in utterance (6), after failing to obtain a valid an-  swer to its query soliciting a missing theater name in (4).  It retained task initiative until utterance (12), after the  user implicitly indicated her intention to take over task  initiative by providing a fully-specified query (utterance  (11)) to a limited prompt (utterance (10)). Second, ini-  tiative distribution may affect the strategies MIMIC se-  lects to achieve its goals. For instance, in the context  of soliciting missing information, when MIMIC did not  have task initiative, a simple information-seeking query  was generated (utterance (4)). On the other hand, when  MIMIC had task initiative, additional guidance was pro-  vided (in the form of valid response choices in utterance  (6)), which helped the user successfully respond to the  system's query. In the context of prompting the user for  a new query, when MIMIC had task initiative, a lim-  ited prompt was selected to better constrain the user's  response (utterance (10)), while an open-ended prompt  was generated to allow the user to take control of the  problem-solving process otherwise (utterances (1) and  (13)).  In the next section, we briefly review a framework for  dynamic initiative modeling. In Section 3, we discuss  how this framework was incorporated into the dialogue  management component of a spoken dialogue system to  allow for automatic adaptation of dialogue strategies. Fi-  nally, we outline experiments evaluating the resulting  system and show that MIMIC's automatic adaptation ca-  pabilities resulted in better system performance.  2.2 An Evidential Framework for Modeling  Initiative  In previous work, we proposed a framework for mod-  eling initiative during dialogue interaction (Chu-Carroll  and Brown, 1998). This framework predicts task and dia-  logue initiative holders on a turn-by-turn basis in human-  human dialogues based on participant roles (such as each  dialogue agent's level of expertise and the role that she  plays in the application domain), cues observed in the  current dialogue turn, and dialogue history. More specif-  ically, we utilize the Dempster-Shafer theory (Shafer,  1976; Gordon and Shortliffe, 1984), and represent the  current initiative distribution as two basic probability as-  signments (bpas) which indicate the amount of support  for each dialogue participant having the task and dia-  logue initiatives. For instance, the bpa mt-cur({S}) = l Although the dialogues we collected in our experiments (Sec-  tion 5) include cases in which MIMIC has neither initiative, such cases  are rare in this application domain, and will not be discussed further in  this paper.  0.3, mt-c~,r({U}) = 0.7 indicates that, with all evidence  taken into account, there is more support (to the degree  0.7) for the user having task initiative in the current urn  than for the system. At the end of each turn, the bpas  are updated based on the effects that cues observed ur-  ing that turn have on changing them, and the new bpas  are used to predict he next task and dialogue initiative  holders.  In this framework, cues that affect initiative distribu-  tion include NoNewlnfo, triggered when the speaker sim-  ply repeats or rephrases an earlier utterance, implicitly  suggesting that the speaker may want to give up initia-  tive, AmbiguousActions, triggered when the speaker pro-  poses an action that is ambiguous in the application do-  main, potentially prompting the hearer to take over ini-  tiative to resolve the detected ambiguity, etc. The ef-  fects that each cue has on changing the current bpas  are also represented asbpas, which were determined by  an iterative training procedure using a corpus of tran-  scribed ialogues where each turn was annotated with the  task/dialogue initiative holders and the observed cues.  The bpas for the next turn are computed by combining  the bpas representing the current initiative distribution  and the bpas representing the effects of cues observed  during the current urn, using Dempster's combination  rule (Gordon and Shortliffe, 1984). The task and dia-  logue initiative holders are then predicted based on the  new bpas. This framework was evaluated using anno-  tated dialogues from four task-oriented domains, achiev-  ing, on average, a correct prediction rate of 97% and 88%  for task and dialogue initiative holders, respectively. In  Section 3.2, we discuss how this predictive model is con-  verted into a generative model by enabling the system to  automatically detect cues that were previously labelled  manually. We further discuss how the model is used  by the dialogue manager for dynamic dialogue strategy  adaptation.  3 MIMIC :  M ixed  Init iative Mov ie   In fo rmat ion  Consu l tant   MIMIC is a telephone-based dialogue system that pro-  vides movie showtime information. It consists of the fol-  lowing main components, implemented on a distributed,  client-server a chitecture (Zhou et al., 1997):  1.  .  Telephony server: this component detects rings  and hang-ups, and enables treaming of audio data  on channels of a telephony board.  Speech recognizer: the recognizer receives audio  data from the telephony server and generates the  word string hypothesis that best matches the audio  input. We used the Lucent Automatic Speech Rec-  ognizer (Reichl and Chou, 1998; Ortmanns et al.,  1999), configured to use class-based probabilistic n-  gram \u001b[1;32;40m language \u001b[0;0m models to allow for rapid updates of  movie/theater/town names.  _ql:l  98 Utterance  (1) S: Hello, this is MIMIC, the movie information system.  (2) How can I help you?  (3) U: What time is Analyze This playing in Montclair?  \\[what time is analyze this playing in montclair\\]  (4) S: What theater would you like? User  (5) U: Montclair.  \\[montclair\\]  (6) S: Choices for theater are Wellmont Theater and Clearviews Screening Zone. System  (7) What theater would you like?  (8) U: Clearviews Screening Zone.  \\[urn movie screening zone\\]  (9) S: Analyze This is playing at Clearviews Screening Zone in Montclair at l:15pm, 4:10pm, System  and 6:30pm.  (10) Please say the name of the movie or theater or town you would like information about.  (11) U: What time is Analyze This playing at the Welimont Theater?  \\[what time is analyze this playing at the wellmont theater\\]  (12) S: Analyze This is playing at Wellmont Theater in Montclair at 3:15pm, 5:45pm, 8:20pm, User  and lO:lOpm.  (13) Can I help you with anything else?  Task  Initiative  User  Dialogue  Initiative  System  System  System  System  System  Figure 1: Excerpt of an Adaptive Mixed Initiative Dialogue  3. NLP/Dialogue component: this main application-  dependent component receives a user utterance hy-  pothesis from the speech recognizer, and generates  system utterance(s) in response. Three major tasks  are carried out by this component: 1) semantic in-  terpretation, which constructs frame-based seman-  tic representations from user utterances, 2)dialogue  management, where response strategies are selected  based on the semantic representation f the user's  utterance, system's domain knowledge, and initia-  tive distribution, and 3) utterance generation, where  utterance templates are chosen and instantiated to  realize the selected response strategies. These three  tasks will be discussed in further detail in the rest of  this section.  4. Text-to-speech engine: the TTS system receives  the word string comprising the system's response  from the dialogue component and converts the text  into speech for output over the telephone. We used  the Bell Labs TTS system (Sproat, 1998), which  in addition to converting plain text into speech, ac-  cepts text strings annotated to override default pitch  height, accent placement, speaking rate, etc. 2  3.1 Semantic Interpretation  MIMIC utilizes a non-recursive frame-based semantic  representation commonly used in spoken dialogue sys-  tems (e.g. (Seneff et al., 1991; Lamel, 1998)), which  represents an utterance as a set of attribute-value pairs.  Figure 2(a) shows the frame-based semantic representa-  tion for the utterance \"What time is Analyze This playing  2 See (Nakatani and Chu-Carroll, 2000) for how MIMIC's dialogue-  level knowledge is used to override default prosodic assignments for  concept-to-speech generation.  Question-Type: When  Movie: Analyze This  Theater: null  Town: Montclair  (a) Semantic Representation  Question-Type: When  Movie: mandatory  Theater: mandatory  Town: optional  (b) Task Specification  Figure 2: Semantic Representation a d Task Specifica-  tion  in Montclair?\"  MIMIC's semantic representation is constructed by  first extracting, for each attribute, a set of keywords from  the user utterance. Using a vector-based topic identifi-  cation process (Salton, 1971; Chu-Carroll and Carpen-  ter, 1999), these keywords are used to determine a set  of likely values (including null) for that attribute. Next,  the utterance is interpreted with respect o the dialogue  history and the system's domain knowledge. This al-  lows MIMIC to handle elliptical sentences and anaphoric  references, as well as automatically infer missing values  and detect inconsistencies in the current representation.  This semantic representation allows for decoupling  of domain-dependent task specifications and domain-  99 independent dialogue management s rategies. Each  query type is specified by a template indicating, for each  attribute, whether a value must, must not, or can option-  ally be provided in order for a query to be considered  well-formed. Figure 2(b) shows that to solicit movie  showtime information (question type when), a movie  name and a theater name must be provided, whereas a town may optionally be provided. These specifications  are determined based on domain semantics, and must be  reconstructed when porting the system to a new domain.  3.2 Dialogue Management  Given a semantic representation, the dialogue history and  the system's domain knowledge, the dialogue manager  selects a set of strategies that guides MIMIC's response  generation process. This task is carried out by three  subprocesses: 1) initiative modeling, which determines  the initiative distribution for the system's dialogue turn,  2) goal selection, which identifies a goal that MIMIC's  response attempts to achieve, and 3) strategy selection,  which chooses, based on the initiative distribution, a set  of dialogue acts that MIMIC will adopt in its attempt to  realize the selected goal.  3.2.1 Initiative Modeling  MIMIC's initiative module determines the task and di-  alogue initiative holders for each system turn in order  to enable dynamic strategy adaptation. It automatically  detects cues triggered uring the current user turn, and  combines the effects of these cues with the current ini-  tiative distribution to determine the initiative holders for  the system's turn.  Cue Detection The cues and the bpas representing  their effects are largely based on a subset of those de-  scribed in (Chu-Carroll and Brown, 1998), 3 as shown  in Figures 3(a) and 3(b). Figure 3(a) shows that obser-  vation of TakeOverTask supports a task initiative shift  to the speaker to the degree .35. The remaining sup-  port is assigned to O, the set of all possible conclusions  (i.e., {speaker,hearer}), indicating that to the degree .65,  observation of this cue does not commit to identifying  which dialogue participant should have task initiative in  the next dialogue turn.  The cues used in MIMIC are classified into two cate-  gories, discourse cues and analytical cues, based on the  types of knowledge needed to detect hem:  I. Discourse cues, which can be detected by consider-  ing the semantic representation f the current utter-  ance and dialogue history:   TakeOverTask, an implicit indication that the  user wants to take control of the problem-  solving process, triggered when the user pro-  vides more information than the discourse x-  pectation.  3We selected only those cues that can be automatically detected in  a spoken dialogue system with speech recognition errors and limited  semantic interpretation capabilities.   NoNewlnfo, an indication that the user is un-  able to make progress toward task completion,  triggered when the semantic representations of  two consecutive user turns are identical (a re-  sult of the user not knowing what to say or the  speech recognizer failing to recognize the user  utterances).  2. Analytical cues, which can only be detected by tak-  ing into account MIMIC's domain knowledge:   lnvalidAction, an indication that the user made  an invalid assumption about the domain, trig-  gered when the system database lookup based  on the user's query returns null.   lnvalidActionResolved, triggered when the  previous invalid assumption is corrected.   AmbiguousAction, an indication that the user  query is not well-formed, triggered when a  mandatory attribute is unspecified or when  more than one value is specified for an at-  tribute.   AmbiguousActionResolved, triggered when the  attribute in question is uniquely instantiated.  Computing Initiative Distribution To determine the  initiative distribution, the bpas representing the effects  of cues detected in the current user utterance are instan-  tiated (i.e., speaker~hearer in Figure 3 are instantiated as  system~user accordingly). These effects are then inter-  preted with respect o the current initiative distribution  by applying Dempster's combination rule (Gordon and  Shortliffe, 1984) to the bpas representing the current ini-  tiative distribution and the instantiated bpas. This results  in two new bpas representing the task and dialogue initia-  tive distributions for the system's turn. The dialogue par-  ticipant with the greater degree of support for having the  task/dialogue initiative in these bpas is the task/dialogue  initiative holder for the system's turn 4 (see Section 4 for  an example).  3.2.2 Goal Selection  The goal selection module selects a goal that MIMIC at-  tempts to achieve in its response by utilizing informa-  tion from analytical cue detection as shown in Figure 4.  MIMIC's goals focus on two aspects of cooperative di-  alogue interaction: 1) initiating subdialogues to resolve  anomalies that occur during the dialogue by attempting  to instantiate an unspecified attribute, constraining an at-  tribute for which multiple values have been specified, or  correcting an invalid assumption i  the case of invalid  41n practice, this is the preferred initiative holder since practical  reasons may prevent the dialogue participant from actually holding the  initiative. For instance, if having task initiative dictates inclusion of  additional helpful information, this can only be realized if M1M1C's  knowledge base provides uch information.  \"INN 100 Cue Class  Discourse  Analytical  Cue  TakeOverTask  NoNewlnfo  InvalidAction  lnvalidActionResolved  AmbiguousAction  AmbiguousActionResolved  BPA  mt-tot({speaker}) = 0.35; mr-tot(O) = 0.65  mt-,~ni({hearer}) = 0.35; mt-nn~(O) = 0.65  mt-i~({hearer}) = 0.35; mt- ia(O) = 0.65  mt-iar({hearer}) = 0.35; mt- iar(O)  = 0.65  mt-aa({hearer}) = 0.35; mt-a~(O) = 0.65  mt . . . .  ({speaker}) = 0.35; mt . . . .  (O) = 0.65  (a)Task Initiative  Cue Class  Discourse  Analytical  Cue  TakeOverTask  NoNewlnfo  lnvalidAction  InvalidActionResolved  AmbiguousAction  AmbiguousActionResolved  BPA  md-tot({speaker}) = 0.35; ma-tot(O) = 0.65  md-nni({hearer}) = 0.35; md-nni(O) -~- 0.65  md-ia ({hearer}) = 0.7; md-ia (O) = 0.3  ma-iar({hearer}) = 0.7; ma-iar(O) = 0.3  ma-aa({hearer}) = 0.7; md_a~(O) = 0.3  ma . . . .  ({speaker}) = 0.7; md . . . .  (O) = 0.3  (b)Dialogue Initiative  Figure 3: Cues and BPAs for Modeling Initiative in MIMIC  Seleet-Goal(SemRep):  (1) IfAmbiguousAction detected  (2) ambiguous-attr +--get-ambiguous(SemRep)  /* get name of ambiguous attribute */  (3) If (number-values(ambiguous-attr) == 0)  /* attribute unspecified *,1  (4) Instantiate(ambiguous-attr)  (5) Else/* more than one value specified */  (6) Constrain(ambiguous-attr)  (7) Else if lnvalidAction detected  (8) ProvideNegativeAnswer(SemRep)  (9) Else/* well-formed query */  (10) answer +-- database-query(SemRep)  (11 ) ProvideAnswer(answer)  Figure 4: Goal Selection Algorithm  user queries (steps 1-8) 5 (van Beeket  al., 1993; Raskutti  and Zukerman, 1993; Qu and Beale, 1999), and 2) pro-  viding answers to well-formed queries (steps 9-11).  3.2.3 Strategy Selection  Previous work has argued that initiative affects the de-  gree of control an agent has in the dialogue interaction  (Whittaker and Stenton, 1988; Walker and Whittaker,  1990; Chu-Carroll and Brown, 1998). Thus, a cooper-  ative system may adopt different strategies to achieve the  same goal depending on the initiative distribution. Since  task initiative models contribution to domain/problem-  solving goals, while dialogue initiative affects the cur-  5An alternative strategy to step (4) is to perform adatabase lookup  based on the ambiguous query and summarize the results (Litman et  al., 1998), which we leave for future work.  rent discourse goal, we developed alternative strategies  for achieving the goals in Figure 4 based on initiative  distribution, as shown in Table 1.  The strategies employed when MIMIC has only dia-  logue initiative are similar to the mixed initiative dia-  logue strategies employed by many existing spoken di-  alogue systems (e.g., (Bennacef et al., 1996; Stent et  al., 1999)). To instantiate an attribute, MIMIC adopts  the lnfoSeek dialogue act to solicit the missing informa-  tion. In contrast, when MIMIC has both initiatives, it  plays a more active role by presenting the user with addi-  tional information comprising valid instantiations of the  attribute (GiveOptions). Given an invalid query, MIMIC  notifies the user of the failed query and provides an open-  ended prompt when it only has dialogue initiative. When  MIMIC has both initiatives, however, in addition to No-  tifyFailure, it suggests an alternative close to the user's  original query and provides a limited prompt. Finally,  when MIMIC has neither initiative, it simply adopts No-  tifyFailure, allowing the user to determine the next dis-  course goal.  3.3 Utterance Generat ion   MIMIC employs a simple template-driven utterance gen-  eration approach. Templates are associated with dialogue  acts as shown in Table 2.6 The generation component re-  ceives from the dialogue manager the selected ialogue  acts and the parameters needed to instantiate the tem-  plates. It then generates the system response, which is  sent to the TTS module for spoken output synthesis.  6In most cases, there is a one-to-one-mapping between dialogue  acts and templates. The exceptions are Answer, NotifyFailure, and  SuggestAlternative, whose templates vary based on the question type.  101  Task + Dialogue Dialogue None  Constrain Clarify Clarify Acknowledge  Instantiate GiveOptions lnfoSeek Acknowledge  InfoSeek  ProvideAnswer Answer Answer Answer  LimitedPrompt OpenPrompt  ProvideNegativeAnswer NotifyFailure NotifyFailure NotifyFailure  SuggestAltemative OpenPrompt  LimitedPrompt  Table 1: Strategy Selection Based on Goal and Initiative Distribution  Dialogue Act  Clarify  Acknowledge  GiveOptions  InfoSeek  Answer  OpenPrompt  LimitedPrompt  NotifyFailure  SuggestAltemative  Template  \"Did you say < valuel > .... or < valuen >.9\"  \"Uh-huh.\"  \"Choices for < attribute > are < valuex > ... < value, >7  \"What < attribute > would you like?\"  E.g., \"< movie > is playing at < theater > at < time1 > ... < time,, >\"  \"Can I help you with anything elseT'  \"Please say the name of the movie or theater or town you would like information about.\"  E.g., \"< movie > is not playing at < theater >.  E.g., \"< movie > is playing at < alternativetheater > at < timex > ... < timen >\"  Table 2: Mappings Between Dialogue Acts and Utterance Templates  4 Examples  To illustrate MIMIC's adaptation capabilities, we return  to the dialogue in Figure 1, which is repeated in Figure 5  and annotated with the cues detected in each user turn  (in boldfaced italics) and the dialogue acts employed for  response generation i each system turn (in boldface).  The bpas representing the initiative distribution for ut-  terance (3) are the initial bpas, which, based on MIMIC's  role as an information provider, are  mt-(3)({S}) = 0.3, mt-(3)({U}) = 0.7;  = 0.6, md- (3 ) ({V})  = 0.4.  The cue AmbiguousAction is detected in utterance (3)  because the mandatory attribute theater was not specified  and cannot be inferred (since the town of Montclair has  multiple theaters). The bpas representing its effect are  instantiated as follows (Figure 3):  mt-,,({S}) = 0.35, mt_ , , (O)  = 0.65;  md-aa({S}) = 0.7, md-aa(O) = 0.3.  Combining the current bpas with the effects of the ob-  served cue, we obtain the following new bpas:  mt-(4)({S}) = 0.4, mt_(a)({U}) = 0.6;  md_(4)({S}) = 0.83, md_(4)({U}) = 0.17.  The updated bpas indicate that MIMIC should have dia-  logue but not task initiative when attempting to resolve  the detected ambiguity in utterance (4).  MIMIC selects Instantiate as its goal to be achieved  (Figure 4), which, based on the initiative distribution,  leads it to select he InfoSeek action (Table I) and gener-  ate the query \"What heater would you like?\"  The user's response in (5) again triggers Ambiguous-  Action, as well as NoNewlnfo since the semantic repre-  sentations of (3) and (5) are identical, given the dialogue  context. When the effects of these cues are taken into  account, we have the following initiative distribution for  utterance (6):  mt-(6)({S}) = 0.62, mt_(6)({U}) = 0.38;  md-(6)({S}) = 0.96, rnd_(6)({V}) = 0.04.  Although Instaatiate is again selected as the goal,  MIMIC now has both task and dialogue initiatives; thus  it selects both GiveOptions and lnfoSeek to achieve this  goal and generates utterances (6) and (7). The addi-  tional information, in the form of valid theater choices,  helps the user provide the missing value in (8), allow-  ing MIMIC to answer the query in (9) and prompt for  the next query. However, despite the limited prompt,  the user provides a well-formed query in (11), trigger-  ing TakeOverTask. Thus, MIMIC answers the query and  switches to an open-ended prompt in (13), relinquishing  task initiative to the user.  In addition to its automatic adaptation capabilities, an-  other advantage of MIMIC is the ease of modifying its  adaptation behavior, enabled by the decoupling of the  initiative module from the goal and strategy selection  processes. For instance, a system-initiative rsion of  MIMIC can be achieved by setting the initial bpas as fol-  lows:  mt-initial({S}) = 1;  md--initial({S}) -~1.  102  (1) S: Hello, this is MIMIC, the movie information system.  (2) How can I help you?  (3) U: What time is Analyze This playing in Montclair?  \\[what time is analyze this playing in montclair\\]  (4) S: What theater would you like?  (5) U: Montclair.  \\[montclair\\]  (6) S: Choices for theater are Wellmont heater and Clearviews Screening Zone  (7) What theater would you like?  (8) U: Clearviews Screening Zone.  \\[um movie screening zone\\]  (9) S: Analyze This is playing at Clearviews Screening Zone in Montclair at 1:15pm,  4:10pm, and 6:30pm.  (10) Please say the name of the movie or theater or town you would like information  about.  (11) U: What time is Analyze This playing at the Wellmont heater?  \\[what time is analyze this playing at the wellmont theater\\]  (12) S: Analyze this is playing at Wellmont heater in Montclair at 3:15pm, 5:45pm,  8:20pm, and 10:10pm.  (13) Can I help you with anything else?  \\[AmbiguousAction\\]  \\[lnfoSeek\\]  \\[AmbiguousAction, NoNewlnfo\\]  \\[GiveOptions\\]  \\[InfoSeek\\]  \\[AmbiguousActionResolved\\]  \\[Answer\\]  \\[LimitedPrompt\\]  \\[TakeOverTask\\]  \\[Answer\\]  \\[OpenPrompt\\]  Figure 5: Annotated Dialogue Shown in Figure 1  This is because in the Dempster-Shafer theory, if the  initial bpas or the bpas for a cue provide definite evi-  dence for drawing a certain conclusion, then no subse-  quent cue has any effect on changing that conclusion.  Thus, MIMIC will retain both initiatives throughout the  dialogue. Alternatively, versions of MIMIC with differ-  ent adaptation behavior can be achieved by tailoring the  initial bpas and/or the bpas for each cue based on the ap-  plication. For instance, for an electronic sales agent, the  effect oflnvalidAction can be increased so that when the  user orders an out-of-stock item, the system will always  take over task initiative and suggest an alternative item.  5 System Evaluation  We conducted two experiments oevaluate MIMIC's au-  tomatic adaptation capabilities. We compared MIMIC  with two control systems: MIMIC-SI, a system-initiative  version of MIMIC in which the system retains both ini-  tiatives throughout the dialogue, and MIMIC-MI, a non-  adaptive mixed-initiative version of MIMIC that resem-  bles the behavior of many existing dialogue systems. In  this section we summarize these experiments and their  results. A companion paper describes the evaluation pro-  cess and results in further detail (Chu-Carroll and Nick-  erson, 2000).  Each experiment involved eight users interacting with  MIMIC and MIMIC-SI or MIMIC-MI to perform aset of  tasks, each requiring the user to obtain specific movie in-  formation. User satisfaction was assessed by asking the  subjects to fill out a questionnaire after interacting with  each version of the system. Furthermore, a number of  performance f atures, largely based on the PARADISE  dialogue valuation scheme (Walker et al., 1997), were  automatically logged, derived, or manually annotated. In  addition, we logged the cues automatically detected in  each user utterance, as well as the initiative distribution  for each turn and the dialogue acts selected to generate  each system response.  The features gathered from the dialogue interactions  were analyzed along three dimensions: system perfor-  mance, discourse features (in terms of characteristics  of the resulting dialogues, such as the cues detected in  user utterances), and initiative distribution. Our results  show that MIMIC's adaptation capabilities 1) led to bet-  ter system performance in terms of user satisfaction, dia-  logue efficiency (shorter dialogues), and dialogue quality  (fewer ASR timeouts), and 2) better matched user expec-  tations (by giving up task initiative when the user intends  to have control of the dialogue interaction) and more effi-  ciently resolved ialogue anomalies (by taking over task  initiative to provide guidance when no progress is made  in the dialogue, or to constrain user utterances when ASR  performance is poor).  6 Conclusions  In this paper, we discussed MIMIC, an adaptive mixed-  initiative spoken dialogue system. MIMIC's automatic  adaptation capabilities allow it to employ appropriate  strategies based on the cumulative ffect of information  dynamically extracted from user utterances during dia-  logue interactions, enabling MIMIC to provide more co-  operative and satisfactory responses than existing non-  adaptive systems. Furthermore, MIMIC was imple-  mented as a general framework for information query  systems by decoupling its initiative module from the  goal selection process, while allowing the outcome of  both processes to jointly determine the response strate-  gies employed. This feature nables easy modification to  MIMIC's adaptation behavior, thus allowing the frame-  work to be used for rapid development and comparisons  103  of experimental prototypes of spoken dialogue systems.  Acknowledgments  The author would like to thank Egbert Ammicht, An-  toine Saad, Qiru Zhou, Wolfgang Reichl, and Stefan  Ortmanns for their help on system integration and on  ASR/telephony server development, Jill Nickerson for  conducting the evaluation experiments, and Bob Carpen-  ter, Diane Litman, Christine Nakatani, and Jill Nickerson  for their comments on an earlier draft of this paper.  References  S. Bennacef, L. Devillers, S. Rosset, and L. Lamel.  1996. Dialog in the RAILTEL telephone-based sys-  tem. In Proceedings of the 4th International Confer-  ence on Spoken Language Processing.  Jennifer Chu-Carroll and Michael K. Brown. 1998. An  evidential model for tracking initiative in collabora-  tive dialogue interactions. User Modeling and User-  Adapted Interaction, 8(3-4):215-253.  Jennifer Chu-Carroll and Bob Carpenter. 1999. Vector-  based \u001b[1;32;40m natural \u001b[0;0m anguage call routing. Computational  Linguistics, 25(3):361-388.  Jennifer Chu-Carroll and Jill S. Nickerson. 2000. Evalu-  ating automatic dialogue strategy adaptation for a spo-  ken dialogue system. In Proceedings of the 1st Con-  ference of the North American Chapter of the Associ-  ation for Computational Linguistics. To appear.  Jean Gordon and Edward H. Shortliffe. 1984. The  Dempster-Shafer theory of evidence. In Bruce  Buchanan and Edward Shortliffe, editors, Rule-Based  Expert Systems: The MYCIN Experiments of the  Stanford Heuristic Programming Project, chapter 13,  pages 272-292. Addison-Wesley.  Lori Lamel. 1998. Spoken \u001b[1;32;40m language \u001b[0;0m dialog system de-  velopment and evaluation at LIMSI. In Proceedings  of the International Symposium on Spoken Dialogue,  pages 9-17.  Diane J. Litman and Shimei Pan. 1999. Empirically  evaluating an adaptable spoken dialogue system. In  Proceedings of the 7th International Conference on  User Modeling, pages 55-64.  Diane J. Litman, Shimei Pan, and Marilyn A. Walker.  1998. Evaluating response strategies in a web-based  spoken dialogue agent. In Proceedings of the 36th  Annual Meeting of the Association for Computational  Linguistics, pages 780-786.  Christine H. Nakatani and Jennifer Chu-Carroll. 2000.  Using dialogue representations forconcept-to-speech  generation. In Proceedings of the ANLP-NAACL  Workshop on Conversational Systems.  Stefan Ortmanns, Wolfgang Reichl, and Wu Chou. 1999.  An efficient decoding method for real time speech  recognition. In Proceedings of the 5th European Con-  ference on Speech Communication a d Technology.  Yan Qu and Steve Beale. 1999. A constraint-based  model for cooperative r sponse generation i informa-  tion dialogues. In Proceedings of the Sixteenth Na-  tional Conference on Artificial Intelligence.  Bhavani Raskutti and Ingrid Zukerman. 1993. Elicit-  ing additional information during cooperative consul-  tations. In Proceedings of the 15th Annual Meeting of  the Cognitive Science Society.  Wolfgang Reichl and Wu' Chou. 1998. Decision tree  state tying based on segmental c ustering for acoustic  modeling. In Proceedings of the International Confer-  ence on Acoustics, Speech, and Signal Processing.  Gerald Salton. 1971. The SMART Retrieval System.  Prentice Hall, Inc.  Stephanie Seneff, James Glass, David Goddeau, David  Goodine, Lynette Hirschman, Hong Leung, Michael  Phillips, Joseph Polifroni, and Victor Zue. 1991. De-  velopment and preliminary evaluation of the MIT  ATIS system. In Proceedings of the DARPA Speech  and Natural Language Workshop, ages 88-93.  Glenn Shafer. 1976. A Mathematical Theory of Evi-  dence. Princeton University Press.  Richard Sproat, editor. 1998. Multilingual Text-to-  Speech Synthesis: The Bell Labs Approach. Kluwer,  Boston, MA.  Amanda Stent, John Dowding, Jean Mark Gawron, Eliz-  abeth Owen Bratt, and Robert Moore. 1999. The  CommandTalk spoken dialogue system. In Proceed-  ings of the 37th Annual Meeting of the Association for  Computational Linguistics, pages 183-190.  Peter van Beek, Robin Cohen, and Ken Schmidt. 1993.  From plan critiquing to clarification dialogue for co-  operative response generation. Computational Intelli-  gence, 9(2):132-154.  Marilyn Walker and Steve Whittaker. 1990. Mixed ini-  tiative in dialogue: An investigation i to discourse  segmentation. In Proceedings of the 28th Annual  Meeting of the Association for Computational Lin-  guistics, pages 70-78.  Marilyn A. Walker, Diane J. Litman, Candance A.  Kamm, and Alicia Abella. 1997. PARADISE: A  framework for evaluating spoken dialogue agents. In  Proceedings of the 35th Annual Meeting of the Associ-  ation for Computational Linguistics, pages 271-280.  Steve Whittaker and Phil Stenton. 1988. Cues and con-  trol in expert-client dialogues. In Proceedings of the  26th Annual Meeting of the Association for Computa-  tional Linguistics, pages 123-130.  Qiru Zhou, Chin-Hui Lee, Wu Chou, and Andrew Pargel-  lis. 1997. Speech technology integration and research  platform: A system study. In Proceedings of the 5th  European Conference on Speech Communication and  Technology.  104  \n",
            "-----------------------------\n",
            "--- document 7: --- document 7: REES: A Large-Scale Relation and Event Extraction System  Chinatsu Aone  SRA International, Inc.  4300 Fair Lakes Court  Fairfax, VA 22033  aonec@verdi.sra.com  Mila Ramos-Santacruz  SRA International, Inc.  4300 Fair Lakes Court  Fairfax, VA 22033  mila@verdi.sra.com  Abstract  This paper reports on a large-scale, end-to-  end relation and event extraction system. At  present, the system extracts a total of 100  types of relations and events, which  represents a much wider coverage than is  typical of extraction systems. The system  consists of three specialized pattem-based  tagging modules, a high-precision co-  reference resolution module, and a  configurable template generation module.  We report quantitative valuation results,  analyze the results in detail, and discuss  future directions.  Introduction  One major goal of information extraction (IE)  technology is to help users quickly identify a  variety of relations and events and their key  players in a large volume of documents. In  contrast with this goal, state-of-the-art  information extraction systems, as shown in the  various Message Understanding Conferences  (MUCs), extract a small number of relations and  events. For instance, the most recent MUC,  MUC-7, called for the extraction of 3 relations  (person-employer, maker-product, and  organization-location) and 1 event (spacecraft  launches). Our goal is to develop an IE system  which scales up to extract as many types of  relations and events as possible with a minimum  amount of porting effort combined with high  accuracy. Currently, REES handles 100 types of  relations and events, and it does so in a modular,  configurable, and scalable manner.  Below, Section 1 presents the ontologies of  relations and events that we have developed.  Section 2 describes REES' system architecture.  Section 3 evaluates the system's performance,  and offers a qualitative analysis of system errors.  Section 4 discusses future directions.  1 Relation and Event Ontologies  As the first step in building a large-scale relation  and event extraction system, we developed  ontologies of the relations and events to be  extracted. These ontologies represent a wide  variety of domains: political, financial, business,  military, and life-related events and relations.  \"Relations\" covers what in MUC-7 are called  Template Elements (TEs) and Template  Relations (TRs). There are 39 types of relations.  While MUC TE's only dealt with singular  entities, REES extracts both singular and plural  entities (e.g., \"five executives\"). The TR  relations are shown in italic in the table below.  Relations  Place Relations 'Artifact Relations  Place-Name&Aliases  Place-Type  Place-Subtype  Place-Descriptor  Place-Country  Artifact-Name&Aliases  Artifact-Type  Artifact-Subtype  Artifact-Descriptor  Artifact-Maker  Artifact-Owner  Organization Relations Person Relations  Org-Name&Aliases  Org-Descriptor  Org-FoundationDate  Org-Nationality  Org-TickerSymbol  Org-Location  Org-P arentOrg  Org-Owner  Org-Founder  Org-StockMarket  Person-Name&Aliases  Person-Type  Person-Subtype  Person-Descriptor  Person-Honorific  Person-Age  Person-PhoneNumber  Person-Nationality  Person-Affiliation  Person-Sibling  Person-Spouse  Person-Parent  Person-Grandparent  76  Person-OtherRelative  Person-BirthPlace  Person-BirthDate  Table 1: Relation Ontology  \"Events\" are extracted along with their event  participants, e.g., \"who did what to whom when  and where?\" For example, for a BUYING  event, REES extracts the buyer, the artifact, the  seller, and the time and location of the BUYING  event. REES currently covers 61 types of  events, as shown below.  Events  Vehicle Transaction  Vehicle departs  Vehicle arrives  Spacecraft launch  Vehicle crash  Personnel Change  Hire  Terminate contract  Promote  Succeed  Start office  Buy artifact  Sell artifact  Import artifact  Export artifact  Give money  Business  Start business  Close business  Make artifact  Acquire company  Sell company  Sue organization  Merge company  Crime Financial  Sexual assault  Steal money  Seize drug  Indict  Arrest  Try  Convict  Sentence  Jail  Currency moves up  Currency moves down  Stock moves up  Stock moves down  Stock market moves up  Stock market moves down  Stock index moves up  Stock index moves down  Political Conflict  Nominate  Appoint  Elect  Expel person  Reach agreement  Hold meeting  Impose mbargo  Topple  Family  Die  Marry  Kill  Injure  Hijack vehicle  Hold hostages  Attack target  Fire weapon  Weapon hit  Invade land  Move forces  Retreat  Surrender  Evacuate  Table 2: Event Ontology  Figures 1 and 2 show sample relation and event  templates. Figure 1 shows a Person-Affiliation  relation template for \"Frank Ashley, a  spokesman for Occidental Petroleum Corp.'\"  <PERSON AFFILIATION-AP8802230207-54> :=  TYPE: PERSON AFFILIATION  PERSON: \\[TE for\"Frank Ashley\"\\]  ORG: \\[TE for \"Occidental Petroleum\"\\]  Figure 1: Example of Relation Template  Figure 2 shows an Attack Target event template  for the sentence \"an Iraqi warplane attacked the  frigate Stark with missiles May 17, 1987. \"  <ATTACK TARGET-AP8804160078-12>: =  i  TYPE: CONFLICT  SUBTYPE: ATTACK TARGET  ATTACKER: \\[TE for \"an Iraqi warplane\"\\]  TARGET: \\[TE for \"the frigate Stark\"\\]  WEAPON: \\[TE for \"missiles\"\\]  TIME: \"May 17, 1987\"  PLACE: \\[TE for \"the gulf'\\]  COMMENT: \"attacked\"  Figure 2: Example of Event Template  2 System Architecture and Components  Figure 3 illustrates the REES system  architecture. REES consists of three main  components: a tagging component (cf. Section  2.1), a co-reference resolution module (cf.  Section 2.2), and a template generation module  (cf. Section 2.3). Figure 3 also illustrates that  the user may run REES from a Graphical User  Interface (GUI) called TemplateTool (cf.  Section 2.4).  2.1 Tagging Modules  The tagging component consists of three  modules as shown in Figure 3: NameTagger,  NPTagger and EventTagger. Each module relies  on the same pattern-based xtraction engine, but  uses different sets of patterns. The NameTagger  recognizes names of people, organizations,  places, and artifacts (currently only vehicles).  77   remplateroot / /v   - ' : . v \"  . . . . . . . .   GUI interaction    Figure 3: The REES System Architecture  The NPTagger then takes the XML-tagged  output of the NameTagger through two phases.  First, it recognizes non-recursive Base Noun  Phrase (BNP) (our specifications for BNP  resemble those in Ramshaw and Marcus 1995).  Second, it recognizes complex NPs for only  the four main semantic types of NPs, i.e.,  Person, Organization, Location, and Artifact  (vehicle, drug and weapon). It makes post-  modifier attachment decisions only for those  NPs that are crucial to the extraction at hand.  During this second phase, relations which can  be recognized locally (e.g., Age, Affiliation,  Maker) are also recognized and stored using  the XML attributes for the NPs. For instance,  the XML tag for \"President of XYZ Corp.\"  below holds an AFFILIATION attribute with  the ID for \"XYZ Corp.\"  <PNP ID=\"03\" AFFILIATION=\"O4\">President of  <ENTITY ID=\"04\">XYZ Corp.</ENTITY>  </PNP>  Building upon the XML output of the  NPTagger, the EventTagger ecognizes  events applying its lexicon-driven,  syntactically-based generic patterns. These  patterns tag events in the presence of at  least one of the arguments specified in the  lexical entry for a predicate. Subsequent  pattems try to find additional arguments as  well as place and time adjunct information  for the tagged event. As an example of the  EventTagger's generic patterns, consider  the simplified pattern below. This pattem  matches on an event-denoting verb that  requires a direct object of type weapon  (e.g., \"fire a gun\")  (&  {AND $VP {ARG2_SYN=DO}  {ARG2_SEM=WEAPON } }  {AND $ARTIFACT {SUBTYPE=WEAPON} })1  The important aspect of REES is its  declarative, lexicon-driven approach. This  approach requires a lexicon entry for each  event-denoting word, which is generally a  I &=concatenation, AND=Boolean operator, $VP  and SARTIFACT are macro references for complex  phrases.  71:1  verb. The lexicon entry specifies the syntactic  and semantic restrictions on the verb's  arguments. For instance, the following lexicon  entry is for the verb \"attack.\" It indicates that  the verb \"attack\" belongs to the CONFLICT  ontology and to the ATTACK_TARGET type.  The first argument for the verb \"attack\" is  semantically an organization, location, person,  or artifact (ARGI_SEM), and syntactically a  subject (ARGI_SYN). The second argument  is semantically an organization, location,  person or artifact, and syntactically a direct  object. The third argument is semantically a  weapon and syntactically a prepositional  phrase introduced by the preposition \"with\".  ATTACK { { {CATEGORY VERB}  {ONTOLOGY CONFLICT}  {TYPE ATTACK_TARGET}  {ARGI_SEM {ORGANIZATION LOCATION  PERSON ARTIFACT} }  {ARGI_SYN {SUBJECT} }  {ARG2_SEM {ORGANIZATION LOCATION  PERSON ARTIFACT} }  {ARG2_SYN {DO}   {ARG3_SEM{WEAPON}   {ARG3_SYN {WITH} } } }  About 50 generic event extraction patterns,  supported by lexical information as shown  above, allow extraction of events and their  arguments in cases like:  An lraqi warplane attacked the frigate Stark  with missiles May 17, 1987.  This generic, lexicon-driven event extraction  approach makes REES easily portable because  new types of events can be extracted by just  adding new verb entries to the lexicon. No  new patterns are required. Moreover, this  approach allows for easy customization  capability: a person with no knowledge of the  pattern \u001b[1;32;40m language \u001b[0;0m would be able to configure  the system to extract new events.  While the tagging component is similar to  other pattern-based IE systems (e.g., Appelt et  al. 1995; Aone et al. 1998, Yangarber and  Grishman 1998), our EventTagger is more  portable through a lexicon-driven approach.  2.2 Co-reference Resolution  After the tagging phase, REES sends the XML  output through a rule-based co-reference  resolution module that resolves:   definite noun phrases of Organization,  Person, and Location types, and   singular person pronouns: he and she.  Only \"high-precision\" rules are currently  applied to selected types of anaphora. That is,  we resolve only those cases of anaphora whose  antecedents the module can identify with high  confidence. For example, the pronoun rules  look for the antecedents only within 3  sentences, and the definite NP rules rely  heavily on the head noun matches. Our high-  precision approach results from our  observation that unless the module is very  accurate (above 80% precision), the co-  reference module can hurt the overall  extraction results by over-merging templates.  2.3 Template Generation Module  A typical template generation module is a  hard-coded post-processing module which has  to be written for each type of template. By  contrast, our Template Generation module is  unique as it uses declarative rules to generate  and merge templates automatically so as to  achieve portability.  2.3.1 Declarative Template Generation  REES outputs the extracted information in the  form of either MUC-style templates, as  illustrated in Figure 1 and 2, or XML. A  crucial part of a portable, scalable system is to  be able to output different ypes of relations  and events without changing the template  generation code. REES maps XML-tagged  output of the co-reference module to templates  using declarative template definitions, which  specifies the template label (e.g.,  ATTACK_TARGET), XML attribute names  (e.g., ARGUMENT l), corresponding template  slot names (e.g., ATTACKER), and the type  restrictions on slot values (e.g., string).  79  2.3.2 Event Merging  One of the challenges of event extraction is to  be able to recognize and merge those event  descriptions which refer to the same event.  The Template Generation module uses a set of  declarative, customizable rules to merge co-  referring events into a single event. Often, the  rules reflect pragmatic knowledge of the world.  For example, consider the rule below for the  DYING event ype. This rule establishes that  if two die events have the same subject, then  they refer to the same event (i.e., a person  cannot die more than once).  {merge  {EVENT 1 {AND {SUBTYPE DIE} {PERSON  $foo}}  {EVENT 2 {AND {SUBTYPE DIE} {PERSON  $foo}}}  2.4 Graphical User Interface (GUI)  For some applications such as database  population, the user may want to validate the  system output. REES is provided with a Java-  based Graphical User Interface that allows the  user to run REES and display, delete, or  modify the system output. As illustrated in  Figure 4, the tool displays the templates on the  bottom half of the screen, and the user can  choose which template to display. The top half  of the screen displays the input document with  extracted phrases in different colors. The user  can select any slot value, and the tool will  highlight the portion of the input text  responsible for the slot value. This feature is  very useful in efficiently verifying system  output. Once the system's output has been  verified, the resulting templates can be saved  and used to populate adatabase.  3 System Evaluat ion  The table below shows the system's recall,  precision, and F-Measure scores for the  training set (200 texts) and the blind set (208  texts) from about a dozen news sources. Each  set contains at least 3 examples of each type of  relations and events. As we mentioned earlier,  \"relations\" includes MUC-style TEs and TRs.  Text Task Templates R P F-M  Set in keys  Rel. 9955 76 74 75.35  Train Events 2525 57 74 64.57  Rel. & 10707 74 74 73.95  Events  Rel. 8938 74 74 73.74  Blind Events 2020 42 75 53.75  Rel. & 9526 69 74 71.39  Events  Table 3: Evaluation Results  The blind set F-Measure for 31 types of  relations (73.95%) exceeded our initial goal of  70%. While the blind set F-Measure for 61  types of events was 53.75%, it is significant to  note that 26 types of events achieved an F-  Measure over 70%, and 37 types over 60% (cf.  Table 4). For reference, though not exactly  comparable, the best-performing MUC-7  system achieved 87% in TE, 76% in TR, and  51% in event extraction.  F-M in Event types  blind set  90-100 2 : Buy artifact. Marry  80-89 9 : Succeed, Merge company, Kill,  Surrender, Arrest, Convict, Sentence,  Nominate, Expel.  70-79 15 : Die, Sell artif~/ct, Export  Artifact, Hire, Start office, Make  artifact, Acquire company, Sue  organization, Stock Index moves  down, Steal money, Indict, Jail,  Vehicle crash, Elect, Hold meeting.  Table 4: Top-performing Event Types  80   Figure 4: TemplateTool  Regarding relation extraction, the difference in  the score between the training and blind sets  was very small. In fact, the total F-Measure on  the blind set is less than 2 points lower than  that of the training set. It is also interesting to  note that for 8 of the 12 relation types where  the F-Measure dropped more than 10 points,  the training set includes less than 20 instances.  In other words, there seems to be a \u001b[1;32;40m natural \u001b[0;0m  correlation between low number of instances in  the training set and low performance in the  blind set.  There was a significant drop between the  training and blind sets in event extraction: 11  points. We believe that the main reason is that  the total number of events in the training set is  fairly low: 801 instances of 61 types of events  (an average of 13/event), where 35 of the event  types had fewer than 10 instances. In fact, 9  out of the 14 event types which scored lower  than 40% F-Measure had fewer than I0  examples. In comparison, there were 34,000  instances of 39 types of relations in the training  set.  The contribution of the co-reference module is  illustrated in the table below. Co-reference  resolution consistently improves F-Measures  both in training and blind sets. Its impact is  larger in relation than event extraction.  Text set Task Co- No co-  reference reference  rules rules  Relations 75.35 72.54  Training Events 64.57 63.62  Relations 73.95 71.34  & Events  Relations 73.74 72.03  Blind Events 53.75 53.22  71.39 69.86 Relations  & Events  Table 5: Comparative results with and without  co-reference rules  In the next two sections, we analyze both false  positives and false negatives.  81  3.1 False Positives (or Precision Errors)  REES produced precision errors  following cases:   Most of the errors were due  in the  to over-  generation of templates. These are mostly  cases of co-referring noun phrases that the  system failed to resolve. For example:  \"Panama ... the nation ... this country.., his  country\"  Rules for the co-reference module are still  under development, and at present REES  handles only limited types of plural noun  phrase anaphora.  Spurious events resulted from verbs in  conditional constructions (e.g., \"if ...  then...\") or from ambiguous predicates.  For instance, \"appoint\" as a POLITICAL  event vs. a PERSONNEL CHANGE  event.  The subject of a verb was misidentified.  This is particularly frequent in reduced  relative clauses.  Kabul radio said the latest deaths brought  to 38 the number of  people killed in the  three car bomb explosions,  (Wrong subject: \"the number of people\" as  the KILLER instead of the victim)  3.2 False Negatives (or Recall Errors)  Below, we list the most frequent recall errors  in the training set.   Some event arguments are mentioned with  event nouns instead of event verbs. The  current system does not handle noun-based  event extraction.  India's acquisition last month of the  nuclear submarine from the Soviet  Union...  (SELLER=\"Soviet Union\" and  TIME=\"last month'\" come with the noun-  based event \"acquisition.\")   Pronouns \"it\" and \"they,\" which carry  little semantic information, are currently  not resolved by the co-reference module.  It also has bought hree late-1970s vintage  ICilo class Soviet submarines and two West  German HDW 209 subs  (Missed BUYER=India because of  unresolved it.)   Verb arguments are a conjunction of noun  phrases. The current system does not  handle coordination of verb arguments.  Hezbollah killed 21 lsraelis and 43 of  Lahad's oldiers  (The system gets only the first object: 21  Israelis. )  Ellipsis cases. The current system does not  handle ellipsis.  The two were sentenced to five-year prison  terms with hard labor by the state security  court...  (Missed PERSON_SENTENCED fill  because of unresolved the two.)   The subject of the event is relatively far  from the event-denoting verb:  Vladislav Listyev, 38, who brought  television interview shows in the style of  Phil Donahue or Larry King to Russian  viewers and pioneered hard-hitting  television journalism in the 1980s, was  shot in the heart by unknown assailants  and died immediately...  (The system missed subject Vladislav  Listyev for attack event shot)   Missed ORG LOCATION relations for  locations that are part of the organization's  name.  Larnaca General Hospital  (Missed ORG_LOCATION TR for this  and Larnaca. )  We asked a person who is not involved in the  development of REES to review the event  extraction output for the blind set. This person  reported that:   In 35% of the cases where the REES  system completely missed an event, it was  because the lexicon was missing the  predicate. REES's event predicate lexicon  is rather small at present (a total of 140  verbs for 61 event types) and is mostly  based on the examples found in the  training set,   In 30% of the cases, the subject or object  was elliptical. The system does not  currently handle ellipsis.  82   In 25% of the cases, syntactic/semantic  argument structures were missing from  existing lexical entries.  It is quite encouraging that simply adding  additional predicates and predicate argument  structures to the lexicon could significantly  increase the blind set performance.  4 Future Directions  We believe that improving co-reference  resolution and adding noun-based event  extraction capability are critical to achieving  our ultimate goal of at least 80% F-Measure  for relations and 70% for events.  4.1 Co-reference Resolution  As discussed in Section 3.1 and 3.2, accurate  co-reference r solution is crucial to improving  the accuracy of extraction, both in terms of  recall and precision. In particular, we  identified two types of high-payoff co-  reference r solution:   definite noun phrase resolution, especially  plural noun phrases   3 rd person neutral pronouns \"it\" and  \"they.\"  4.2 Noun-based Event Extraction  REES currently handles only verb-based  events. Noun-based event extraction adds  more complexity because:  Nouns are often used in a generic, non-  referential manner (e.g., \"We see a merger  as being in the consumer's interest\"), and  When referential, nouns often refer to  verb-based events, thus requiring noun-  verb co-reference resolution (\"An F-14  crashed shortly after takeoff... The crash\").  However, noun-based events are crucial  because they often introduce additional key  information, as the underlined phrases below  indicate:  While Bush's meetings with prominent anti-  apartheid leaders uch as Archbishop  Desmond Tutu and Albertina Sisulu are  important...  We plan to develop a generic set of patterns for  noun-based event extraction to complement the  set of generic verb-based extraction patterns.  5 Conclusions  In this paper, we reported on a fast, portable,  large-scale event and relation extraction system  REES. To the best of our knowledge, this is  the first attempt to develop an IE system which  can extract such a wide range of relations and  events with high accuracy. It performs  particularly well on relation extraction, and it  achieves 70% or higher F-Measure for 26 types  of events already. In addition, the design of  REES is highly portable for future addition of  new relations and events.  Acknowledgements  This project would have not been possible  without the contributions of Arcel Castillo,  Lauren Halverson, and Sandy Shinn. Our  thanks also to Brandon Kennedy, who  prepared the hand-tagged data.  References  Aone, Chinatsu, Lauren Halverson, Tom Hampton,  and Mila Ramos-Santacruz. 1998. \"SRA:  Description of the IE 2 System Used for MUC-7.\"  In Proceedings ofthe 7thMessage Understanding  Conference (MUC-7).  Appelt, Douglas E., Jerry R Hobbs, John Bear,  David Israel, Megumi Kameyama, Andy Kehler,  David Martin, Karen Myers, and Mabry Tyson.  1995. \"SRI International FASTUS System: MUC-  6 Test Results and Analysis.\" In Proceedings of  the 6 th Message Understanding Conference  (MUC-6).  Ramshaw, Lance A., and Mitchell P. Marcus. 1995.  \"Text Chunking Using Transformation-Based  Learning\". In Proceedings of the 3 rd ACL  Workshop on Very Large Corpora (WVLC95).  Yangarber, Roman and Ralph Grishman. 1998.  \"NYU: Description of the Proteus~PET System as  Used for MUC-7 ST.\" In Proceedings of the 6 th  Message Understanding Conference (MUC-7).  83  \n",
            "-----------------------------\n",
            "--- document 8: --- document 8: Unit Completion for a Computer-aided Translation  System  Ph i l ippe  Lang la i s ,  George  Foster  and  Guy  Lapa lme  RAL I  / D IRO  Universit6 de Montrea l   C.P. 6128, succursale Centre-vi l le  Montra l  (Qubec) ,  Canada,  H3C 3J7  { f elipe,f oster, lapalme }@iro. umontreal, ca  Typing  Abst ract   This work is in the context of TRANSTYPE, a sys-  tem that observes its user as he or she types a trans-  lation and repeatedly suggests completions for the  text already entered. The user may either accept,  modify, or ignore these suggestions. We describe the  design, implementation, and performance of a pro-  totype which suggests completions of units of texts  that are longer than one word.  1 I n t roduct ion   TRANSTYPE is part of a project set up to explore  an appealing solution to Interactive Machine Trans-  lation (IMT). In constrast to classical IMT systems,  where the user's role consists mainly of assisting the  computer to analyse the source text (by answering  questions about word sense, ellipses, phrasal attach-  ments, etc), in TRANSTYPE the interaction is direct-  ly concerned with establishing the target ext.  Our interactive translation system works as fol-  lows: a translator selects a sentence and begins typ-  ing its translation. After each character typed by  the translator, the system displays a proposed com-  pletion, which may either be accepted using a spe-  cial key or rejected by continuing to type. Thus  the translator remains in control of the translation  process and the machine must continually adapt it-  s suggestions in response to his or her input. We  are currently undertaking a study to measure the  extent o which our word-completion prototype can  improve translator productivity. The conclusions of  this study will be presented elsewhere.  The first version of TrtANSTYPE (Foster et al.,  1997) only proposed completions for the current  word. This paper deals with predictions which ex-  tend to the next several words in the text. The po-  tential gain from multiple-word predictions can be  appreciated in the one-sentence translation task re-  ported in table 1, where a hypothetical user saves  over 60% of the keystrokes needed to produce a  translation i a word completion scenario, and about  85% in a \"unit\" completion scenario.  In all the figures that follow, we use different fonts  to differentiate he various input and output: italics  are used for the source text, sans-serif for characters  typed by the user and typewr i te r - l i ke  for charac-  ters completed by the system.  The first few lines of the table 1 give an idea of  how TransType functions. Let us assume the unit s-  cenario (see column 2 of the table) and suppose that  the user wants to produce the sentence \"Ce projet  de loi est examin~ ~ la chambre des communes\" as a  translation for the source sentence \"This bill is ex-  amined in the house of commons\". The first hypoth-  esis that the system produces before the user enters  a character is lo i  (law). As this is not a good guess  from TRANSTYPE the user types the first character  (c) of the words he or she wants as a translation.  Taking this new input into account, TRANSTYPE  then modifies its proposal so that it is compatible  whith what the translator has typed. It suggests  the desired sequence ce projet de Ioi, which the user  can simply validate by typing a dedicated key. Con-  tinuing in this way, the user and TRANSTYPE alter-  nately contribute to the final translation. A screen  copy of this prototype is provided in figure 1.  2 The  Core  Eng ine   The core of TRANSTYPE is a completion engine  which comprises two main parts: an evaluator which  assigns probabilistic scores to completion hypotheses  and a generator which uses the evaluation function  to select he best candidate for completion.  2.1 The  Eva luator   The evaluator is a function p(t\\[t', s) which assigns to  each target-text unit t an estimate of its probability  given a source text s and the tokens t' which precede  t in the current ranslation of s. 1 Our approach to  modeling this distribution is based to a large extent  on that of the IBM group (Brown et al., 1993), but  it differs in one significant aspect: whereas the IB-  M model involves a \"noisy channel\" decomposition,  we use a linear combination of separate prediction-  s from a \u001b[1;32;40m language \u001b[0;0m model p(tlt ~) and a translation  model p(tls ). Although the noisy channel technique  1We assume the existence of a determinist ic  procedure for  tokenizing the target text.  135  This bill is examined in the house of commons  word-completion task unit-completion task  ce  projet  de  Ioi  est  examin~  chambre  des  communes  preL completions  ce+ / lo i   C/'  p+ /es t   p / ro je t   d+ / t rbs   d/e  I+ / t=~s  I /o i   e+ /de  e / s t   e+ /en  e/xamin6  ~+ /par  ~/ 1~  + /chambre  de+ /co,~unes  d/e  + /communes   de/s  pref. completions  c-l- /loJ.  c/e pro je t  de 1oi  e+ /de  e / s t   ex+ /~ la  chambre des communes.  + /b l a  chambre des con~unes  e/n  ex /min~  Table 1: A one-sentence s ssion illustrating the word- and unit-completion tasks. The first column indicates  the target words the user is expected to produce. The next two columns indicate respectively the prefixes  typed by the user and the completions proposed by the system in a word-completion task. The last two  columns provide the same information for the unit-completion task. The total number of keystrokes for  both tasks is reported in the last line. + indicates the acceptance key typed by the user. A completion is denoted by a/13 where a is the typed prefix and 13 the completed part. Completions for different prefixes  are separated by .  is powerful, it has the disadvantage that p(slt' , t) is  more expensive to compute than p(tls ) when using  IBM-style translation models. Since speed is cru-  cial for our application, we chose to forego the noisy  channel approach in the work described here. Our  linear combination model is described as follows:  pCtlt',s) = pCtlt') a(t ' ,s)  + pCtls) \\[1 - exit',s)\\] (1)   ~   v J  \u001b[1;32;40m language \u001b[0;0m translation  where a(t', s) E \\[0, 1\\] are context-dependent inter-  polation coefficients. For example, the translation  model could have a higher weight at the start of a  sentence but the contribution of the \u001b[1;32;40m language \u001b[0;0m mod-  el might become more important in the middle or  the end of the sentence A study of the weightings  for these two models is described elsewhere In the  work described here we did not use the contribution  of the \u001b[1;32;40m language \u001b[0;0m model (that is, a(t' ,  s) = O, V t', s).  Techniques for weakening the independence as-  sumptions made by the IBM models 1 and 2 have  been proposed in recent work (Brown et al., 1993;  Berger et al., 1996; Och and Weber, 98; Wang and  Waibel, 98; Wu and Wong, 98). These studies report  improvements on some specific tasks (task-oriented  limited vocabulary) which by nature are very differ-  ent from the task TRANSTYPE is devoted to. Fur-  thermore, the underlying decoding strategies are too  time consuming for our application We therefore  use a translation model based on the simple linear in-  terpolation given in equation 2 which combines pre-  dictions of two translation models - -  Ms and M~ - -   both based on IBM-like model 2(Brown et al., 1993).  Ms was trained on single words and Mu, described  in section 3, was trained on both words and units.  - -  _ (2 )   word unit  where Ps and Pu stand for the probabilities given re-  spectively by Ms and M~. G(s) represents he new  sequence of tokens obtained after grouping the to-  kens of s into units. The grouping operator G is  illustrated in table 2 and is described in section 3.  2.2  The  Generator   The task of the generator is to identify units that  match the current prefix typed by the user, and pick  the best candidate according to the evaluator. Due  to time considerations, the generator introduces a division of the target vocabulary into two parts: a  small active component whose contents are always  searched for a match to the current prefix, and a  much larger passive part over (380,000 word form-  s) which comes into play only when no candidates  are found in the active vocabulary. The active part  is computed ynamically when a new sentence is s-  elected by the translator. It is composed of a few  entities (tokens and units) that are likely to appear  in the translation. It is a union of the best can-  didates provided by each model Ms and M~ over  the set of all possible target tokens (resp. units)  that have a non-null translation probability of being  translated by any of the current source tokens (resp.  units). Table 2 shows the 10 most likely tokens and  units in the active vocabulary for an example source  sentence.  136  that.  is  what .  the . p r ime,  minister . said   and .  i  have.  outlined what .  has .   happened . since then . .   c' - est. ce -que ,  le- premier - ministre, a-  d i t . , .e t . j ' ,  ai. r4sum4- ce. qui .s ' -  est-  produit - depuis  .  g(s) that is what  the prime minister said  , and i   have . outlined  what has happened  since  then  .  As  A~     es t   ce   m in i s t re   que .  e t   a  p remier   l i e   ce  qu i  s' es t  p rodu i t   e t  je  - c '  es t  ce  que .  vo i l~   ce  que   qu '  es t  - c '  es t   ,  e t   le p remier  min is t re   d i sa i t   Table 2: Role of the generator for a sample pair of  sentences (t is the translation of s in our corpus).  G(s) is the sequence of source tokens recasted by  the grouping operator G. A8 indicates the 10 best  tokens according to the word model, Au the 10 best  units according to the unit model.  3 Mode l ing  Un i t  Assoc ia t ions   Automatically identifying which source words or  groups of words will give rise to which target words  or groups of words is a fundamental problem which  remains open. In this work, we decided to proceed  in two steps: a) monolingually identifying roups of  words that would be better handled as units in a giv-  en context, and b) mapping the resulting source and  target units. To train our unit models, we used a  segment of the Hansard corpus consisting of 15,377  pairs of sentences, totaling 278,127 english token-  s (13,543 forms) and 292,865 french tokens (16,399  forms).  3.1 F inding Monol ingual  Uni ts   Finding relevant units in a text has been explored in  many areas of \u001b[1;32;40m natural \u001b[0;0m anguage processing. Our ap-  proach relies on distributional and frequency statis-  tics computed on each sequence of words found in a  training corpus. For sake of efficiency, we used the  suffix array technique to get a compact representa-  tion of our training corpus. This method allows the  efficient retrieval of arbitrary length n-grams (Nagao  and Mori, 94; Haruno et al., 96; Ikehara et al., 96;  Shimohata et al., 1997; Russell, 1998).  The literature abounds in measures that can help  to decide whether words that co-occur are linguisti-  cally significant or not. In this work, the strength of  association of a sequence of words w\\[ = w l , . . . ,  wn  is computed by two measures: a likelihood-based one  p(w'~) (where g is the likelihood ratio given in (Dun-  ning, 93)) and an entropy-based one e(w'~) (Shimo-  hata et al., 1997). Letting T stand for the training  text and m a token:  p(w~) = argming(w~, uS1  ) (3)  ie\\]l,n\\[  e(w'~) = 0.5x  +k  ~rnlw,~meT h ( Ireq(w'~ m) k Ir~q(wT) \\]  Intuitively, the first measurement accounts for the  fact that parts of a sequence of words that should  be considered as a whole should not appear often by  themselves. The second one reflects the fact that a  salient unit should appear in various contexts (i.e.  should have a high entropy score).  We implemented a cascade filtering strategy based  on the likelihood score p, the frequency f ,  the length  l and the entropy value e of the sequences. A  first filter (.~\"1 (lmin, fmin, Pmin, emin)) removes any  sequence s for which l (s) < lmin or p(s) < Pmin  or e(s) < e,nin or f ( s )  < fmin.  A second filter  (~'2) removes sequences that are included in pre-  ferred ones. In terms of sequence reduction, apply-  ing ~1 (2, 2, 5.0, 0.2) on the 81,974 English sequences  of at least two tokens een at least twice in our train-  ing corpus, less than 50% of them (39,093) were fil-  tered: 17,063 (21%) were removed because of their  low entropy value, 25,818 (31%) because of their low  likelihood value.  3.2 Mapping  Mapping the identified units (tokens or sequences) to  their equivalents in the other \u001b[1;32;40m language \u001b[0;0m was achieved  by training a new translation model (IBM 2) us-  ing the EM algorithm as described in (Brown et al.,  1993). This required grouping the tokens in our  training corpus into sequences, on the basis of the  unit lexicons identified in the previous tep (we will  refer to the results of this grouping as the sequence-  based corpus). To deal with overlapping possibilities,  we used a dynamic programming scheme which opti-  mized a criterion C given by equation 4 over a set S  of all units collected for a given \u001b[1;32;40m language \u001b[0;0m plus all sin-  gle words. G(w~) is obtained by returning the path  that maximized B(n) .  We investigated several C-  criteria and we found C~--a length-based measurc  to be the most satisfactory. Table 2 shows an output  of the grouping function.  Oi l  i=o   B( i )  = argmax  /~\\[1,i\\[ ,w~_les ) + B( i  - I - 1) (4)  0 i f j<=i   with: Cl (w~)= j - - i  + l e lse  137  source unit (s)  we have 1748  we must 720  this bill 640  people of canada 282  mr. speaker : 269  what is happening 190  of course , 178  is it the pleasure of the house to 14  adopt the  the world  child care  the free trade agreement  post-secondary education  the first time  the canadian aviation safety board  the next five years  the people of china  f(8) target units (\\[a,p\\])  \\[nous,0.49\\] \\[avons,0.41\\] \\[, nous avons,0.07\\]  \\[nous devons,0.61\\] \\[ilrant,0.19\\] [nous,0.14\\]  \\[ce projet de 1oi,0.35\\] \\[projet de loi .,0.21\\] [projet de loi,0.18\\]  \\[les canadiens,0.26\\] \\[des canadiens,0.21\\] \\[la population,0.07\\]  \\[m. le prdsident :,0.80\\] [a,0.07\\] \\[h la,0.06\\]  Ice qui se passe,0.21\\] Ice qui se,0.16\\] [et,0.15\\]  \\[dvidemment ,0.26\\] \\[naturellement,0.08\\] \\[bien stir,0.08\\]  \\[plait-il h la chambre d' adopter,0.49\\] \\[la motion ?,0.42\\] [motion  ?,0.04\\]  201 \\[le monde,O.46\\] [du monde,O.33\\] lie monde entier,O.19\\]  86 lies garderies,O.59\\] \\[la garde d' enfants,O.23\\] \\[des services de  garde d' enfants,O.13\\]  75 \\[1' accord de libre-dchange,O.96\\] \\[la ddcision du gatt,O.04\\]  66 \\[1' euseignement postsecondaize,O.75\\] \\[1' dducation postsec-  ondaire,O.15\\] \\[des fonds,O.06\\]  62 \\[la premiere fois,l.00\\]  36 lie bureau canadien de la s~urit~ adrienne,O.55\\] \\[du bureau cana-  dien de la sdcurit~ adrienne,O.31\\] \\[1'un,O.14\\]  26 \\[au cours des cinq prochaines ann~es,O.53\\] \\[cinq prochaines an-  ndes,O.27\\] \\[25 milliards de d ollars,O.lO\\]  17 \\[le peuple chinois,0.38\\] \\[la population chinoise,0.25\\] \\[les chi-  nois,O.13\\]  Table 3: Bilingual associations. The first column indicates a source unit, the second one its frequency in the  training corpus. The third column reports its 3-best ranked target associations (a being a token or a unit,  p being the translation probability). The second half of the table reports NP-associations obtained after the  filter described in the text.  We investigated three ways of estimating the pa-  rameters of the unit model. In the first one, El,  the translation parameters are estimated by apply-  ing the EM algorithm in a straightforward fashion  over all entities (tokens and units) present at least  twice in the sequence-based corpus 2. The two next  methods filter the probabilities obtained with the Ez  method. In E2, all probabilities p(tls ) are set to 0  whenever s is a token (not a unit), thus forcing the  model to contain only associations between source  units and target entities (tokens or units). In E3  any parameter of the model that involves a token  is removed (that is, p(tls ) = 0 if t or s is a token).  The resulting model will thus contain only unit as-  sociations. In both cases, the final probabilities are  renormalized. Table 3 shows a few entries from a  unit model (Mu) obtained after 15 iterations of the  EM-algorithm on a sequence corpus resulting from  the application of the length-grouping criterion (dr)  over a lexicon of units whose likelihood score is above  5.0. The probabilities have been obtained by appli-  cation of the method E2.  We found many partially correct associations  Cover the years/au fils des, we have/nous, etc) that  illustrate the weakness of decoupling the unit iden-  tification from the mapping problem. In most cas-  2The entities een only once are mapped to a special \"un-  known\" word  es however, these associations have a lower proba-  bility than the good ones. We also found few er-  ratic associations (the first time/e'dtait, some hon.  members/t, etc) due to distributional rtifacts. It is  also interesting to note that the good associations  we found are not necessary compositional in nature  (we must/il Iaut, people of canada/les canadiens, of  eourse/6videmment, etc).  3.3 F i l ter ing   One way to increase the precision of the mapping  process is to impose some linguistic constraints on  the sequences such as simple noun-phrase contraints  (Ganssier, 1995; Kupiec, 1993; hua Chen and Chen,  94; Fung, 1995; Evans and Zhai, 1996). It is also  possible to focus on non-compositional compounds,  a key point in bilingual applications (Su et al., 1994;  Melamed, 1997; Lin, 99). Another interesting ap-  proach is to restrict sequences to those that do not  cross constituent boundary patterns (Wu, 1995; Fu-  ruse and Iida, 96). In this study, we filtered for po-  tential sequences that are likely to be noun phrases,  using simple regular expressions over the associated  part-of-speech tags. An excerpt of the association  probabilities of a unit model trained considering on-  ly the NP-sequences i given in table 3. Applying  this filter (referred to as JrNp in the following) to the  39,093 english sequences still surviving after previ-  ous filters ~'1 and ~'2 removes 35,939 of them (92%).  138  model spared ok good nu u  1 baseline - model 1 48.98 0 0 747 0  2 basel ine - model 2 51.83 0 0 747 0  3 E1 + ~'1(2, 2, 0, 0.2) 50.98 527 1702 5 626  4 E1+~'1(2,2,5,0.2)  51.61 596 2149 5 658  5 E1 + ~-~ (2, 2, 5, 0.2) + 9r2 51.72 633 2265 5 657  6 E2 + ~'~(2,2,0,0.2) 51.39 514 1551 43 578  7 2 + ~-~ (2, 2, 5, 0.2) 51.99 470 1889 46 614  8 E2 + ~'~(2,2,5,0.2) + ~'2 52.12 493 1951 46 606  9 E3 + ~-1(2, 2, 0, 0.2) 51.07 577 1699 43 588  10 E2 + ~-1(2, 2, 5, 0.2) 51.47 629 2124 46 618  11 E2+~'~(2 ,2 ,5 ,0 .2 )+~'2  51.68 665 2209 46 615  12 ~1 -}- .~1 (2, 2, 5, 0.2) -}- .~2 -}- ~:NP 52.83 416 1302 4 564  13 E2 + ~'1(2, 2, 5, 0.2) + ~NP 53.12 439 1031 228 425  14 2 + ~'~ (2, 2, 5, 0.2) + 5r2 + ~'NP 53.16 458 1052 199 439  15 ~3 -{- ~ : 0.4 -}- ~-1(2, 2, 5, 0.2) 4- .~NP 53.22 495 1031 228 425  Table 4: Completion results of several translation models, spared: theoretical proportion of characters  saved; ok: number of target units accepted by the user; good: number of target units that matched the  expected whether they were proposed or not; nu: number of sentences for which no target unit was found  by the translation model; u: number of sentences for which at least one helpful unit has been found by the  model, but not necessarily proposed.  More than half of the 3,154 remaining NP-sequences  contain only two words.  4 Resu l t s   We collected completion results on a test corpus  of 747 sentences (13,386 english tokens and 14,506  french ones) taken from the Hansard corpus. These  sentences have been selected randomly among sen-  tences that have not been used for the training.  Around 18% of the source and target words are not  known by the translation model.  The baseline models (line 1 and 2) are obtained  without any unit model (i.e. /~ = 1 in equation 2).  The first one is obtained with an IBM-like model 1  while the second is an IBM-like model 2. We observe  that for the pair of \u001b[1;32;40m language \u001b[0;0ms we considered, model  2 improves the amount of saved keystrokes of almost  3% compared to model 1. Therefore we made use of  alignment probabilities for the other models.  The three next blocks in table 4 show how the  parameter estimation method affects performance.  Training models under the C1 method gives the worst  results. This results from the fact that the word-  to-word probabilities trained on the sequence based  corpus (predicted by Mu in equation 2) are less ac-  curate than the ones learned from the token based  corpus. The reason is simply that there are less oc-  currences of each token, especially if many units are  identified by the grouping operator.  In methods C2 and C3, the unit model of equation  2 only makes predictions pu(tls ) when s is a source u-  nit, thus lowering the noise compared to method 1.  We also observe in these three blocks the influence  of sequence filtering: the more we filter, the better  the results. This holds true for all estimation meth-  ods tried. In the fifth block of table 4 we observe  the positive influence of the NP-filtering, especially  when using the third estimation method.  The best combination we found is reported in line  15. It outperforms the baseline by around 1.5%.  This model has been obtained by retaining all se-  quences een at least two times in the training cor-  pus for which the likelihood test value was above 5  and the entropy score above 0.2 (5rl (2, 2, 5, 0.2)). In  terms of the coverage of this unit model, it is in-  teresting to note that among the 747 sentences of  the test session, there were 228 for which the model  did not propose any units at all. For 425 of the re-  maining sentences, the model proposed at least one  helpful (good or partially good) unit. The active vo-  cabulary for these sentences contained an average of  around 2.5 good units per sentence, of which only  half (495) were proposed during the session. The  fact that this model outperforms others despite it-  s relatively poor coverage (compared to the others)  may be explained by the fact that it also removes  part of the noise introduced by decoupling the i-  dentification of the salient units from the training  procedure. Furthermore, as we mentionned earlier,  the more we filter, the less the grouping scheeme  presented in equation 4 remains necessary, thus re-  ducing a possible source of noise.  The fact that this model outperforms others, de-  spite its relatively poor coverage, is due to the fact  139  E ich le r  C )pt lons   l am p leased  to  t~ lce  ]par t  in  th i s  debate  tod  W .  Us ing  rod  W \"s techno log ies ,  i t  i s  poss ib le  fo r  a l l  C~m~dia~s  to   reg is ter  the i r  votes  on  i s s t les  of  pub l i c  spend ing  and  pub l i c   I )o r ro~v ing .   II me fa l t  p la le l r  de  prendre  la paro le  au Jourd 'hu i  dana  le cadre  de  e   d~bat .   Gr~ice  & la  techno log le  moderne ,  toue  lea  Canad len= peuvent  6e   prononcer  sur  le=;  quest ion= de  d6pen=e== et  d\" e rnprunta  de  I\" I~tat  .  Not re  p  Figure 1: Example of an i teraction i  TRANSTYPE with the source text in the top half of the screen. The  target text is typed in the bottom half with suggestions given by the menu at the insertion point.  that it also removes part of the noise that is intro-  duced by dissociating the identification ofthe salient  units from the training procedure. ~rthermore, as  we mentioned earlier, the more we filter, the less the  grouping scheme presented in equation 4 remains  necessary, thus further reducing an other possible  source of noise.  5 Conclusion  We have described a prototype system called  TRANSTYPE which embodies an innovative ap-  proach to interactive machine translation in which  the interaction is directly concerned with establish-  ing the target ext. We proposed and tested a mech-  anism to enhance TRANSTYPE by having it predic-  t sequences of words rather than just completions  for the current word. The results show a modest  improvement in prediction performance which will  serve as a baseline for our future investigations. One  obvious direction for future research is to revise our  current strategy of decoupling the selection of units  from their bilingual context.  Acknowlegments  TRANSTYPE is a project funded by the Natural Sci-  ences and Engineering Research Council of Canada.  We are undebted to Elliott Macklovitch and Pierre  Isabelle for the fruitful orientations they gave to this  work.  References  Adam L. Berger, Stephen A. Della Pietra, and Vin-  cent J. Della Pietra. 1996. A maximum entropy  approach to \u001b[1;32;40m natural \u001b[0;0m \u001b[1;32;40m language \u001b[0;0m processing. Compu-  tational Linguistics, 22(1):39-71.  Peter F. Brown, Stephen A. Della Pietra, Vincen-  t Della J. Pietra, and Robert L. Mercer. 1993.  The mathematics of machine trmaslation: Pa-  rameter estimation. Computational Linguistics,  19(2):263-312, June.  Ted Dunning. 93. Accurate methods for the statis-  tics of surprise and coincidence. Computational  Linguistics, 19(1):61-74.  David A. Evans and Chengxiang Zhai. 1996. Noun-  phrase analysis in unrestricted text for informa-  tion retrieval. In Proceedings of the 34th Annu-  al Meeting of the Association for Computational  Linguistics, pages 17-24, Santa Cruz, California.  George Foster, Pierre Isabelle, and Pierre Plamon-  don. 1997. Target-text Mediated Interactive Ma-  chine Translation. Machine Translation, 12:175-  194.  Pascale Fung. 1995. A pattern matching method for  finding noun and proper noun translations from  noisy parallel corpora. In Proceedings ofthe 33rd  Annual Meeting of the Association for Compu-  tational Linguistics, pages 236-243, Cambridge,  Massachusetts.  Osamu Furuse and Hitoshi Iida. 96. Incremen-  140  tal translation utilizing constituent boundray pat-  terns. In Proceedings of the 16th International  Conference On Computational Linguistics, pages  412-417, Copenhagen, Denmark.  Eric Gaussier. 1995. Modles statistiques et patron-  s morphosyntaxiques pour l'extraction de lcxiques  bilingues. Ph.D. thesis, Universit de Paris 7, jan-  vier.  Masahiko Haruno, Satoru Ikehara, and Takefumi  Yamazaki. 96. Learning bilingual collocations by  word-level sorting. In Proceedings of the 16th In-  ternational Conference On Computational Lin-  guistics, pages 525-530, Copenhagen, Denmark.  Kuang hua Chen and Hsin-Hsi Chen. 94. Extract-  ing noun phrases from large-scale texts: A hybrid  approach and its automatic evaluation. In Pro-  ceedings of the 32nd Annual Meeting of the Asso-  ciation for Computational Linguistics, pages 234-  241, Las Cruces, New Mexico.  Satoru Ikehara, Satoshi Shirai, and Hajine Uchino.  96. A statistical method for extracting uinterupt-  ed and interrupted collocations from very large  corpora. In Proceedings of the 16th International  Conference On Computational Linguistics, pages  574-579, Copenhagen, Denmark.  Julian Kupiec. 1993. An algorithm for finding noun  phrase correspondences in bilingual corpora. In  Proceedings of the 31st Annual Meeting of the  Association for Computational Linguistics, pages  17-22, Colombus, Ohio.  Dekang Lin. 99. Automatic identification of non-  compositional phrases. In Proceedings of the 37th  Annual Meeting of the Association for Computa-  tional Linguistics, pages 317-324, College Park,  Maryland.  I. Dan Melamed. 1997. Automatic discovery of non-  compositional coumpounds in parallel data. In  Proceedings of the 2nd Conference on Empirical  Methods in Natural Language Processing, pages  97-108, Providence, RI, August, lst-2nd.  Makoto Nagao and Shinsuke Mori. 94. A new  method of n-gram statistics for large number of  n and automatic extraction of words and phrases  from large text data of japanese. In Proceedings  of the 16th International Conference On Com-  putational Linguistics, volume 1, pages 611-615,  Copenhagen, Denmark.  Franz Josef Och and Hans Weber. 98. Improving  statistical \u001b[1;32;40m natural \u001b[0;0m anguage translation with cate-  gories and rules. In Proceedings of the 36th Annu-  al Meeting of the Association for Computational  Linguistics, pages 985-989, Montreal, Canada.  Graham Russell. 1998. Identification of salient to-  ken sequences. Internal report, RALI, University  of Montreal, Canada.  Sayori Shimohata, Toshiyuki Sugio, and Junji  Nagata. 1997. Retrieving collocations by co-  occurrences and word order constraints. In Pro-  ceedings of the 35th Annual Meeting of the Asso-  ciation for Computational Linguistics, pages 476-  481, Madrid Spain.  Keh-Yih Su, Ming-Wen Wu, and Jing-Shin Chang.  1994. A corpus-based approach to automatic om-  pound extraction. In Proceedings of the 32nd An-  nual Meeting of the Association for Computation-  al Linguistics, pages 242-247, Las Cruces, New  Mexico.  Ye-Yi Wang and Alex Waibel. 98. Modeling with  structures in statistical machine translation. In  Proceedings of the 36th Annual Meeting of the  Association for Computational Linguistics, vol-  ume 2, pages 1357-1363, Montreal, Canada.  Dekai Wu and Hongsing Wong. 98. Machine trans-  lation with a stochastic grammatical channel. In  Proceedings of the 36th Annual Meeting of the  Association for Computational Linguistics, pages  1408-1414, Montreal, Canada.  Dekai Wu. 1995. Stochastic inversion transduc-  tion grammars, with application to segmentation,  bracketing, and alignment of parallel corpora. In  Proceedings of the International Joint Conference  on Artificial Intelligence, volume 2, pages 1328-  1335, Montreal, Canada.  141  \n",
            "-----------------------------\n",
            "--- document 11: --- document 11: A Framework for MT and Multilingual NLG Systems Based on  Uniform Lexico-Structural Processing  Benoit Lavoie  CoGenTex, Inc.  840 Hanshaw Road  Ithaca, NY  USA, 14850  benoit@cogentex.com  Richard Kittredge  CoGenTex, Inc.  840 Hanshaw Road  Ithaca, NY  USA, 14850  richard @ cogentex.com  Tanya Korelsky  CoGenTex, Inc.  840 Hanshaw Road  Ithaca, NY  USA, 14850  tanya @ cogentex.com  Owen Rambow * ATT Labs-Research, B233  180 Park Ave, PO Box 971  Florham Park, NJ  USA, 07932  rambow @research.att.com  Abstract  In this paper we describe an implemented  framework for developing monolingual or  multilingual \u001b[1;32;40m natural \u001b[0;0m \u001b[1;32;40m language \u001b[0;0m generation  (NLG) applications and machine translation  (MT) applications. The framework  demonstrates a uniform approach to  generation and transfer based on declarative  lexico-structural transformations of  dependency structures of syntactic or  conceptual levels (\"uniform lexico-structural  processing\"). We describe how this  framework has been used in practical NLG  and MT applications, and report he lessons  learned.  1 Introduction  In this paper we present a linguistically  motivated framework for uniform lexico-  structural processing. It has been used for  transformations of conceptual and syntactic  structures during generation i monolingual nd  multilingual \u001b[1;32;40m natural \u001b[0;0m \u001b[1;32;40m language \u001b[0;0m generation (NLG)  and for transfer in machine translation (MT).  Our work extends directions taken in systems  such as Ariane (Vauquois and Boitet, 1985),  FoG (Kittredge and Polgu6re, 1991), JOYCE  (Rainbow and Korelsky, 1992), and LFS  (Iordanskaja et al., 1992). Although it adopts  the general principles found in the above-  mentioned systems, the approach presented in  this paper is more practical, and we believe,  would eventually integrate better with emerging  statistics-based approaches toMT.  * The work performed on the framework by this co-  author was done while at CoGenTex, Inc.  The framework consists of a portable Java  environment for building NLG or MT  applications by defining modules using a core  tree transduction engine and single declarative  ASCII specification \u001b[1;32;40m language \u001b[0;0m for conceptual or  syntactic dependency tree structures 1 and their  transformations. Developers can define new  modules, add or remove modules, or modify  their connections. Because the processing of the  transformation engine is restricted to  transduction of trees, it is computationally  efficient.  Having declarative rules facilitates their reuse  when migrating from one programming  environment toanother; if the rules are based on  functions pecific to a programming \u001b[1;32;40m language \u001b[0;0m,  the implementation f these functions might no  longer be available in a different environment.  In addition, having all lexical information and  all rules represented eclaratively makes it  relatively easy to integrate into the framework  techniques for generating some of the rules  automatically, for example using corpus-based  methods. The declarative form of  transformations makes it easier to process them,  compare them, and cluster them to achieve  proper classification and ordering.  1 In this paper, we use the term syntactic dependency  (tree) structure as defined in the Meaning-Text  Theory (MTT; Mel'cuk, 1988). However, we  extrapolate from this theory when we use the term  conceptual dependency (tree) structure, which has no  equivalent in MTT (and is unrelated to Shank's CD  structures proposed inthe 1970s).  60  Thus, the framework represents a generalized  processing environment that can be reused in  different ypes of \u001b[1;32;40m natural \u001b[0;0m \u001b[1;32;40m language \u001b[0;0m processing  (NLP) applications. So far the framework has  been used successfully to build a wide variety of  NLG and MT applications in several limited  domains (meteorology, battlefield messages,  object modeling) and for different \u001b[1;32;40m language \u001b[0;0ms  (English, French, Arabic, and Korean).  In the next sections, we present the design of the  core tree transduction module (Section 2),  describe the representations that it uses (Section  3) and the linguistic resources (Section 4). We  then discuss the processing performed by the  tree transduction module (Section 5) and its  instantiation for different applications (Section  6). Finally, we discuss lessons learned from  developing and using the framework (Section 7)  and describe the history of the framework  comparing it to other systems (Section 8).  2 The Framework's Tree Transduction Module  The core processing engine of the framework is  a generic tree transduction module for lexico-  structural processing, shown in Figure 1. The  module has dependency stuctures as input and  output, expressed in the same tree formalism,  although not necessarily at the same level (see  Section 3). This design facilitates the pipelining  of modules for stratificational transformation. I   fact, in an application, there are usually several  instantiations of this module.  The transduction module consists of three  processing steps: lexico-structural pre-  processing, main lexico-structural processing,  and lexico-structural post-processing. Each of  these steps is driven by a separate grammar, and  all three steps draw on a common feature data  base and lexicon. The grammars, the lexicon  and the feature data base are referred to as the  linguistic resources (even if they sometimes  apply to a conceptual representation). All  linguistic resources are represented in a  declarative manner. An instantiation of the tree  transduction module consists of a specification  of the linguistic resources.  Input Dependency Structure  ~ L exlco-Structural Preproce~ing  Intermediate Dependency StructttreL_~  Lexico-Structm'al Processing  Intermediate + Dependency Structure  ~ Lexico-Structural  Postprocessing  Output / /~   Dependency SUucturc  i  Figure 1: Design of the Tree Transduction Module  3 The Framework's Representations  The representations used by all instantiations of the tree transduction module in the framework  are dependency tree structures. The main  characteristics of all the dependency tree  structures are:   A dependency tree is unordered (in contrast  with phrase structure trees, there is no  ordering between the branches of the tree).   All the nodes in the tree correspond to  lexemes (i.e., lexical heads) or concepts  depending on the level of representation. I   contrast with a phrase structure  representation, there are no phrase-structure  nodes labeled with nonterminal symbols.  Labelled arcs indicate the dependency  relationships between the lexemes.  The first of these characteristics makes a  dependency tree structure a very useful  representation for MT and multilingual NLG,  since it gives linguists a representation that  allows them to abstract over numerous cross-  linguistic divergences due to \u001b[1;32;40m language \u001b[0;0m specific  ordering (Polgu~re, 1991).  We have implemented 4 different types of  dependency tree structures that can be used for  NLG, MT or both:   Deep-syntactic structures (DSyntSs);   Surface syntactic structures (SSyntSs);  61   Conceptual structures (ConcSs);   Parsed syntactic structures (PSyntSs).  The DSyntSs and SSyntSs correspond closely to  the equivalent structures of the Meaning-Text  Theory (MTT; Mel'cuk, 1988): both structures  are unordered syntactic representations, but a  DSyntS only includes full meaning-bearing  lexemes while a SSyntS also contains function  words such as determiners, auxiliaries, and  strongly governed prepositions. In the  implemented applications, the DSyntSs are the  pivotal representations involved in most  transformations, as this is also often the case in  practice in linguistic-based MT (Hutchins and  Somers, 1997). Figure 2 illustrates a DSyntS  from a meteorological application, MeteoCogent  (Kittredge and Lavoie, 1998), represented using  the standard graphical notation and also the  RealPro ASCII notation used internally in the  framework (Lavoie and Rambow, 1997). As  Figure 2 illustrates, there is a straightforward  mapping between the graphical notation and the  ASCII notation supported in the framework.  This also applies for all the transformation rules  in the framework which illustrates the  declarative nature of our approach,  I 1  LOW  -5 TO  't  LOw  (  A'I~R -5  ATTR TO  (  il HIGH  (  A'\\]I~R 20  )  )  )  Low -S to high 20  Figure 2: DSyntS (Graphical nd ASCII Notation)  The ConcSs correspond to the standard frame-  like structures used in knowledge representation,  with labeled arcs corresponding to slots. We  have used them only for a very limited  meteorological domain (in MeteoCogent), and  we imagine that they will typically be defined in  a domain-specific manner.  Figure 3 illustrates the mapping between an  interlingua defined as a ConcS and a  corresponding English DSyntS. This example,  also taken from MeteoCogent, illustrates that the  conceptual interlingua in NLG can be closer to a  database representation f domain data than to  its linguistic representations.  As mentioned in (Polgu~re, 1991), the high level  of abstraction of the ConcSs makes them a  suitable interlingua for multilingual NLG since  they bridge the semantic discrepancies between  \u001b[1;32;40m language \u001b[0;0ms, and they can be produced easily from  the domain data. However, most off-the-shelf  parsers available for MT produce only syntactic  structures, thus the DSyntS level is often more  suitable for transfer.  Cones   #TEMPERATURE  Low -5 to Mlgh 20  DS~tS   LOW  -5 TO  ItlGH  Figure 3: ConcS Interlingua nd English DSyntS  Finally, the PSyntSs correspond to the parser  outputs represented using RealPro's dependency  structure formalism. The PSyntSs may not be  valid directly for realization or transfer since  they may contain unsupported features or  dependency relations. However, the PSyntSs  are represented in a way to allow the framework  to convert hem into valid DSyntS via lexico-  structural processing. This conversion is done  via conversion grammars customized for each  parser. There is a practical need to convert one  syntactic formalism to another and so far we  have implemented converters for three off-the-  shelf parsers (Palmer et al., 1998).  4 The Framework's Linguistic Resources  As mentioned previously, the framework is  composed of instantiations of the tree  62  transduction module shown in Figure 1. Each  module has the following resources:   Feature Data-Base: This consists of the  feature system defining available features  and their possible values in the module.   Lexicon: This consists of the available  lexemes or concepts, depending on whether  the module works at syntactic or conceptual  level. Each lexeme and concept is defined  with its features, and may contain specific  lexico-structural ules: transfer rules for MT,  mapping rules to the next level of  representation for surface realization of  DSyntS or lexicalization of ConcS.   Main Grammar: This consists of the lexico-  structural mapping rules that apply at this  level and which are not lexeme- or concept-  specific (e.g. DSynt-rules for the DSynt-  module, Transfer-rules for the Transfer  module, etc.)   Preprocessing rammar: This consists of  the lexico-structural mapping rules for  transforming the input structures in order to  make them compliant with the main  grammar, if this is necessary. Such rules are  used to integrate new modules together  when discrepancies in the formalism need to  be fixed. This grammar can also be used  for adding default features (e.g. setting the  default number of nouns to singular) or for  applying default transformations (e.g.  replacing non meaning-bearing lexemes  with features).  Postprocessing rammar: This consists of  lexico-structural mapping rules for  transforming the output structures before  they can be processed by the next module.  As for the preprocessing rules, these rules  can be used to fix some discrepancies  between modules.  Our representation f the lexicon at the lexical  level (as opposed to conceptual) is similar to the  one found in RealPro. Figure 4 shows a  specification for the lexeme SELL. This lexeme  is defined as a verb of regular morphology with  two lexical-structural mappings, the first one  introducing the preposition TO for its 3 r actant,  and the preposition FOR for its 4 th actant: (a  seller) X1 sells (merchandise) X2 to (a buyer)  X3 for (a price) X4. What is important is that  each mapping specifies a transformation  between structures at different levels of  representation but that are represented in one  and the same representation formalism (DSyntS  and SSyntS in this case). As we will see  below, grammar ules are also expressed in a  similar way.  LEX~ME: SELL  CATEGORY:  verb   FEATURES:  \\[ \\]  GOV-PATTERN: \\ [   DSYNT-RULE:   SELL ( I I I  $X3 )  <- ->  SELL  ( complet ive2  TO  ( p repos i t iona l  $X3 ) )  DSYNT-RULE :  SELL ( IV $X4 )  <- ->  SELL  ( complet ive3  FOR  ( p repos i t iona l  $X4 )  \\]  MORPHOLOGY:  \\[  ( \\[ tense :past  \\] so ld  \\[ inv  ( \\[ mood:past -par t  \\] so ld  \\[ inv  ( \\[ \\] sel l  \\[ reg  \\]  Figure 4: Specification ofLexeme SELL  At the conceptual level, the conceptual lexicon  associates lexical-structural mapping with  concepts in a similar way. Figure 5 illustrates  the mapping at the deep-syntactic level  associated with the concept #TEMPERATURE.  Except for the slight differences in the labelling,  this type of specification is similar to the one  used on the lexical level. The first mapping rule  corresponds to one of the lexico-structural  transformations u ed to convert he interlingual  ConcS of Figure 3 to the corresponding DSyntS.  ZONCEPT:  #TEMPERATURE  5EXICAL:  \\[  L~-RULE:   #TEMPERATURE ( #min imum SX  #maxim~ $Y  <- ->  LOW ( ATTR $X  ATTR TO  ( II H IGH  ( ATTR SY ) ) )  LEX-RULE:   #TEMPERATURE ( #min im~ SX  <- ->  LOW ( ATTR $X )  LEX-RULE:   #TEMPE~TURE ( #max imum $X  <- ->  H IGH ( ATTR SX )  \\]  Figure 5: Specification ofConcept #TEMPERATURE  63  Note that since each lexicon entry can have  more than one lexical-structural mapping rule,  the list of these rules represents a small grammar  specific to this lexeme or concept.  Realization grammar ules of the main grammar  include generic mapping rules (which are not  lexeme-specific) such as the DSyntS-rule  illustrated in Figure 6, for inserting a determiner.  DSYNT-RULE:   $X  \\[ c lass :noun ar t i c le :de f  \\]  $X  ( determinat ive  THE )  Figure 6: Deep-Syntactic Rule for Determiner Insertion  The lexicon formalism has also been extended to  implement lexeme-specific lexico-structural  transfer rules. Figure 7 shows the lexico-  structural transfer of the English verb lexeme  MOVE to French implemented for a military  and weather domain (Nasr et al., 1998):  Cloud will move into the western regions.  Des nuages envahiront les rdgions ouest.  They moved the assets forward.  -.9 lls ont amen~ les ressources vers l 'avant.  The 79 dcg moves forward.  ---~ La 79 dcg avance  vers l'avant.  A disturbance will move north of Lake Superior.  --~ Une perturbation se diplacera au nord du lac  supdrieur.  LEXEME : MO~'E  CATEGORY : verb  FEATORES : \\[ \\]  TRANSFER: \\[  TRANSFER-RULE:  MOVE  I ATTR INTO \\ [ c lass :prepos i t ion \\ ]   ( II SXl ) )  .-.>  E2~VAH IR \\[class:verb\\]  ( II SX1 )  TRANSFER-RULE :  MOVE  ( II $X2 )  AMENER \\[class:verb\\]  \\[ II $X2 )  TRANSFER-RULE:  MOVE  ( ATTR SX \\[Iexe~e:FORWARD class:adverb\\] )  AVANCER  ( ATTR SX )  TRANSFER-RULE :  MOVE  <-->  DEPLACER \\[class:verb refl:\\]  \\]  Figure 7: Lexico-Structural Transfer of English Lexerne  MOVE to French  More general exico-structural rules for transfer  can also be implemented using our grammar rule  formalism. Figure 8 gives an English-French  transfer ule applied to a weather domain for the  transfer of a verb modified by the adverb  ALMOST:  It almost rained.  --o II a fail l i  pleuvoir.  TRANSFER-RULE:   SX  \\[ c lass :verb  \\]  ( ATTR ALMOST )  <- ->  FA ILL IR  \\[ c lass :verb  \\]  ( I I  SX  \\[ mood: in f  \\] )  Figure 8: English to French Lexico-Structural  Transfer Rule with Verb Modifier ALMOST  More details on how the structural divergences  described in (Dorr, 1994) can be accounted for  using our formalism can be found in (Nasr et  al., 1998).  5 The Rule Processing  Before being processed, the rules are first  compiled and indexed for optimisation. Each  module applies the following processing.  The rules are assumed to be ordered from most  specific to least specific. The application of the  rules to the structures i  top-down in a recursive  way from the f'n-st rule to the last. For the main  grammar, before applying a grammar ule to a  given node, dictionary lookup is carried out in  order to first apply the lexeme- or concept-  specific rules associated with this node. These  are also assumed to be ordered from the most  specific to the least specific.  If a lexico-structural transformation involves  switching a governor node with one of its  dependents in the tree, the process is reapplied  with the new node governor. When no more  rules can be applied, the same process is applied  to each dependent of the current governor.  When all nodes have been processed, the  processing is completed,  6 Using the Framework to build Applications  Figure 9 shows how different instantiations of  the tree transduction module can be combined to  64  build NLP applications. The diagram does not  represent a particular system, but rather shows  the kind of transformations that have been  implemented using the framework, and how they  interact. Each arrow represents one type of  processing implemented by an instantiation of  the tree transduction module. Each triangle  represents a different level of representation.  Scope of the  Framework  ~Conversion bl  Parsed  PSyntS LI  Parsing  Sentence  PI \"ng  C'nezoa~ 1  ~ e Transfer ~_~ , Co.verMon  D$ ntS LI  ~SyntS ~ealizalion  / \\   SSyntS LI SSyntS 1.2 ~ yntS ealization  A DSyntS L2 Parsed DSym51 PSyntS L2  Realiza~o~  SSym~ Realizatio parsin  Input Generated Generated Input  Sentence LI Sentence LI Sentence 1.2 Sentence L2.  I concS Concepmd suar.tm~ SSyntS Suffaee:Syntnetlc su'uet~'e  os~ts t~sy~ac~ Psy~s ~d:~n~c  Figure 9: Scope of the Framework's Transformations  For example, in Figure 9, starting with the  \"Input Sentence LI\" and passing through  Parsing, Conversion, Transfer, DSyntS  Realization and SSyntS Realization to  \"Generated Sentence L2\" we obtain an Ll-to-L2  MT system. Starting with \"Sentence Planning\"  and passing through DSyntS Realization, and  SSyntS Realization (including linearization and  inflection) to \"Generated Sentence LI\", we  obtain a monolingual NLG system for L1.  So far the framework has been used successfully  for building a wide variety of applications in  different domains and for different \u001b[1;32;40m language \u001b[0;0ms:  NLG:   Realization of English DSyntSs via SSyntS  level for the domains of meteorology  (MeteoCogent; Kittredge and Lavoie, 1998)  and object modeling (ModelExplainer;  Lavoie et al., 1997).   Generation of English text from conceptual  interlingua for the meteorology domain  (MeteoCogent). (The design of the  interlingua can also support he generation  of French but this functionality has not yet  been implemented.)  MT:   Transfer on the DSyntS level and realization  via SSyntS level for English--French,  English--Arabic, English---Korean and  Korean--English. Translation in the  meteorology and battlefield omains (Nasr  et al., 1998).   Conversion of the output structures from  off-the-shelf English, French and Korean  parsers to DSyntS level before their  processing by the other components in the  framework (Palmer et al., 1998).  7 Lessons Learned Using the Framework  Empirical results obtained from the applications  listed in Section 6 have shown that the approach  used in the framework is flexible enough and  easily portable to new domains, new \u001b[1;32;40m language \u001b[0;0ms,  and new applications. Moreover, the time spent  for development was relatively short compared  to that formerly required in developing similar  types of applications. Finally, as intended, the  limited computational power of the transduction  module, as well as careful implementation,  including the compilation of declarative  linguistic knowledge to Java, have ensured  efficient run-time behavior. For example, in the  MT domain we did not originally plan for a  separate conversion step from the parser output  to DSyntS. However, it quickly became apparent  that there was a considerable gap between the  output of the parsers we were using and the  DSyntS representation that was required, and  furthermore, that we could use the tree  transduction module to quickly bridge this gap.  Nevertheless, our tree transduction-based  approach has some important limitations. In  particular, the framework requires the developer  of the transformation rules to maintain them and  specify the order in which the rules must be  applied. For a small or a stable grammar, this  does not pose a problem. However, for large or  rapidly changing grammar (such as a transfer  grammar in MT that may need to be adjusted  when switching from one parser to another), the  65  burden of the developer's task may be quite  heavy. In practice, a considerable amount of  time can be spent in testing a grammar after its  revision.  Another major problem is related to the  maintenance of both the grammar and the  lexicon. On several occasions during the  development of these resources, the developer in  charge of adding lexical and grammatical data  must make some decisions that are domain  specific. For example, in MT, writing transfer  rules for terms that can have several meanings or  uses, they may simplify the problem by  choosing a solution based on the context found  in the current corpus, which is a perfectly \u001b[1;32;40m natural \u001b[0;0m  strategy. However, later, when porting the  transfer esources to other domains, the chosen  strategy may need to be revised because the  context has changed, and other meanings or uses  are found in the new corpora. Because the  current approach is based on handcrafted rules,  maintenance problems of this sort cannot be  avoided when porting the resources to new  domains.  An approach such as the one described in (Nasr  et al., 1998; and Palmer and al., 1998) seems to  be solving a part of the problem when it uses  corpus analysis techniques for automatically  creating a first draft of the lexical transfer  dictionary using statistical methods. However,  the remaining work is still based on handcrafting  because the developer must refine the rules  manually. The current framework offers no  support for merging handcrafted rules with new  lexical rules obtained statistically while  preserving the valid handcrafted changes and  deleting the invalid ones. In general, a better  integration of linguistically based and statistical  methods during all the development phases is  greatly needed.  8 History of the Framework and Comparison  with Other Systems  The framework represents a generalization of  several predecessor NLG systems based on  Meaning-Text Theory: FoG (Kittredge and  Polgu~re, 1991), LFS (Iordanskaja et al., 1992),  and JOYCE (Rambow and Korelsky, 1992).  The framework was originally developed for the  realization of deep-syntactic structures in NLG  (Lavoie and Rambow, 1997). It was later  extended for generation of deep-syntactic  structures from conceptual interlingua (Kittredge  and Lavoie, 1998). Finally, it was applied to  MT for transfer between deep-syntactic  structures of different \u001b[1;32;40m language \u001b[0;0ms (Palmer et al.,  1998). The current framework encompasses the  full spectrum of such transformations, i.e. from  the processing of conceptual structures to the  processing of deep-syntactic structures, either  for NLG or MT.  Compared to its predecessors (Fog, LFS,  JOYCE), our approach as obvious advantages  in uniformity, declarativity and portability. The  framework has been used in a wider variety of  domains, for more \u001b[1;32;40m language \u001b[0;0ms, and for more  applications (NLG as well as MT). The  framework uses the same engine for all the  transformations at all levels because all the  syntactic and conceptual structures are  represented asdependency tree structures.  In contrast, the predecessor systems were not  designed to be rapidly portable. These systems  used programming \u001b[1;32;40m language \u001b[0;0ms or scripts for the  implementation f the transformation rules, and  used different ypes of processing at different  levels of representation. For instance, in LFS  conceptual structures were represented as  graphs, whereas syntactic structures were  represented as trees which required different  types of processing at these two levels.  Our approach also has some disadvantages  compared with the systems mentioned above.  Our lexico-structural transformations are far  less powerful than those expressible using an  arbitrary programming \u001b[1;32;40m language \u001b[0;0m. In practice,  the formalism that we are using for expressing  the transformations is inadequate for long-range  phenomena (inter-sentential or intra-sentential),  including syntactic phenomena such as long-  distance wh-movement and discourse  phenomena such as anaphora nd ellipsis. The  formalism could be extended to handle intra-  sentential syntactic effects, but inter-sentential  discourse phenomena probably require  procedural rules in order to access lexemes in  66 other sentences. In fact, LFS and JOYCE  include a specific module for elliptical structure  processing.  Similarly, the limited power of the tree  transformation rule formalism distinguishes the  framework from other NLP frameworks based  on more general processing paradigms uch as  unification of FUF/SURGE in the generation  domain (Elhadad and Robin, 1992).  9 Status  The framework is currently being improved in  order to use XML-based specifications for  representing the dependency structures and the  transformation rules in order to offer a more  standard development environment and to  facilitate the framework extension and  maintenance.  Acknowledgements  A first implementation of the framework (C++  processor and ASCII formalism for expressing  the lexico-structural transformation rules)  applied to NLG was developed under SBIR  F30602-92-C-0015 awarded by USAF Rome  Laboratory. The extensions to MT were  developed under SBIR DAAL01-97-C-0016  awarded by the Army Research Laboratory. The  Java implementation and general improvements  of the framework were developed under SBIR  DAAD17-99-C-0008 awarded by the Army  Research Laboratory. We are thankful to Ted  Caldwell, Daryl McCullough, Alexis Nasr and  Mike White for their comments and criticism on  the work reported in this paper.  References  Dorr, B. J. (1994) Machine translation divergences:  A formal description and proposed solution. In  Computational Linguistics, vol. 20, no. 4, pp. 597-  635.  Elhadad, M. and Robin, J. (1992) Controlling  Content Realization with Functional Unification  Grammars. In Aspects of Automated Natural  Language Generation, Dale, R., Hovy, E., Rosner,  D. and Stock, O. Eds., Springer Verlag, pp. 89-  104.  Hutchins, W. J. and Somers, H. L. (1997) An  Introduction to Machine Translation. Academic  Press, second edition.  Iordanskaja, L., Kim, M., Kittredge, R., Lavoie, B.  and Polgu6re, A. (1992) Generation of Extended  Bilingual Statistical Reports. In Proceedings of the  15th International Conference on Computational  Linguistics, Nantes, France, pp. 1019-1023.  Kittredge, R. and Lavoie, B. (1998) MeteoCogent: A Knowledge-Based Tool For Generating Weather  Forecast Texts. In Proceedings of the American  Meteorological Society AI Conference (AMS-98),  Phoenix, Arizona, pp. 80--83.  Kittredge, R. and Polgu~re, A. (1991) Dependency  Grammars for Bilingual Text Generation: Inside  FoG's Stratificational Models. In Proceedings of  the International Conference on Current Issues in  Computational Linguistics, Penang, Malaysia, pp.  318-330.  Lavoie, B. (1995) Interlingua for Bilingual Statistical  Reports. In Notes of IJCAI-95 Workshop on  Multilingual Text Generation, Montr6al, Canada,  pp. 84---94.  Lavoie, B. and Rambow, O. (1997) A Fast and  Portable Realizer for Text Generation Systems. In  Proceedings of the Fifth Conference on Applied  Natural Language Processing, Washington, DC.,  pp. 265-268.  Lavoie, B., Rambow, O. and Reiter, E. (1997)  Customizable Descriptions of Object-Oriented  Models. In Proceedings of the Fifth Conference on  Applied Natural Language Processing,  Washington, DC., pp. 253-256.  Mel'cuk, I. (1988) Dependency Syntax. State  University of New York Press, Albany, NY.  Nasr, A., Rambow, O., Palmer, M. and Rosenzweig,  J. (1998) Enriching lexical transfer with cross-  linguistic semantic features. In Proceedings of the  Interlingua Workshop at the MT Summit, San  Diego, California.  Palmer, M., Rambow, O. and Nasr, A. (1998) Rapid  Prototyping of Domain-Specific Machine  Translation Systems. In Proceedings of the Third  Conference on Machine Translation in the  Americas (AMTA-98), PA, USA, pp. 95-102.  Polgu6re, A. (1991) Everything has not been said  about interlinguae: the case of multi-lingual text  generation system. In Proc. of Natural Language  Processing Pacific Rim Symposium, Singapore.  Rambow, O. and Korelsky, T. (1992) Applied Text  Generation. In Proceedings of the 6th International  Workshop on Natural Language Generation,  Trento, Italy, pp. 40--47.  Vauquois, B. and Boitet C. (1985) Automated  translation at Grenoble University. In  Computational Linguistics, Vol. 11, pp. 28-36.  67  \n",
            "-----------------------------\n",
            "--- document 12: --- document 12: A Representation for Complex and Evolving Data Dependencies  in Generation  C Me l l i sh  $, R Evans  t, L Cah i l l  t, C Doran  t, D Pa iva  t, M Reape $, D Scot t  t, N T ipper  t  t Information Technology Research Institute, University of Brighton, Lewes Rd, Brighton, UK  SDivision of Informatics, University of Edinburgh, 80 South Bridge, Edinburgh, UK  rags@itri, brighton, ac. uk  http :/www. itri. brighton, ac. uk/proj ect s/rags  Abst rac t   This paper introduces an approach to represent-  ing the kinds of information that components  in a \u001b[1;32;40m natural \u001b[0;0m \u001b[1;32;40m language \u001b[0;0m generation (NLG) sys-  tem will need to communicate to one another.  This information may be partial, may involve  more than one level of analysis and may need  to include information about the history of a  derivation. We present a general representation  scheme capable of handling these cases. In ad-  dition, we make a proposal for organising inter-  module communication i an NLG system by  having a central server for this information. We  have validated the approach by a reanalysis of  an existing NLG system and through a full im-  plementation of a runnable specification.  1 In t roduct ion   One of the distinctive properties of \u001b[1;32;40m natural \u001b[0;0m an-  guage generation when compared with other  \u001b[1;32;40m language \u001b[0;0m ngineering applications i that it has  to take seriously the full range of linguistic rep-  resentation, from concepts to morphology, or  even phonetics. Any processing system is only  as sophisticated as its input allows, so while a  \u001b[1;32;40m natural \u001b[0;0m \u001b[1;32;40m language \u001b[0;0m understanding system might  be judged primarily by its syntactic prowess,  even if its attention to semantics, pragmatics  and underlying conceptual analysis is minimal,  a generation system is only as good as its deep-  est linguistic representations. Moreover, any at-  tempt to abstract away from individual gener-  ation systems to a more generic architectural  specification faces an even greater challenge:  not only are complex linguistic representations  required, able to support the dynamic evolu-  tionary development of data during the gener-  * Now at the MITRE Corporation, Bedford, MA, USA,  cdoran@mitre, org.  ation process, but they must do so in a generic  and flexible fashion.  This paper describes a representation devel-  oped to meet these requirements. It offers a  formally well-defined eclarative representation  \u001b[1;32;40m language \u001b[0;0m, which provides a framework for ex-  pressing the complex and dynamic data require-  ments of NLG systems. The approach supports  different levels of representation, mixed repre-  sentations that cut across levels, partial and  shared structures and 'canned' representations,  as well as dynamic relationships between data  at different stages in processing. We are using  the approach to develop a high level data model  for NLG systems as part of a generic generation  architecture called RAGS 1.  The framework has been implemented in the  form of a database server for modular genera-  tion systems. As proof of concept of the frame-  work, we have reimplemented an existing NLG  system. The system we chose was the Caption  Generation System (CGS) (Mittal et al., 1995;  Mittal et al., 1998). The reimplementation in-  volved defining the interfaces to the modules of  CGS in terms of the RAGS representations and  then implementing modules that had the requi-  site input and output representations.  Generation systems, especially end-to-end,  applied generation systems, have, unsurpris-  ingly, many things in common. Reiter (1994)  proposed an analysis of such systems in terms  of a simple three stage pipeline. More recently,  the RAGS project attempted to repeat he anal-  1This work is supported by ESPRC grants  GR/L77041 (Edinburgh) and GR/L77102 (Brighton),  RAGS: Reference Architecture for Generation Systems.  We would also like to acknowledge the contribution of  Jo Calder to the ideas and formalisation described in  this paper. In particular, parts of this paper are based  on (Calder et al., 1999).  119  ysis (Cahill et al., 1999a), but found that while  most systems did implement a pipeline, they  did not implement the same pipeline - different  functionalities occurred in different places and  different orders in different systems. In order  to accommodate his result, we sought to de-  velop an architecture that is more general than  a simple pipeline, and thus supports the range  of pipelines observed, as well as other more com-  plex control regimes (see (Cahill et al., 1999a;  Cahill et al., 1999b)). In this paper, we argue  that supporting such an architecture requires  careful consideration of the way data represen-  tations interact and develop. Any formal frame-  work for expressing the architecture must take  account of this.  2 The  representat iona l  requ i rements   o f  generat ion  sys tems  We noted in the introduction that generation  systems have to deal with a range of linguis-  tic information. It is \u001b[1;32;40m natural \u001b[0;0m, especially in the  context of a generic architecture proposal, to  model this breadth in terms of discrete layers  of representation: (1999a) introduce layers such  as conceptual, semantic, rhetorical, syntactic  and document structure, but the precise demar-  cation is not as important here as the princi-  ple. The different kinds of information are typi-  cally represented differently, and built up sepa-  rately. However the layers are far from indepen-  dent: objects at one layer are directly related to  those at others, forming chains of dependency  from conceptual through rhetorical and seman-  tic structure to final syntactic and document re-  alisation. This means that data resources, such  as grammars and lexicons, and processing mod-  ules in the system, are often defined in terms of  mixed  data: structures that include informa-  tion in more than one representation layer. So  the ability to represent such mixed structures  in a single formal framework is an important  property of a generic data proposal.  In addition, it is largely standard in gener-  ation as elsewhere in \u001b[1;32;40m language \u001b[0;0m applications, to  make extensive use of par t ia l  representations,  often using a type system to capture grades of  underspecification. An immediate corollary of  providing support for partial structures is the  notion that they may become further specified  over time, that data structures evolve. If the  framework seeks to avoid over-commitment to  particular processing strategies it needs to pro-  vide a way of representing such evolution ex-  plicitly if required, rather than relying on de-  structive modification of a structure. Related  to this, it should provide explicit support for  representing a l te rnat ive  specifications at any  point. Finally, to fully support efficient pro-  cessing across the range of applications, from  the simple to the most complex, the represen-  tation must allow for compact sharing of infor-  mation in tang led  structures (two structures  which share components).  In addition to these direct requirements of the  generation task itself, additional requirements  arise from more general methodological consid-  erations: we desire a representation that is for-  mally well  def ined,  allows for theoretical rea-   son ing about the data and performance of sys-  tems, and supports control regimes from simple  deterministic pipelines to complex parallel ar-  chitectures.  3 The  Representat ion  Scheme  In this section, we present our proposal for a  general representation scheme capable of cover-  ing the above requirements. Our formulation is  layered: the foundation is a simple, flexible, rig-  orously defined graph representation formalism,  on top of which we introduce notions of com-  plex types and larger data structures and rela-  tionships between them. This much is sufficient  to capture the requirements just discussed. We  suppose a yet higher level of specification could  capture a more constraining data model but  make no specific proposals about this here, how-  ever the following sections use examples that do  conform to such a higher level data model.  The lowest level of the representation scheme  is:   re lat iona l :  the basic data entity is x -~ y,  an ar row representing a relation from ob-  ject x to object y;   typed:  objects and arrows have an asso-  ciated type system, so it is possible to de-  fine classes and subclasses of objects and  arrows.  At the most fundamental level, this is more or  less the whole definition. There is no commit-  ment to what object or arrow types there are or  120  how they relate to each other. So a representa-  tion allowed by the scheme consists of:   a set of objects, organised into types;   a set of binary relations, organised into  types;   a set of arrows, each indicating that a rela-  tion holds between one object and another  object.  Sets,  sequences  and  funct ions   For the next level, we introduce more struc-  ture in the type system to support sets, se-  quences and functions. Objects are always  atomic (though they can be of type set, se-  quence or function) - it is not possible to make  an object which actually is a set of two other  objects (as you might with data structures in a  computer program). To create a set, we intro-  duce a set type for the object, and a set mem-  bership arrow type (el), that links the set's el-  ements to the set. Similarly, for a sequence, we  introduce a sequence type and sequence mem-  ber arrow types (1-el, 2-el, 3-el, . . .  ), and for a  function, we have a complex type which spec-  ifies the types of the arrows that make up the  domain and the range of the function.  SemRep  ~ fun(Role.SemRep)  7 V show SemRep SemRep  Figure 1: The partial semantic representation  of \"The second chart shows the number of days  on the market\"  As an example, consider Figure 1, which  shows a semantic representation (SemRep) from  the CGS reimplementation. Here, the tree  nodes correspond to objects, each labelled with  its type. The root node is of type SemRep, and  although it is not an explicit sequence type, we  can see that it is a triple, as it has three sequence  member arrows (with types 1-el, 2-el and 3-el).  Its first arrow's target is an object of type DR  (Discourse Referent). Its second represents a set  of SemPred (Semantic Predicate) objects, and in  this case there's just one, of type show. Its third  element is a (partial) function, from Role arrow  types (agent and affected are both subtypes of  Role) to SemReps. (In this case, the SemReps  have not yet been fully specified.)  Local  and  non- loca l  a r rows   The second extension to the basic representa-  tion scheme is to distinguish two different ab-  stract kinds of arrows - local and non-local.  Fundamentally we are representing just a homo-  geneous network of objects and relationships. In  the example above we saw a network of arrows  that we might want to view as a single data  structure, and other major data types might  similarly appear as networks. Additionally, we  want to be able to express relationships between  these larger 'structures' - between structures  of the same type (alternative solutions, or re-  vised versions) or of different ypes (semantic  and syntactic for example). To capture these  distinctions among arrows, we classify our ar-  row types as local or non-local (we could do  this in the type system itself, or leave it as an  informal distinction). Local arrows are used to  build up networks that we think of as single  data structures. Non-local arrows express rela-  tionships between such data structures.  All the arrow types we saw above were local.  Examples of non-local arrows might include:  real ises These arro~vs link something more ab-  stract to something less abstract hat re-  alises it. Chains of realises arrows might  lead from the original conceptual input to  the generator through rhetorical, seman-  tic and syntactic structures to the actual  words that express the input.  revises These arrows link a structure to an-  other one of the same type, which is con-  sidered to be a 'better' solution - perhaps  because it is more instantiated. It is impor-  tant to note that parts of larger structures  can be revised without revising the entire  structure.  coreference These arrows link structures  which are somehow \"parallel\" and which  perhaps hare some substructure, i.e., tan-  gled structures. For instance, document  representations may be linked to rhetorical  representations, either as whole isomorphic  structures or at the level of individual con-  stituents.  121  Notice that the representation scheme does  not enforce any kind of well-formedness with  respect o local and non-local arrows. In fact,  although it is \u001b[1;32;40m natural \u001b[0;0m to think of a 'structure' as  being a maximal network of local arrows with  a single root object, there's no reason why this  should be so - networks with multiple roots rep-  resent tangled structures (structures that share  content), networks that include non-local links  might be mixed representations, containing in-  formation of more than one sort. Such tech-  niques might be useful for improving generator  efficiency, or representing canned text or tem-  plates, cf. (Calder et al., 1999).  Par t ia l  and  Opaque s t ruc tures   Partial structures are essential when a module  needs to produce a skeleton of a representa-  tion that it does not have the competence to  completely fill out. For instance, lexical choice  brings with it certain syntactic commitments,  but in most NLG systems lexical choice occurs  some time before a grammar is consulted to  flesh out syntactic structure in detail.  Figure 2: A partial structure  By simply leaving out local arrows, we can  represent a range of partial structures. Con-  sider Fig. 2, where the triangles represent local  structure, representing a sentence object and its  component verb phrase. There is a link to a sub-  ject noun phrase object, but none of the local  arrows of the actual noun phrase are present. In  subsequent processing this local structure might  be filled in. This is possible as long as the noun  phrase object has been declared to be of the  right type.  An opaque structure is one which has an in-  complete derivational history - for example part  of a syntactic structure without any correspond-  ing semantic structure. Three possible reasons  for having such structures are (a) to allow struc-  ture to be introduced that the generator is not  capable of producing directly, (b) to prevent he  generator from interfering with the structure  thus built (for example, by trying to modify an  idiom in an inappropriate way), or (c) to im-  prove generator efficiency by hiding detail that  may lead to wasteful processing. An opaque  structure is represented simply by the failure  to include a rea l i ses  arrow to that structure.  Such structures provide the basis for a gener-  alised approach to \"canning\".  4 Imp lementat ion   There are many ways that modules in an  NLG system could communicate information  using the representation scheme just outlined.  Here we describe a particularly general model  of inter-module communication, based around  modules communicating with a single cen-  tralised repository of data called the whiteboard  (Calder et al., 1999). A whiteboard is a cumu-  lative typed relational blackboard:   t yped  and  re lat iona l :  because it is based  on using the above representation scheme;   a b lackboard :  a control architec-  ture and data store shared between  processing modules; typically, modules  add/change/remove objects in the data  store, examine its contents, and/or ask to  be notified of changes;   cumulat ive :  unlike standard blackboards,  once data is added, it can't be changed or  removed. So a structure is built incremen-  tally by making successive copies of it (or of  constituents of it) linked by rev ises  links  (although actually, there's no constraint on  the order in which they are built).  A whiteboard allows modules to add ar-  rows (typically forming networks through ar-  rows sharing source or target objects), to in-  spect the set of arrows looking for particular  configurations of types, or to be informed when  a particular type of arrow (or group of arrows)  is added.  The whiteboard is an active database server.  This means that it runs as an independent pro-  cess that other modules connect o by appropri-  ate means. There are essentially three kinds of  interaction that a module might have with the  whiteboard server:   pub l i sh  - add an arrow or arrows to the  whiteboard;  122   query  - look for an arrow or arrows in the  whiteboard;   wa i t  - register interest in an arrow or ar-  rows appearing in the whiteboard.  In both query and wait ,  arrows are specified  by type, and with a hierarchical type system on  objects and relations, this amounts to a pattern  that matches arrows of subtypes as well. The  wait  function allows the whiteboard to take the  initiative in processing - if a module wai ts  on a  query then the whiteboard waits until the query  is satisfied, and then tells the module about it.  So the module does not have to continuously  scan the whiteboard for work to do, but can  let the whiteboard tell it as soon as anything  interesting happens.  Typically a module will start up and regis-  ter interest in the kind of arrow that represents  the module's input data. It will then wait for  the whiteboard to notify it of instances of that  data (produced by other modules), and when-  ever anything turns up, it processes it, adding  its own results to the whiteboard. All the mod-  ules do this asynchronously, and processing con-  tinues until no module has any more work to  do. This may sound like a recipe for confusion,  but more standard pipelined behaviour is not  much different. In fact, pipelining is exactly a  data-based constraint - the second module in a  pipeline does not start until the first one pro-  duces its output.  However, to be a strict pipeline, the first mod-  ule must produce all of its output before the sec-  ond one starts. This can be achieved simply by  making the first module produce all its output  at once, but sometimes that is not ideal - for ex-  ample if the module is recursive and wishes to  react to its own output. Alternative strategies  include the use of markers in the whiteboard,  so that modules can tell each other that they've  finished processing (by adding a marker), or  extending the whiteboard architecture itself so  that modules can tell the whiteboard that they  have finished processing, and other modules can  wait for that to occur.  5 Reconst ruct ion  o f  the  Capt ion   Generat ion  System  In order to prove this representation scheme  in practice, we have implemented the white-  board in Sicstus Prolog and used it to support  data communications between modules in a re-  construction of the Caption Generation System  (Mittal et al., 1995). CGS is a system developed  at the University of Pittsburgh, which takes in-  put from the SAGE graphics presentation sys-  tem (Roth et al., 1994) and generates captions  for the graphics SAGE produces. We selected it  for this effort because it appeared to be a fairly  simple pipelined system, with modules perform-  ing clearly defined linguistic tasks. As such, we  thought it would be a good test case for our  whiteboard specification.  Although the CGS is organised as a pipeline,  shown in Figure 3, the representations commu-  nicated between the modules do not correspond  to complete, separate instances of RAGS data-  type representations. Instead, the representa-  tions at the various levels accumulate along the  pipeline or are revised in a way that does not  correspond exactly to module boundaries. Fig-  ure 3 gives a simple picture of how the different  levels of representation build up. The labels for  the RAGS representations refer to the following:   I = conceptual;   II -- semantic;   I I I  = rhetorical;   IV = document;   V = syntactic.  For instance, some semantic (II) information is  produced by the Text Planning module, and  more work is done on this by Aggregation, but  the semantic level of representation is not com-  plete and final until the Referring Expression  module has run. Also, for instance, at the  point where the Ordering module has run, there  are partially finished versions of three different  types of representation. It is clear from this that  the interfaces between the modules are more  complex than could be accounted for by just re-  ferring to the individual evels of representation  of RAGS. The ability to express combinations of  structures and partial structures was fundamen-  tal to the reimplementation of CGS. We high-  light below a few of the interesting places where  these features were used.  123  AbsSemRep  I-el ~ ~  .................................... SemRep  --(~------~_set{KBPredl ~ fun(Role,set(KBId)) I-el ~3-e l   . . . .  /X  . . . . . . . .   el agent affected . . . .  DR fun(Role,set(SemRep)) ~i/  ~ ..... ~ el?set(SemPredi~t A ~ .   nresent set(KSld) 0 . . . . . .  v  ~--\"- ................. / agen, /  \\a\\] Jec,ea  el / \\ el . . . . .  \" . . . . . . . . . .  ~ J / \"k~ present S~mRep SemRep  chart1 chart2  Figure 4: Combined Abstract Semantic Representation a d Concrete Semantic Representation for  the output: \"These two charts present information about house sales from data-set ts-1740\"  CG$ aroh i ta ,~ lu 'e  RAGS representat/on$  II I l l  IV ~' SAGE  - -  . . . . . . . . . .   tuning II  - . . . . . . . . . .   I1 I11 iV   --' . . . . . . . . . .   I\\[ I11 IV  . . . . . . . . . .  I ;11@  11 III I v  v  . . . . . . . . .   II I11 IV V  . . . . . . . . .  III1   II 111 IV V  l - -  . . . . . . . . . .  I I I I I   FUF  Figure 3: A RAGS view of the CGS system  5.1 Referr ing Express ion Generat ion   In many NLG systems, (nominal) referring ex-  pression generation is an operation that is in-  voked at a relatively late stage, after the struc-  ture of individual sentences i  fairly well speci-  fied (at least semantically). However, referring  expression generation eeds to go right back to  the original world model/knowledge base to se-  lect appropriate semantic ontent o realise a  particular conceptual item as an NP (whereas  all other content has been determined much ear-  lier). In fact, there seems to be no place to  put referring expression generation i a pipeline  without there being some resulting awkward-  ness.  In RAGS, pointers to conceptual items can  be included inside the first, \"abstract\", level of  semantic representation (AbsSemRep), which is  intended to correspond to an initial bundling of  conceptual material under semantic predicates.  On the other hand, the final, \"concrete\", level  of semantic representation (SemRep) is more  like a fully-fledged logical form and it is no  longer appropriate for conceptual material to  be included there. In the CGS reimplementa-  tion, it is necessary for the Aggregation mod-  ule to reason about the final high-level semantic  representation f sentences, which means that  this module must have access to \"concrete\" se-  mantic representations. The Referring Expres-  sion generation module does not run until later,  which means that these representations cannot  be complete.  Our way around this was to ensure that the  initial computation of concrete semantics from  abstract semantics (done as part of Aggrega-  tion here) left a record of the relationship by  including realises arrows between correspond-  ing structures. That computation could not be  completed whenever it reached conceptual ma-  terial - at that point it left a \"hole\" (an ob-  ject with no further specification) in the con-  crete semantic representation li ked back to the  conceptual material. When referring expression  was later invoked, by following the arrows in the  124  resulting mixed structure, it could tell exactly  which conceptual entity needed to be referred  to and where in the semantic structure the re-  sulting semantic expression should be placed.  Figure 4 shows the resulting arrangement for  one example CGS sentence. The dashed lines  indicate realises, i.e. non-local, arrows.  5.2 Handling Centering Information  The CGS Centering module reasons about the  entities that will be referred to in each sentence  and produces a representation which records the  forward and backward-looking centers (Grosz et  al., 1995). This representation is later used by  the Referring Expression generation module in  making pronominalisation decisions. This in-  formation could potentially also be used in the  Realisation module.  Since Centering is not directly producing re-  ferring expressions, its results have to sit around  until they can actually be used. This posed  a possible problem for us, because the RAGS  framework does not provide a specific level of  representation for Centering information and  therefore seems on first sight unable to account  for this information being communicated be-  tween modules. The solution to the problem  came when we realised that Centering informa-  tion is in fact a kind of abstract syntactic in-  formation. Although one might not expect ab-  stract syntactic structure to be determined until  the Realisation module (or perhaps lightly ear-  lier), the CGS system starts this computation i the Centering module.  Thus in the reimplementation, the Centering  module computes (very partial) abstract syn-  tactic representations for the entities that will  eventually be realised as NPs. These represen-  tations basically just indicate the relevant Cen-  tering statuses using syntactic features. Figure  5 shows an example of the semantics for a typi-  cal output sentence and the two partial abstract  syntactic representations computed by the Cen-  tering module for what will be the two NPs in  that sentence 2. As before, dashed lines indicate  realises arrows. Of course, given the discussion  of the last section, the semantic representation  objects that are the source of these arrows are in  fact themselves linked back to conceptual enti-  ties by being the destination of realises arrows  2FVM = Feature Value Matrix.  from them.  When the Referring Expression generation  module runs, it can recover the Centering infor-  mation by inspecting the partial syntactic rep-  resentations for the phrases it is supposed to  generate. These partial representations are then  further instantiated by, e.g., Lexical Choice at  later stages of the pipeline.  6 Conc lus ion   The representation scheme we have proposed  here is designed specifically to support he re-  quirements of the current state-of-the-art NLG  systems, and our pilot implementation demon-  strates the practical applicability of the pro-  posal. Tangled, partial and mixed structures  are of obvious utility to any system with a flex-  ible control strategy and we have shown here  how the proposed representation scheme sup-  ports them. By recording the derivational his-  tory of computations, it also supports decisions  which partly depend on earlier stages of the  generation process (e.g., possibly, lexical choice)  and revision-based architectures which typically  make use of such information. We have shown  how the representation scheme might be the ba-  sis for an inter-module communication model,  the whiteboard, which supports a wide range of  processing strategies that require the represen-  tation of complex and evolving data dependem  cies. The fact that the whiteboard is cumula-  tive, or monotonic in a logical sense, means that  the whiteboard also supports reasoning about  the behaviour of NLG systems implemented in  terms of it. This is something that we would  like to exploit directly in the future.  The reimplementation f the CGS system  in the RAGS framework was a challenge to  the framework because it was a system that  had already been developed completely inde-  pendently. Even though we did not always un-  derstand the detailed motivation for the struc-  ture of CGS being as it was, within a short time  we reconstructed a working system with mod-  ules that corresponded closely to the original  CGS modules. The representation scheme we  have proposed here was a key ingredient in giv-  ing us the flexibility to achieve the particular  processing scheme used by CGS whilst remain-  ing faithful to the (relatively simple) RAGS  data model.  125  SemRep  fun(Role,setlSemRep))  sl S \" ' .   t t ~ .   2 AbsSynRep \"~ AbsSynRep _(:5 ~ ,   , , / \\ \\  ckward-looking-cemer ckward.looking-cenler  + +  Figure 5: Arrangement of centering information for the output sentence above  The representation scheme is useful in situa-  tions where modules need to be defined and im-  plemented to work with other modules, possibly  developed by different people. In such cases, the  representation scheme we propose permits pre-  cise definition of the interfaces of the modules,  even where they are not restricted to a single  'level' of representation. Even though the con-  trol structure of CGS is quite simple, we found  that the use of a centralised whiteboard was use-  ful in helping us to agree on interfaces and on  the exact contribution that each module should  be making. Ultimately, it is hoped that the use  of a scheme of this type will permit much more  widespread 'plug-and-play' among members of  the NLG community.  Re ferences   Lynne Cahill, Christy Doran, Roger Evans, Chris  Mellish, Daniel Paiva, Mike Reape, Donia Scott,  and Neil Tipper. 1999a. In Search of a Reference  Architecture for NLG Systems. In Proceedings of  the 7th European Workshop on Natural Language  Generation, pages 77-85, Toulouse.  Lynne Cahill, Christy Doran, Roger Evans, Chris  Mellish, Daniel Paiva, Mike Reape, Donia Scott,  and Neil Tipper. 1999b. Towards a Reference  Architecture for Natural Language Genera-  tion Systems. Technical Report ITRI-99-14,  Information Technology Research Institute  (ITRI), University of Brighton. Available at  http://www, i t r i  .brighton. ac. uk/proj ects/rags.   Jo Calder, Roger Evans, Chris Mellish, and Mike  Reape. 1999. \"Free choice\" and templates: how  to get both at the same time. In \"May I speak  freely?\" Between templates and free choice in nat-  ural \u001b[1;32;40m language \u001b[0;0m generation, number D-99-01, pages  19-24. Saarbriicken.  B.J. Grosz, A.K. Joshi, and S. Weinstein. 1995.  Centering: a framework for modelling the local co-  herence of discourse. Computational Linguistics,  21 (2):203-226.  V. O. Mittal, S. Roth, J. D. Moore, J. Mattis, and  G. Carenini. 1995. Generating explanatory cap-  tions for information graphics. In Proceedings of  the 15th International Joint Conference on Ar-  tificial Intelligence (IJCAI'95), pages 1276-1283,  Montreal, Canada, August.  V. O. Mittal, J. D. Moore, G. Carenini, and S. Roth.  1998. Describing complex charts in \u001b[1;32;40m natural \u001b[0;0m lan-  guage: A caption generation system. Computa-  tional Linguistics, 24(3):431-468.  Ehud Reiter. 1994. Has a consensus NL generation  architecture appeared and is it psycholinguisti-  cally plausible? In Proceedings of the Seventh In-  ternational Workshop on Natural Language Gen-  eration, pages 163-170, Kennebunkport, Maine.  Steven F. Roth, John Kolojejchick, Joe Mattis, and  Jade Goldstein. 1994. Interactive graphic design  using automatic presentation knowledge. In Pro-  ceedings of CHI'9~: Human Factors in Computing  Systems, Boston, MA.  126  \n",
            "-----------------------------\n",
            "--- document 13: --- document 13: J avox: A Toolkit for Building Speech-Enabled Applications  Michae l  S. Fu lkerson  and A lan  W.  B ie rmann  Department  of Computer  Science  Duke University  Durham,  North Carol ina 27708, USA  {msf, awb}@cs, duke. edu  Abst rac t   JAVOX provides a mechanism for the development  of spoken-\u001b[1;32;40m language \u001b[0;0m systems from existing desktop  applications. We present an architecture that al-  lows existing Java 1 programs to be speech-enabled  with no source-code modification, through the use  of reflection and automatic modification to the ap-  plication's compiled code. The grammars used in  JAvox are based on the Java Speech Grammar For-  mat (JSGF); JAVOX grammars have an additional  semantic omponent based on our JAVOX Script-  ing Language (JSL). JAVOX has been successfully  demonstrated onreal-world applications.  1 Overv iew  JAVOX is an implemented set of tools that allows  software developers to speech-enable existing appli-  cations. The process requires no changes to the  program's source code: Speech capacity is plugged-  in to the existing code by modifying the compiled  program as it loads. JAVOX is intended to provide  similar functionality o that usually associated with  menus and mouse actions in graphical user interfaces  (GUIs). It is completely programmable - develop-  ers can provide a speech interface to whatever func-  tionality they desire. J ivox  has been successfully  demonstrated with several GUI-based applications.  Previous ystems to assist in the development of spoken-langnage systems (SLSs) have focused on  building stand-alone, customized applications, uch  as (Sutton et al., 1996) and (Pargellis et al., 1999).  The goal of the JAVOX toolkit is to speech-enable  traditional desktop applications - this is similar to  the goals of the MELISSA project (Schmidt et al.,  1998). It is intended to both speed the develop-  ment of SLSs and to localize the speech-specific code  within the application. JAVOX allows developers to  add speech interfaces to applications at the end of  the development process; SLSs no longer need to be  built from the ground up.  We will briefly present an overview of how JAVOX  works, including its major modules. First, we  1Java and Java Speech are registered trademarks of Sun  Microsystems, Inc.  will examine TRANSLATOR, the implemented JAVOX  \u001b[1;32;40m natural \u001b[0;0m anguage processing (NLP) component; its  role is to translate from \u001b[1;32;40m natural \u001b[0;0m \u001b[1;32;40m language \u001b[0;0m utterances  to the JhVOX Scripting Language (JSL). Next, we  will discuss JSL in conjunction with a discussion of  EXECUTER, the interface between JAVOX and the  application. We will explain the JhvOX infrastruc-  ture and its current implementation in Java. In  conclusion, we will discuss the current state of the  project and where it is going.  2 Bas ic  Operat ion   J ivox can be used as the sole location of NLP for  an application; the application is written as a non-  speech-enabled program and JhvOX adds the speech  capability. The current implementation is written  in Java and works with Java programs. The linkage  between the application program and JhvOX is cre-  ated by modifying - at load time - all constructors in  the application to register new objects with JAVOX.  For this reason, the application's source code does  not need any modification to enable JAVOX. A thor-  ough discussion of this technique ispresented in Sec-  tion 4. The schematic n Figure 1 shows a high-level  overview of the JAVOX architecture.  Issuing a voice command begins with a user ut-  terance, which the speech recognizer processes and  passes to the NLP component, TRANSLATOR. We  are using the IBM implementation f Sun's Java  Speech application program interface (API) (Sun  Microsystems, Inc., 1998) in conjunction with IBM's  VIAVOICE. The job of TRANSLATOR - or a differ-  ent module conforming to its API - is to translate  the utterance into a form that represents he corre-  sponding program actions. The current implemen-  tation of TRANSLATOR uses a context-free grammar,  with each rule carrying an optional JSL fragment.  A typical bottom-up arser processes utterances and  a complete JSL program results. The resulting JSL  is forwarded to EXECUTER, where the JSL code is  executed. For example, in a hypothetical banking  application, the utterance add $100 to the account  might be translated into the JSL command:  myBalance = myBa lance  + i00;  105  File Edit Tools  Typical  Desktop  Application  l  ~ r  ~ y  Operating  System  \"~\" : ~l (virtual machine)  , Translator =, ~.  Executer I I 1 ~ ~   ( J Speech I I  l J Recognizer J:  Javox  B\"  !  i .   Figure 1: Schematic of the JAVOX architecture.  The job of EXECUTER - or a different module that  conforms to EXECUTER'S API - is to execute and  monitor upcalls into the running application. The  upcalls are the actual functions that would be made  by the appropriate mouse clicks or menu selections  had the user not used speech. For this reason, we are  currently concentrating our efforts on event-driven  programs, the class of most GUI applications. Their  structure is usually amenable to this approach. Our  implementation f EXECUTER performs the upcalls  by interpreting and executing JSL, though the tech-  nology could be used with systems other than JSL.  In the banking example, EXECUTER would identify  the myBalemce variable and increment i by $100.  The main JAVOX components, TRANSLATOR and  EXECUTER, are written to flexible APIs. Develop-  ers may choose to use their own custom components  instead of these two. Those who want a different  NLP scheme can implement a different version of  TRANSLATOR and - as long as it outputs JSL -  still use EXECUTER. Conversely, those who want a  different scripting system can replace JSL and still  use TRANSLATOR and even EXECUTER's low-level  infrastructure.  3 Javox  Grammars   The JAVOX infrastructure is not tied to any par-  ticular NLP method; in fact, the JAVOX grammar  system is the second NLP implementation we have  used. It is presented here because it is straightfor-  ward, easy to implement, and surprisingly powerful.  JAVOX grammars axe based on Sun's Java Speech  Grammar Format (JSGF) (Sun Microsystems, Inc.,  1998). JSGF is a rule-based, speech-recognition  grammar, designed to specify acceptable input to  a recognizer. In JAVOX grammars, each JSGF  rule  may be augmented with a fragment of JAVOX Script-  ing Language code - we refer to JAVOX grammars as  scriptable grammars. The result of parsing an utter-  ance with a JAVOX grammar is a complete piece of  JSL code, which is then interpreted to perform the  action specified by the user.  The process of speech-enabling an application in  JAVOX consists of writing a grammar that con-  tains the \u001b[1;32;40m language \u001b[0;0m to be used and the correspond-  ing actions to be performed. Building on top of  3SGF means - in many cases - only one file is  needed to contain all application-specific informa-  tion. JSL-specific code is automatically stripped  from the grammar at runtime, leaving an ordinary  JSGF grammar. This JSGF grammar is sent to a  Java-Speech-compliant recognizer as its input gram-  mar. In the current Java implementation, each Java  source file (Foo. java) can have an associated JAVOX  grammar file (Foo. gram) that contains all the infor-  mation needed to speak to the application. Encap-  sulating all \u001b[1;32;40m natural \u001b[0;0m anguage information in one file  also means that porting the application to different  \u001b[1;32;40m language \u001b[0;0ms i  far easier than in most SLSs.  3.1 Ser ip tab le  Grammars   Since JSGF grammars are primarily speech-  recognition grammars, they lack the ability to en-  code semantic information. They only possess a lim-  ited tag mechanism. Tags allow the recognizer to  output a canonical representation f the utterance  instead of the recognition verbatim. For example,  106  publ ic  <ACTION> = move \\[the\\] <PART> <DIR>;  publ ic  <PART> = eyes;  publ ic  <PART> = ( cap I hat );  publ ic  <DIR> = up;  publ ic  <DIR> = down;  Grammar 1: A JSGF fragment from the Mr. Potato Head domain.  the tag rm may be the output from both delete the  file and remove it.  Tags are not implemented in JAVOX grammars;  instead, we augment he rules of JSGF with frag-  ments of a scripting \u001b[1;32;40m language \u001b[0;0m, which contains much  richer semantic information than is possible with  tags. TRANSLATOR receives the raw utterance from  the recognizer and translates it into the appropriate  semantic representation. JAvox grammars do not  mandate the syntax of the additional semantic por-  tion. Though JSL is presented here, TRANSLATOR  has been used to form Prolog predicates and Visual  Basic fragments.  JSGF rules can be explicitly made public or are  implicitly private. Public rules can be imported by  other grammars and can serve as the result of a  recognition; a private rule can be used in a recog-  nition, but cannot be the sole result. The five rules  in Grammar 1 are from a JSGF-only grammar frag-  ment from the Mr. Potato Head 2 domain (discussed  later). Grammar 1 allows eight sentences, uch as  move the eyes up, move the eyes down, move the  cap up, move the cap down, and move cap up. Rule  names are valid Java identifiers enclosed within an-  gle brackets; the left-hand side (LHS) is everything  to the left of the equality sign and the right-hand side  (RHS) is everything to the right. JAVOX grammars  include the standard constructs available in JSGF,  these include:  Impor ts  Any grammar file can be imported into  other grammar files, though only public rules  are exported. This allows for the creation  of grammar libraries. When using JSL, Java  classes can also be imported.  Comments Grammars can be documented using  Java comments: single-line comments ( / / )  and  delimited ones (/* until */).  Parenthesis Precedence can be modified with  parentheses.  A l te rnat ives  A vertical bar ( I ) can be used to sep-  arate alternative elements, as in the <PART> rule  of Grammar 1.  Opt iona ls  Optional elements are enclosed within  brackets (\\[ and \\] ), such as the in Grammar l's  <ACTION> rule.  2Mr. Potato Head is a registered trademark ofHasbro, Inc.  K leene  Star  Operator  A postfix Kleene star (*)  operator can be used to indicate that the pre-  ceding element may occur zero or more times.  P lus  Operator  A similar operator to indicate that  an element may appear one or more times.  A grammar's rules may be organized however the  developer wishes. Some may choose to have one  rule per utterance, while others may divide rules to  the parts-of-speech level or group them by semantic  value. In practice, we tend to write rules grouped by  semantic value for nouns and verbs and at the parts-  of-speech level for function words. Grammar 2shows  the Mr. Potato Head grammar augmented with JSL  fragments.  The semantic omponent of each rule is separated  from the RHS by a colon and delimited with a brace  and colon ({: until :}). Using Grammar 2, the  parse and translation for Move the cap up is shown  in Figure 2.  Each rule may have either one semantic fragment  or any number of named fields. A single fragment  is sufficient when there is a one-to-one correlation  between a lexical item and its representation in the  program. Occasionally, a single lexical item may re-  quire several components to adequately express its  meaning within a program. In Grammar 2, there  is a one-to-one correlation between the direction of  movement and the s l ideUp and s l ideDown func-  tions in the <DIR> rules. These functions can also  written as a single s l ide  function, with the direction  of the movement given by two parametric variables  (cos and sin). In this situation, the direction rule  (<DIR.}/F>) needs to be expressed with two values,  each known as a named field. The word up may be  represented by the named fields cos and sin,  with  the values 0 and 1 respectively.  Another issue in JSL - which does not arise in the  syntax-only JSGF - is the need to uniquely identify  multiple sub-rules of the same type, when they oc-  cur in the same rule. For example, in a geometry  grammar, two <POINT>s may be needed in a rule to  declare a <LINE>, as in:  public <LINE> = make a line from  <POINT> to <POINT> : ...  Uniquely numbering the sub-rules eliminates the  ambiguity as to which <POINT> is which. Numbering  107  publ ic   public  public  public  public  public  public  public  <ACTION> = move [the] <PART> <DIR> : {: <PART>.<DIR>();  :};  <PART> = eyes : {: Canvas.eyes0bj :};  <PART> = ( cap I hat ): {: Canvas.cap0bj :};  <DIR> = up : {: s l ideUp :};  <DIR> = down : {: s l ideDown :};  <ACTION_NF> = slide [the] <PART> <DIR> : {: <PART>.s l ide(<DIR:cos>,<DIR:s in>);  :};  <DIR_NF> = up : cos {: 0 :}  sin {: 1 :};  <DIR_NF> = down : cos {: 0 :}  sin {: -I :};  Grammar  2: A JAVOX grammar fragment for the Mr. Potato Head domain.  |  T   aava,=. e~re -Ob:l  Up ( e =ZAdet~  Figure 2: The JAVOX translation process - NL  to JSL  - for Move the cap up.  can be used in both the RttS and the semantic por-  tion of a rule; numbering is not allowed in the LHS  of a rule. Syntactically, sub-rules are numbered with  a series of single quotes3:  public <LINE> = make a line from  <POINT'> to <POINT''> : ...  3.2 Javox Scr ipt ing Language ( JSL)   The JAVOX Scripting Language (JSL) is a stand-  alone programming \u001b[1;32;40m language \u001b[0;0m, developed for use with  the JAVOX infrastructure. JSL can be used to ma-  nipulate a running Java program and can be thought  of as an application-independent macro \u001b[1;32;40m language \u001b[0;0m.  The EXECUTER module interprets JSL and per-  forms the specified actions. The specifics of JSL  are not important o understanding JAVOX; for this  reason, only a brief summary is presented here.  JSL can read of modify the contents of an ob-  ject's fields (data members) and can execute m th-  ods (member functions) on objects. Unlike Java,  JSL is loosely-typed: Type checking is not done un-  til a given method is executed. JSL has its own  variables, which can hold objects from the host ap-  plication; a JSL variable can store an object of  any type and no casting is required. JSL supports  Java's primitive types, Java's reference types (ob-  jects), and Lisp-like lists. Though JSL does support  3This representation is motivated by the grammars of  (Hipp, 1992).  Java's primitive types, they are converted into their  reference-type equivalent. For example, an integer  is stored as a java. lang. Integer and is converted  back to an integer when needed.  JSL has the standard control flow mechanisms  found in most conventional programming \u001b[1;32;40m language \u001b[0;0ms,  including if-else, for and while loops. With the  exception of the evaluation of their boolean expres-  sions, these constructs follow the syntax and behav-  ior of their Java counterparts. Java requires that  if-else conditions and loop termination criteria be  a boolean value. JSL conditionals are more flexi-  ble; in addition to booleans, it evaluates non-empty  strings as true, empty strings as false, non-zero val-  ues as true, zero as false, non-null objects as true,  and nu l l  as false.  In addition to Java's control f ow mechanisms,  JSL also supports fo reach  loops, similar to those  found in Perl. These loops iterate over both JSL  lists and members of java.util.List, executing  the associated code block on each item. JSL lists  are often constructed by recursive rules in order to  handle conjunctions, as seen in Section 5.  4 Infrastructure  The JAVOX infrastructure has been designed to com-  pletely separate NLP  code from the application's  code. The application still can be run without  JAVOX, as a typical, non-speech-enabled program  - it is only speech-enabled when run with JAVOX.  108  From the application's perspective, JAVOX operates  at the systems-level and sits between the applica-  tion and the operating system (virtual machine), as  shown in Figure 1. TRANSLATOR interfaces with the  speech recognizer and performs all necessary NLP.  EXECUTER interfaces directly with the application  and performs upcalls into the running program.  Java has two key features that make it an ideal  test platform for our experimental implementation:  reflection and a redefineable loading scheme. Re-  flection provides a running program the ability to  inspect itself, sometimes called introspection. Ob-  jects can determine their parent classes; every  class is itself an object in Java (an instance of  j ava.lang.Class). Methods, fields, constructors,  and all class attributes can be obtained from a Class  object. So, given an object, reflection can determine  its class; given a class, reflection can find its meth-  ods and fields. JAVOX uses reflection to (1) map  from the JSL-textual representation of an object  to the actual instance in the running program; (2)  find the appropriate j ava.lang.reflect.Methods  for an object/method-name combination; and (3)  actually invoke the method, once all of its arguments  are known.  Reflection is very helpful in examining the appli-  cation program's tructure; however, prior to using  reflection, EXECUTER needs access to the objects in  the running program. To obtain pointers to the ob-  jects, JAVOX uses JOIE,  a load-time transformation  tool (Cohen et al., 1998). JO IE  allows us to modify  each application class as it is loaded into the virtual  machine. The JAVOX transform adds code to every  constructor in the application that registers the new  object with Executer.  Conceptually, the following  line is added to every constructor:  Executer. register (this).  This modification is done as the class is loaded,  the compiled copy - on disk - is not changed. This  allows the program to still be run without JhVOX,  as a non-speech application. EXECUTER can  - once  it has the registered objects - use reflection to ob-  tain everything else it needs to perform the actions  specified by the JSL.  5 Example   Our longest running test application has been a  Mr. Potato Head program; that allows users to ma-  nipulates a graphical representation of the classic  children's toy. Its operations include those typically  found in drawing programs, to include moving, recol-  oring and hiding various pieces of Mr. Potato Head.  Grammar 3 shows a portion of application's gram-  mar needed to process the utterance Move the eyes  and glasses up. The result of parsing this utterance  is shown in Figure 3.  Once TRANSLATOR has processed an utterance, it forwards the resulting JSL fragment to EXECUTER.  Figure 4 provides a reduced class diagram for the  Mr. Potato Head application; the arrows correspond  to the first iteration in the following trace. The  following steps are performed as the JSL fragment  from Figure 3 is interpreted:  1. A new variable - local to EXECUTER - named  $ i te r  is created. Any previously-declared vari-  able with the same name is destroyed.  2. The fo reach  loop starts by initializing the  loop variable to the first item in the list:  Canvas.eyes0bj. This object's name consists  of two parts; the steps to locate the actual in-  stance in the application are:  (a) The first part of the name, Canvas, is  mapped to the only instance of the Canvas  class in the context of this application.  JAVOX has a reference to the instance be-  cause it registered with EXECUTER when it  was created, thanks to a JO IE  transforma-  tion.  (b) The second part of the name, eyes0bj,  is  found through reflection. Every instance of  Canvas has a field named eyes0bj of type  BodyPaxt. This field is the eyes0bj for  which we are looking.  3. Once eyes0bj is located, the appropriate  method must be found. We determine -  through reflection - that there are two meth-  ods in the BodyPart class with the name move,  as seen in Figure 4.  4. We next examine the two arguments and de-  termine them to be both integers. Had the ar-  guments been objects, fields, or other method  calls, this entire procedure would be done re-  cursively on each.  5. We examine each possible method and deter-  mine that we need the one with two integer  arguments, not the one taking a single Point   argument.  6. Now that we have the object, the method, and  the arguments, the upcall is made and the  method is executed in the application. The re-  sult is that Mr. Potato Head's eyes move up on  the screen.  7. This process is repeated for glass0bj and the  loop terminates.  After this process, both the eyes and glasses have  moved up 20 units and Executer waits for additional  input. The application continues to accept mouse  and keyboard commands, just as it would without  speech.  109  public <modPOS> = move <PARTS> <DIR> : {:  dim Slier;  foreach $iter (<PARTS>)  $iter.move(<DIR:X>,<DIR:Y>);  :};  public <PARTS> = [<ART>] <PART> : {: [<PART>] :};  public <PARTS> = <PARTS> [<CONJ>] [<ART>] <PART> : {:  public <DIR> = up : X {: 0 :} : Y {: -20 :};  public <DIR> = left : X {: -20 :} : Y {: 0 :};  public <ART> = (the [ a I an);  public <CONJ> = ( and I plus );  public <PART> = eyes : {: Canvas.eyesObj :};  public <PART> = glasses : {: Canvas.glassObj :};  [<PARTS> , <PART>] : } ;  Grammar 3: A detailed JAVOX grammar for the Mr. Potato Head domain.    r - -  +- -~ +  <pJu~> -~> \"_ c=,',vam.eye=ob::l  I <=,\" :  <co~>--~ ~ : +  \" + +  I  I <\"=\"  I<=\"\"I<\"\"I I:  <+-'+ l  =,.o. +,._ ,   Figure 3: The translation process for the utterance Move the eyes and g/asses up.  6 Discuss ion and Future Work  In practice, building a JAvox-based, speech in-  terface - for limited-functionality applications - is  straightforward and reasonably quick. To date, we  have used three diverse applications as our test plat-  forms. Speech-enabling the last of these, an image  manipulation program, took little more than one  person-day. Though these applications have been  small; we are beginning to explore JAvOX's scala-  bility to larger applications. We are also develop-  ing a library of JAVOX grammars for use with the  standard Java classes. This resource will shorten  development times even more; especially compared  to building a SLS from the ground up.  One of the existing challenges is to work with  applications consisting entirely of dynamic objects,  those that cannot be identified at load time. Some  typical dynamic-object applications are drawing  programs or presentation software; in both cases,  the user creates the interesting objects during run-  time. We have implemented a system in JSL which  allows objects to be filtered based on an attribute,  such as color in the utterance: Move the blue square.  In situations where there is a one-to-one correla-  tion between a lexical item in the grammar and an  object in the program, it is often the case that the  lexical item is very similar to the element's identi-  fier. It is quite often the same word or a direct syn-  onym. Since JAVOX is primarily performing upcalls  based on existing functions within the program, it  also can be predicted what type of objects will co-  occur in utterances. In the Mr. Potato Head applio  110  f-/,pp f~A-d o-, .........................................................  ~ Canv&i :,TFr 4tma  /i I / i snvas { } / i J Snv\" { } BOC~ylL- { } / t L _ _  l=o~(~i==:~,  Figure 4: A simplified class diagram for the Mr. Potato Head application.  cation, we can assume that objects representing a  Point or integers will occur when the user speaks  of moving a BodyPart. We are developing a system  to exploit hese characteristics to automatically gen-  erate JAVOX grammars from an application's com-  piled code. The automatically-generated grammars  are intended to serve as a starting point for develop-  ers - though they may certainly require some hand  crafting. Our current, grammar-generation oolas-  sumes the program is written with Java's standard  naming conventions. It is imaginable that additional  data sources - such as a sample corpus - will al-  low us to more accurately generate grammars for an  application. Though in its infancy, we believe this  approach olds vast potential for SLS development.  7 Conc lus ion   JAVOX provides a fast and flexible method to add a  speech-interface to existing Java applications. The  application program requires no source-code modifi-  cation: The JAVOX infrastructure provides all NLP  capabilities. We have implemented a grammar and  scripting system that is straightforward enough that  inexperienced developers and those unfamiliar with  NLP can learn it quickly. We have demonstrated the  technology on several programs and are commencing  work on more ambitious applications. The current  implementation f JAVOX is available for download  at:  References  Geoff A. Cohen, Jeffrey S. Chase, and David L.  Kaminsky. 1998. Automatic program transforma-  tion with JOIE. In USENIX Annual Technical  Conference (N098), New Orleans, LA.  D. Richard Hipp. 1992. A New Technique for Pars-  ing Ill-formed Spoken Natural-\u001b[1;32;40m language \u001b[0;0m Dialog.  Ph.D. thesis, Duke University.  Andrew Pargellis, JeffKuo, and Chin-Hui Lee. 1999.  Automatic dialogue generator creates user de-  fined applications. In 6th European Conference on  Speech Communication and Technology, volume 3,  pages 1175--1178, Budapest, Hungary.  Paul Schmidt, Sibylle Rieder, Axel Theofilidis, Mar-  ius Groenendijk, Peter Phelan, Henrik Schulz,  Thierry Declerck, and Andrew Brenenkamp.  1998. Natural \u001b[1;32;40m language \u001b[0;0m access to software applica-  tions. In Proceedings of COLING-ACL-98, pages  1193-1197, Montreal, Quebec.  Sun Microsystems, Inc. 1998. Java speech API spec-  ification 1.0.  Stephen Sutton, David G. Novick, Ronald A. Cole,  Pieter Vermeulen, Jacques de Villiers, Johan  Schalkwyk, and Mark Fanty. 1996. Building  10,000 spoken-dialogue systems. In Proceedings of the International Conference on Spoken Language  Processing (ICSLP), pages 709--712, Philadel-  phia, PA.  http ://www. cs. duke. edu/~msf/j avox   8 Acknowledgments   This work has been partially supported by the De-  fense Advanced Research Projects Agency under  contract F30602-99-C-0060.  111  \n",
            "-----------------------------\n",
            "--- document 14: --- document 14: Abstract  Cross-Language Multimedia Information Retrieval  Sharon Flank  emotion, Inc.  2600 Park Tower Dr., Vienna, VA 22180 USA  sharon.flank@emotion.com  Simple measures can achieve high-accuracy  cross-\u001b[1;32;40m language \u001b[0;0m r trieval in carefully chosen  applications. Image retrieval is one of those  applications, with results ranging from 68%  of human translator performance for  German, to 100% for French.  1 Introduction  contain strings of keywords. Typical queries  are, as in most Web search applications, two  to three words in length. At this point, all of  the captions are in English. eMotion hosts a  large database of images for sale and for  licensing, PictureQuest. At least 10% of  PictureQuest's user base is outside the  United States. The tests were performed on  the PictureQuest database of approximately  400,000 images.  Information is increasingly global, and the  need to access it crosses \u001b[1;32;40m language \u001b[0;0m barriers.  The topic of this paper, cross-\u001b[1;32;40m language \u001b[0;0m  information retrieval, concerns the automatic  retrieval of text in one \u001b[1;32;40m language \u001b[0;0m via a query  in a different \u001b[1;32;40m language \u001b[0;0m. A considerable  body of literature has grown up around  cross-\u001b[1;32;40m language \u001b[0;0m information retrieval (e.g.  Grefenstette 1998, TREC-7 1999). There  are two basic approaches. Either the query  can be translated, or each entire document  can be translated into the same \u001b[1;32;40m language \u001b[0;0m as  the query. The accuracy of retrieval across  \u001b[1;32;40m language \u001b[0;0ms, however, is generally not good.  One of the weaknesses that plagues cross-  \u001b[1;32;40m language \u001b[0;0m retrieval is that we do not have a  good sense of who the users are, or how best  to interact with them.  In this paper we describe a multimedia  application for which cross-\u001b[1;32;40m language \u001b[0;0m  information retrieval works particularly  well. eMotion, Inc. has developed a \u001b[1;32;40m natural \u001b[0;0m  \u001b[1;32;40m language \u001b[0;0m information retrieval application  that retrieves images, such as photographs,  based on short textual descriptions or  captions. The captions are typically one to  three sentences, although they may also  Recent Web utilization data for PictureQuest  indicate that of the 10% of users from  outside the United States, a significant  portion come from Spanish-speaking,  French-speaking, and German-speaking  countries. It is expected that adding  appropriate \u001b[1;32;40m language \u001b[0;0m interfaces and listing  PictureQuest in foreign-\u001b[1;32;40m language \u001b[0;0m search  engines will dramatically increase non-  English usage.  The Cross-Language Multimedia  Retrieval Application  This paper offers several original  contributions to the literature on cross-  \u001b[1;32;40m language \u001b[0;0m information retrieval. First, the  choice of application is novel, and  significant because it simplifies the \u001b[1;32;40m language \u001b[0;0m  problem enough to make it tractable.  Because the objects retrieved are images and  not text, they are instantly comprehensible  to the user regardless of \u001b[1;32;40m language \u001b[0;0m issues.  This fact makes it possible for users to  perform a relevance assessment without he  need for any kind of translation. More  important, users themselves can select  objects of interest, without recourse to  translation. The images are, in fact,  13  associated with caption information, but,  even in the monolingual system, few users  ever even view the captions. It should be  noted that most of the images in  PictureQuest are utilized for advertising and  publishing, rather than for news  applications. Users of history and news  photos do tend to check the captions, and  often users in publishing will view the  captions. For advertising, however, what the  image itself conveys is far more important  than the circumstances under which it was  created.  Another significant contribution of this  paper is the inclusion of a variety of  machine translation systems. None of the  systems tested is a high-end machine  translation system: all are freely available on  the Web.  Another key feature of this paper is the  careful selection of an accuracy measure  appropriate to the circumstances of the  application. The standard measure, percent  of monolingual performance achieved, is  used, with a firm focus on precision. In this  application, users are able to evaluate only  what they see, and generally have no idea  what else is present in the collection. As a  result, precision is of far more interest o  customers than recall. Recall is, however, of  interest to image suppliers, and in any case it  would not be prudent to optimize for  precision without taking into account the  recall tradeoff.  The PictureQuest application avoids several  of the major stumbling blocks that stand in  the way of high-accuracy cross-\u001b[1;32;40m language \u001b[0;0m  retrieval. Ballesteros and Croft (1997) note  several pitfalls common to cross-\u001b[1;32;40m language \u001b[0;0m  information retrieval:  (1) The dictionary may not contain  specialized vocabulary (particularly  bilingual dictionaries).  (2) Dictionary translations are inherently  ambiguous and add extraneous terms  to the query.  (3) Failure to translate multi-term  concepts as phrases reduces  effectiveness.  In the PictureQuest application, these pitfalls  are minimized because the queries are short,  not paragraph-long descriptions as in TREC  (see, e.g., Voorhees and Harman 1999).  This would be a problem for a statistical  approach, since the queries present little  context, but, since we are not relying on  context (because reducing ambiguity is not  our top priority) it makes our task simpler.  Assuming that the translation program keeps  multi-term concepts intact, or at least that it  preserves the modifier-head structure, we  can successfully match phrases. The  captions (i.e. the documents o be retrieved)  are mostly in sentences, and their phrases  are intact. The phrase recognizer identifies  meaningful phrases (e.g. fire engine) and  handles them as a unit. The pattern matcher  recognizes core noun phrases and makes it  more likely that hey will match correctly.  Word choice can be a major issue as well for  cross-\u001b[1;32;40m language \u001b[0;0m retrieval systems. Some  ambiguity problems can be resolved through  the use of a part-of-speech tagger on the  captions. As Resnik and Yarowsky (in  press) observe, part-of-speech tagging  considerably reduces the word sense  disambiguation problem. However, some  ambiguity remains. For example, the  decision to translate a word as car,  automobile, or vehicle, may dramatically  affect retrieval accuracy. The PictureQuest  14  system uses a semantic net based on  WordNet (Fellbaum 1998) to expand terms.  Thus a query for car or automobile will  retrieve ssentially identical results; vehicle  will be less accurate but will still retrieve  many of the same images. So while word  choice may be a significant consideration for  a system like that of Jang et al., 1999, its  impact on PictureQuest is minimal.  The use of WordNet as an aid to information  retrieval is controversial, and some studies  indicate it is more hindrance than help (e.g.  Voorhees 1993, 1994, Smeaton, Kelledy and  O'Donnell 1995). WordNet uses extremely  fine-grained distinctions, which can interfere  with precision even in monolingual  information retrieval. In a cross-\u001b[1;32;40m language \u001b[0;0m  application, the additional senses can add  confounding mistranslations. If, on the  other hand, WordNet expansion is  constrained, the correct ranslation may be  missed, lowering recall. In the PictureQuest  application, we have tuned WordNet  expansion levels and the corresponding  weights attached to them so that WordNet  serves to increase recall with minimal  impact on precision (Flank 2000). This  tuned expansion appears to be beneficial in  the cross-\u001b[1;32;40m language \u001b[0;0m application as well.  Gilarranz, Gonzalo and Verdejo (1997)  point out that, for cross-\u001b[1;32;40m language \u001b[0;0m  information retrieval, some precision is lost  in any case, and WordNet is more likely to  enhance cross-linguistic than monolingual  applications.  In fact, Smeaton and Quigley (1996)  conclude that WordNet is indeed helpful in  image retrieval, in particular because image  captions are too short for statistical analysis  to be useful. This insight is what led us to  develop a proprietary image retrieval engine  in the first place: fine-grained linguistic  analysis is more useful that a statistical  approach in a caption averaging some thirty  words. (Our typical captions are longer than  those reported in Smeaton and Quigley  1996).  3 Translation Methodology  We performed preliminary testing using two  translation methodologies. For the initial  tests, we chose European \u001b[1;32;40m language \u001b[0;0ms: French,  Spanish, and German. Certainly this choice  simplifies the translation problem, but in our  case it also reflects the most pressing  business need for translation. For the  French, Spanish, and German tests, we used  Systran as provided by AltaVista  (Babelfish); we also tested several other  Web translation programs. We used native  speakers to craft queries and then translated  those queries either manually or  automatically and submitted them to  PictureQuest. The resulting image set was  evaluated for precision and, in a limited  fashion, for recall.  The second translation methodology  employed was direct dictionary translation,  tested only for Spanish. We used the same  queries for this test. Using an on-line  Spanish-English dictionary, we selected, for  each word, the top (top-frequency)  translation. We then submitted this word-  by-word translation to PictureQuest.  (Unlike AltaVista, this method spell-  corrected letters entered without the  necessary diacritics.) Evaluation proceeded  in the same manner. The word-by-word  method introduces a weakness in phrase  recognition: any phrase recognition  capabilities in the retrieval system are  defeated if phrases are not retained in the  input. We can assume that the non-English-  speaking user will, however, recognize  phrases in her or his own \u001b[1;32;40m language \u001b[0;0m, and look  15  them up as phrases where possible. Thus we  can expect at least those multiword phrases  that have a dictionary entry to be correctly  understood. We still do lose the noun  phrase recognition capabilities in the  retrieval system, further confounded by the  fact that in Spanish adjectives follow the  nouns they modify. In the hombre de  negocios example in the data below, both  AltaVista and Langenscheidt correctly  identify the phrase as multiword, and  translate it as businessman rather than man  of businesses.  The use of phrase recognition has been  shown to be helpful, and, optimally, we  would like to include it. Hull and  Grefenstette 1996 showed the upper bound  of the improvements possible by using  lexicalized phrases. Every phrase that  appeared was added to the dictionary, and  that tactic did aid retrieval. Both statistical  co-occurrence and syntactic phrases are also  possible approaches. Unfortunately, the  extra-system approach we take here relies  heavily on the external machine translation  to preserve phrases intact. If AltaVista (or,  in the case of Langenscheidt, he user)  recognizes a phrase and translates it as a  unit, the translation is better and retrieval is  likely to be better. If, however, the  translation mistakenly misses a phrase,  retrieval quality is likely to be worse. As for  compositional noun phrases, if the  translation preserves normal word order,  then the PicmreQuest-internal oun phrase  recognition will take effect. That is, ifjeune  fille translates as young girl, then  PictureQuest will understand that young is  an adjective modifying girl. In the more  difficult case, if the translation preserves the  correct order in translating la selva africana,  i.e. the African jungle, then noun phrase  recognition will work. If, however, it comes  out as the jungle African, then retrieval will  be worse. In the architecture d scribed here,  fixing this problem requires access to the  internals of the machine translation program.  4 Evaluation  Evaluating precision and recall on a large  corpus is a difficult task. We used the  evaluation methods detailed in Flank 1998.  Precision was evaluated using a crossing  measure, whereby any image ranked higher  than a better match was penalized. Recall  per se was measured only with respect o a  defined subset of the images. Ranking  incorporates some recall measures into the  precision score, since images ranked too low  are a recall problem, and images marked too  high are a precision problem. If there are  three good matches, and the third shows up  as #4, the bogus #3 is a precision problem,  and the too-low #4 is a recall problem.  For evaluation of the overall cross-\u001b[1;32;40m language \u001b[0;0m  retrieval performance, we simply measured  the ratio between the cross-\u001b[1;32;40m language \u001b[0;0m and  monolingual retrieval accuracy (C/M%).  This is standard; see, for example, Jang et al.  1999.  Table 1 illustrates the percentage of  monolingual retrieval performance we  achieved for the translation tests performed.  In this instance, we take the precision  performance of the human-translated queries  and normalize it to 100%, and adjust the  other translation modalities relative to the  human baseline.  Language Raw  Precision (%)  French (Human) 80  French 86  (AltaVista)  French 66  (Transparent  Language)  C/M  (%)  100  100  83  16  Language Raw  Precision (%)  French (Intertran) 44  Spanish (Human) 90  Spanish 53  (AltaVista)  63 Spanish  (Langenscheidt  Bilingual  Dictionary)  German (Human) 80  German 54  (AltaVista)  C/M  (%)  55  100  59  70  100  68  Several other factors make the PictureQuest  application a particularly good application  for machine translation technology. Unlike  document ranslation, there is no need to  match every word in the description; useful  images may be retrieved even if a word or  two is lost. There are no discourse issues at  all: searches never use anaphora, and no one  cares if the translated query sounds good or  not.  In addition, the fact that the objects being  retrieved were images greatly simplified the  endeavor. Under normal circumstances,  developing a user-friendly interface is a  major challenge. Users with only limited (or  nonexistent) reading knowledge of the  \u001b[1;32;40m language \u001b[0;0m of the documents need a way to  determine, first, which ones are useful, and  second, what they say. In the PictureQuest  application, however, the retrieved assets are  images. Users can instantly assess which  images meet heir needs.  In conclusion, it appears that simple on-line  translation of queries can support effective  cross-\u001b[1;32;40m language \u001b[0;0m information retrieval, for  certain applications. We showed how an  image retrieval application eliminates ome  of the problems of cross-\u001b[1;32;40m language \u001b[0;0m r trieval,  and how carefully tuned WordNet expansion  simplifies word choice issues. We used a  variety of machine translation systems, none  of them high-end and all of them free, and  nonetheless achieved commercially viable  results.  5 Appendix: Data  Source Example Score  Human men repairing road 100  AV men repairing wagon 0  Lang. man repair oad 100  Human woman wearing red 100  shopping in store  AV woman dressed red buying 90 (2 of  in one tends 20 bad)  Lang. woman clothe red buy in wearing  shop red is lost  75 (5 of  20 bad)  Human cars driving on the 100  highway  AV cars handling by the 80' (4 of  freeway 20 bad)  Lang. cart handle for the 0  expressway  Human lions hunting in the 80 (1 of 5  African forest bad)  AV lions hunting in the 80 (1 of 5  African forest bad)  Lang. lion hunt in thejungle 45 (11 of  gSt \\] I 20 bad)  ~'~ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  I~:~ i ~  Human juggler using colorful balls 67 (1 of 3  bad)  AV juggler with using balls of 50 (4 of 8  colors bad)  Lang. juggler by means of use (0; 1  ball colour should be  there)  17  Source Example Score  Human blonde children playing 90(#3  with marbles should be  #1;  remainder  of top 20  ok)  AV blond children playing 90 (2 of  with marbles 20 bad)  Lang. young fair play by means 50 (1 of 2  of marble bad)  Human buying power  AV spending power 45 (11 of  20 bad)  Lang.  AV  purchasing power 100  successful businessman i 60 (8 of  office 20 bad)  Lang. successful businessman i 6 (8 of 20  office bad)  Human mother and daughter 100 (but  baking bread in the kitchen no full  matches)  AV mother and daughter 30 (14 of  \\[horneando-removed\\] 20 bad)  bread in the kitchen  Lang. mother and child bake 100 (but  bread in the kitchen no full  matches)  Human old age and loneliness 100  AV oldness and solitude 0  Lang. old age and loneliness 100  5.1 Spanish  Human translations, tested on PictureQuest:  90% (normalize to 100%)  AltaVista: 53% (59% normalized)  Langenscheidt, word-by-word: 63% (70%  normalized)  5.1.1 AltaVista  For AltaVista, we left out the words that  AltaVista didn't translate.  5.1.2 Langenscheidt  Langenscheidt, word-by-word: 63% (70%  normalized)  For the Langenscheidt word-by-word, we  used the bilingual dictionary to translate  each word separately as if we knew no  English at all, and always took the first  translation. We made the following  adjustments:  1. Left out \"una,\" since Langenscheidt  mapped it to \"unir\" rather than to either a or  one  2. Translated \"e\" as and instead of  e  5.2 French  Human translations, tested on PictureQuest:  80%  AltaVista: 86% (100% normalized)  Transparent Language (freetranslation.com):  66% (83% normalized)  Intertran (www.intertran.net:2000): 44%  (55% normalized)  \\[French examples originally drawn from  http ://humanities.uchicago.edu/ARTFL/proj  ects/academie/1835.searchform.html:  French-French\\]  Source : Example Score  ~,, ~ i!, ~ii~l! \"  ~:s~:: ~ ~'~  ~  Human signs of the zodiac 100  AV signs of the zodiac 100  TrLang sign zodiaque 0  IntrTran  Human  \\[signes\\] any zodiac  fish in water  100  30 (14 of 20  bad)  AV fish in water 30 (14 of 20  bad)  TrLang fish in water 30 (14 of 20  bad)  fish at water IntrTran 30 (14 of 20  bad)  18  Source Example Score  i  Human painful earaches lO0  AV Painful earaches 100  TrLang the painful ear evil 0  the \\[manx\\] \\[doreille\\]' 0  distressing  to take a rabbit by the  ears  To take a rabbit by the  IntrTran  ,~ ~ ~ii ~  Human  AV  65 (7 of 20  bad)  65 (7 of 20  bad) ears  TrLang take a rabbit by the ears 65 (7 of 20  bad)  IntrTran  Human  capture a bunny by the  ears  cat which lives in wood  80 (1 of 5  bad)  %~!,~:,.' i~: ~'\"  45 (11 of 20  bad)  AV Cat which lives in wood 45 (11 of 20  bad)  TrLang cat that lives in wood 65 (7 of 20  bad)  cat thanksgiving lives at  the forest  to leave a house  IntrTran  Human  70 (6 of 20  bad)  60 (8 of 20  bad)  AV To leave a house 60 (8 of 20  bad)  TrLang to go out of a house 95 (1 of 20  bad)  IntrTran come out dune' dwelling 90 (18 of 20  house bad)  Human carpenter's tool 95 (1 of 20  bad)  AV Instrument of carpenter 100  TrLang instrument of carpenter 100  I IntrTran implement any carpenter 35 (13 of 20  bad)  Human to play the violin 100  AV to play of the violin 100  TrLang to play the violin 100  IntrTran gamble any violin 0  Human pleasures of the body 100  Source Example Score  AV Pleasures of the body 100  100 TrLang  IntrTran  the pleasures of the body  the delight any body  Human a girl eats fruit  AV a girl eats fruit 100  TrLang a girl eats fruit 100  IntrTran a girl am eating any fruit 65 (7 of 20  bad)  0  100  5.3 German  Human translations, tested on PictureQuest:   80% (100% normal ized)   AltaVista 54% (68% normal ized)   Source Example Score  Human boys golf course 95  AV golf course 95  Human artificial paradise 100  AV artificial paradiese 0  Human solar energy for automobiles 95  AV solar energy for auto 95 ........................ ~, , ,~ :~,,~ . ~.~ ~ ~ ~; : .  , .  ~<.~  Human hiking through the forest 90  AV migrations by the forest 0  Human an elephant in a zoo 25  (#17  should  be #2)  AV elephant in the zoo 100  ............... i!~ n = ~!~ ~ ~  Human the synthesis of I00  desoxyribonucleic acid  AV the synthesis of the 0  Desoxynribonukleinsaeure  Human black cars 100  AV black auto 100  Human playing together 60  young together play  19  Source Example Score  Human women in blue 65  AV Ladies in blue 75  Human woman at work 65  AV Ladies on work 40  6 Acknowledgements  I am grateful to Doug Oard for comments on  an earlier version of  this paper.  7 References  Ballesteros, Lisa, and W. Bruce Croft, 1997. \"Phrasal  Translation and Query Expansion Techniques for  Cross-Language Information Retrieval,\" in AAAI  Spring Symposium on Cross-Language Text and  Speech Retrieval, Stanford University, Palo Alto,  California, March 24-26, 1997.  Fellbaum, Christiane, ed., 1998. WordNet: An  Electronic Lexical Database. Cambridge, MA: MIT  Press.  Flank, Sharon. 2000. \"Does WordNet Improve  Multimedia Information Retrieval?\" Working paper  Flank, Sharon. 1998 \"A Layered Approach to NLP-  Based Information Retrieval,\" in Proceedings of  COLING-ACL, 36th Annual Meeting of the  Association for Computational Linguistics, Montreal,  Canada, 10-14 August 1998.  Gilarranz, Julio, Julio Gonzalo and Felisa Verdejo.  1997. \"An Approach to Conceptual Text Retrieval  Using the EuroWordNet Multilingual Semantic  Database,\" in AAAI Spring Symposium on Cross-  Language Text and Speech Retrieval, Stanford  University, Palo Alto, California, March 24-26,  1997. (http://www.clis.umd.edu/dlrg/filter/sss/papers)  Grefenstette, Gregory, ed., 1998. Cross-Language  Information Retrieval. Norwell, MA: Kluwer.  Hull, David A. and Gregory Grefenstette, 1996.  \"Experiments in Multilingual Information Retrieval,\"  m Proceedin s o the 19 th L  \" g f nternational Conference  on Research and Development in Information  Retrieval (SIGIR96) Zurich, Switzerland.  Jang, Myung-Gil, Sung Hyon Myaeng, and Se  Young Park, 1999. \"Using Mutual Information to  Resolve Query Translation Ambiguities and Query  Term Weighting,\" in Proceedings of 37 th Annual  Meeting of the Association for Computational  Linguistics, College Park, Maryland.  McCarley, J. Scott, 1999. \"Should We Translate the  Documents or the Queries in Cross-Language  Information Retrieval?\"  Resnik, Philip and Yarowsky, David, in press.  \"Distinguishing Systems and Distinguishing Sense:  New Evaluation Methods for Word Sense  Disambiguation,\" Natural Language Engineering.  Smeaton, Alan F., F. Kelledy and R. O'Donnell,  1995. \"TREC-4 Experiments at Dublin City  University: Thresholding Posting Lists, Query  Expansion with WordNet and POS Tagging of  Spanish,\" in Donna K. Harman (ed.) NIST Special  Publication 500-236: The Fourth Text REtrieval  Conference (TREC-4), Gaithersburg, MD, USA:  Department of Commerce, National Institute of  Standards and Technology.  (http://trec.nist.gov/pubs/trec4/t4_proceedings.html)  Smeaton, Alan F. and I. Quigley, 1996. \"Experiments  on Using Semantic Distances Between Words in  Image Caption Retrieval,\" in Proceedings of the 19 th  International Conference on Research and  Development in Information Retrieval (SIGIR96)  Zurich, Switzerland.  Voorhees, Ellen M. 1994. \"Query Expansion Using  Lexical-Semantic Relations,\" in Proceedings of the  17 th International ACM SIGIR Conference on  Research and Development in Information Retrieval,  pp. 61-70.  Voorhees, Ellen M. 1993. \"Using WordNet to  Disambiguate Word Senses for Text Retrieval,\" in  Proceedings of the 16 th International ACM SIGIR  Conference on Research and Development in  Information Retrieval, pp. 171-180.  Voorhees, Ellen M. and Donna K. Harman, editors,  1999 The 7 th Text Retrieval Conference (TREC- 7).  20  \n",
            "-----------------------------\n",
            "--- document 15: --- document 15: Disti l l ing dialogues - A method using \u001b[1;32;40m natural \u001b[0;0m dialogue  dialogue systems development  Arne  JSnsson  and  N i l s  Dah lb~ick   Depar tment  of Computer  and  In format ion  Sc ience  L inkSp ing  Un ivers i ty   S-581 83, L INKOPING  SWEDEN  nilda@ida.liu.se, arnjo@ida.liu.se  corpora for  Abst ract   We report on a method for utilising corpora col-  lected in \u001b[1;32;40m natural \u001b[0;0m settings. It is based on distilling  (re-writing) \u001b[1;32;40m natural \u001b[0;0m dialogues to elicit the type of  dialogue that would occur if one the dialogue par-  ticipants was a computer instead of a human. The  method is a complement toother means uch as Wiz-  ard of Oz-studies and un-distilled \u001b[1;32;40m natural \u001b[0;0m dialogues.  We present he distilling method and guidelines for  distillation. We also illustrate how the method af-  fects a corpus of dialogues and discuss the pros and  cons of three approaches in different phases of dia-  logue systems development.  1 In t roduct ion   It has been known for quite some time now, that  the \u001b[1;32;40m language \u001b[0;0m used when interacting with a comput-  er is different from the one used in dialogues between  people, (c.f. JSnsson and Dahlb~ick (1988)). Given  that we know that the \u001b[1;32;40m language \u001b[0;0m will be different,  but not how it will be different, we need to base  our development of \u001b[1;32;40m natural \u001b[0;0m \u001b[1;32;40m language \u001b[0;0m dialogue sys-  tems on a relevant set of dialogue corpora. It is our  belief that we need to clarify a number of different  issues regarding the collection and use of corpora in  the development of speech-only and multimodal dia-  logue systems. Exchanging experiences and develop-  ing guidelines in this area are as important as, and in  some sense a necessary pre-requisite to, the develop-  ment of computational models of speech, \u001b[1;32;40m language \u001b[0;0m,  and dialogue/discourse. It is interesting to note the  difference in the state of art in the field of natu-  ral \u001b[1;32;40m language \u001b[0;0m dialogue systems with that of corpus  linguistics, where issues of the usefulness of different  samples, the necessary sampling size, representative-  ness in corpus design and other have been discussed  for quite some time (e.g. (Garside t al., 1997; Atkins  et al., 1992; Crowdy, 1993; Biber, 1993)). Also the  neighboring area of evaluation of NLP systems (for  an overview, see Sparck Jones and Galliers (1996))  seems to have advanced further.  Some work have been done in the area of natu-  ral \u001b[1;32;40m language \u001b[0;0m dialogue systems, e.g. on the design  of Wizard of Oz-studies (Dahlb~ck et al., 1998),  on measures for inter-rater eliability (Carletta,  1996), on frameworks for evaluating spoken dialogue  agents (Walker et al., 1998) and on the use of differ-  ent corpora in the development of a particular sys-  tem (The Carnegie-Mellon Communicator, Eskenazi  et al. (1999)).  The question we are addressing in this paper is  how to collect and analyse relevant corpora. We be-  gin by describing what we consider to be the main  advantages and disadvantages of the two currently  used methods; studies of human dialogues and Wiz-  ard of Oz-dialogues, especially focusing on the eco-  logical validity of the methods. We then describe a  method called 'distilling dialogues', which can serve  as a supplement to the other two.  2 Natural and Wizard of  Oz-Dialogues  The advantage of using real dialogues between peo-  ple is that they will illustrate which tasks and needs  that people actually bring to a particular service  provider. Thus, on the level of the users' general  goals, such dialogues have a high validity. But there  are two drawbacks here. First; it is not self-evident  that users will have the same task expectations from  a computer system as they have with a person. Sec-  ond, the \u001b[1;32;40m language \u001b[0;0m used will differ from the \u001b[1;32;40m language \u001b[0;0m  used when interacting with a computer.  These two disadvantages have been the major  force behind the development of Wizard of Oz-  methods. The advantage here is that the setting will  be human-computer interaction. But there are im-  portant disadvantages, too. First, on the practical  side, the task of setting up a high quality simulation  environment and training the operators ('wizards')  to use this is a resource consuming task (Dahlb~ck et  al., 1998). Second, and probably even more impor-  tant, is that we cannot hen observe real users using  a system for real life tasks, where they bring their  own needs, motivations, resources, and constraints  to bear. To some extent this problem can be over-  come using well-designed so called 'scenarios'. As  pointed out in Dahlb~ck (1991), on many levels of  analysis the artificiality of the situation will not af-  44  fect the \u001b[1;32;40m language \u001b[0;0m used. An example of this is the  pattern of pronoun-antecedent relations. But since  the tasks given to the users are often pre-described  by the researchers, this means that this is not a good  way of finding out which tasks the users actually  want to perform. Nor does it provide a clear enough  picture on how the users will act to find something  that satisfies their requirements. If e.g. the task is  one of finding a charter holiday trip or buying a TV-  set within a specified set of constraints (economical  and other), it is conceivable that people will stay  with the first item that matches the specification,  whereas in real life they would probably look for  alternatives. In our experience, this is primarily a  concern if the focus is on the users' goals and plans,  but is less a problem when the interest is on lower-  level aspects, such as, syntax or patterns of pronoun-  antecedent relationship (c.f. Dahlb~ick (1991)).  To summarize; real life dialogues will provide a  reasonably correct picture of the way users' ap-  proach their tasks, and what tasks they bring to  the service provider, but the \u001b[1;32;40m language \u001b[0;0m used will not  give a good approximation of what the system un-  der construction will need to handle. Wizard of Oz-  dialogues, on the other hand, will give a reasonable  approximation of some aspects of the \u001b[1;32;40m language \u001b[0;0m used,  but in an artificial context.  The usual approach has been to work in three  steps. First analyse real human dialogues, and based  on these, in the second phase, design one or more  Wizard of Oz-studies. The final step is to fine-tune  the system's performance on real users. A good ex-  ample of this method is presented in Eskenazi et al.  (1999). But there are also possible problems with  this approach (though we are not claiming that this  was the case in their particular project). Eskenazi et  al. (1999) asked a human operator to act 'computer-  like' in their Wizard of Oz-phase. The advantage  is of course that the human operator will be able  to perform all the tasks that is usually provided by  this service. The disadvantage is that it puts a heavy  burden on the human operator to act as a comput-  er. Since we know that lay-persons' ideas of what  computers can and cannot do are in many respects  far removed from what is actually the case, we risk  introducing some systematic distortion here. And  since it is difficult to perform consistently in similar  situations, we also risk introducing non-systematic  distortion here, even in those cases when the 'wiz-  ard' is an NLP-professional.  Our suggestion is therefore to supplement he  above mentioned methods, and bridge the gap be-  tween them, by post-processing human dialogues to  give them a computer-like quality. The advantage,  compared to having people do the simulation on the  fly, is both that it can be done with more consis-  tency, and also that it can be done by researchers  that actually know what human-computer \u001b[1;32;40m natural \u001b[0;0m  \u001b[1;32;40m language \u001b[0;0m dialogues can look like. A possible dis-  advantage with using both Wizard of Oz-and real  computer dialogues, is that users will quickly adapt  to what the system can provide them with, and will  therefore not try to use it for tasks they know it  cannot perform. Consequently, we will not get a full  picture of the different services they would like the  system to provide.  A disadvantage with this method is, of course,  that post-processing takes some time compared to  using the \u001b[1;32;40m natural \u001b[0;0m dialogues as they are. There is al-  so a concern on the ecological validity of the results,  as discussed later.  3 Distilling dialogues  Distilling dialogues, i.e. re-writing human interac-  tions in order to have them reflect what a human-  computer interaction could look like involves a num-  ber of considerations. The main issue is that in cor-  pora of \u001b[1;32;40m natural \u001b[0;0m dialogues one of the interlocutors i not a dialogue system. The system's task is instead  performed by a human and the problem is how to  anticipate the behaviour of a system that does not  exist based on the performance of an agent with dif-  ferent performance characteristics. One important  aspect is how to deal with human features that are  not part of what the system is supposed to be able   to handle, for instance if the user talks about things  outside of the domain, such as discussing an episode  of a recent TV show. It also involves issues on how  to handle situations where one of the interlocuters  discusses with someone lse on a different opic, e.g.  discussing the up-coming Friday party with a friend  in the middle of an information providing dialogue  with a customer.  It is important for the distilling process to have at  least an outline of the dialogue system that is under  development: Will it for instance have the capacity  to recognise users' goals, even if not explicitly stat-  ed? Will it be able to reason about the discourse  domain? What services will it provide, and what  will be outside its capacity to handle?  In our case, we assume that the planned dialogue  system has the ability to reason on various aspects  of dialogue and properties of the application. In our  current work, and in the examples used for illustra-  tion in this paper, we assume a dialogue model that  can handle any relevant dialogue phenomenon and  also an interpreter and speech recogniser being able  to understand any user input that is relevant o the  task. There is is also a powerful domain reason-  ing module allowing for more or less any knowledge  reasoning on issues that can be accomplished with-  in the domain (Flycht-Eriksson, 1999). Our current  system does, however, not have an explicit user task  model, as opposed to a system task model (Dahlb~ick  45  and JSnsson, 1999), which is included, and thus, we  can not assume that the 'system' remembers utter-  ances where the user explains its task. Furthermore,  as our aim is system development we will not con-  sider interaction outside the systems capabilities as  relevant o include in the distilled dialogues.  The context of our work is the development a  multi-modal dialogue system. However, in our cur-  rent work with distilling dialogues, the abilities of  a multi-modal system were not fully accounted for.  The reason for this is that the dialogues would be  significantly affected, e.g. a telephone conversation  where the user always likes to have the next con-  nection, please will result in a table if multi-modal  output is possible and hence a fair amount of the di-  alogne is removed. We have therefore in this paper  analysed the corpus assuming a speech-only system,  since this is closer to the original telephone conversa-  tions, and hence needs fewer assumptions on system  performance when distilling the dialogues.  4 Dis t i l l a t ion  gu ide l ines   Distilling dialogues requires guidelines for how to  handle various types of utterances. In this section  we will present our guidelines for distilling a corpus  of telephone conversations between a human infor-  mation provider on local buses 1to be used for devel-  oping a multimodal dialogue system (Qvarfordt and  JSnsson, 1998; Flycht-Eriksson and JSnsson, 1998;  Dahlb~ick et al., 1999; Qvarfordt, 1998). Similar  guidelines are used within another project on devel-  oping Swedish Dialogue Systems where the domain  is travel bureau information.  We can distinguish three types of contributors:  'System' (i.e. a future systems) utterances, User ut-  terances, and other types, such as moves by other  speakers, and noise.  4.1 Modifying system utterances  The problem of modifying 'system' utterances can  be divided into two parts: how to change and when  to change. They are in some respects intertwined,  but as the how-part affects the when-part more we  will take this as a starting point.   The 'system' provides as much relevant infor-  mation as possible at once. This depends on  the capabilities of the systems output modal-  ities. If we have a screen or similar output  device we present as much as possible which  normally is all relevant information. If we, on  the other hand, only have spoken output the  amount of information that the hearer can inter-  pret in one utterance must be considered when  1The bus time table dialogues are collected at  LinkSping University and are available (in Swedish) on  http://www.ida.l iu.se/~arnjo/kfb/dialoger.html  distilling. The system might in such cases pro-  vide less information. The principle of provid-  ing all relevant information is based on the as-  sumption that a computer system often has ac-  cess to all relevant information when querying  the background system and can also present it  more conveniently, especially in a multimodal  system (Ahrenberg et al., 1996). A typical ex-  ample is the dialogue fragment in figure 1. In  this fragment he system provides information  on what train to take and how to change to a  bus. The result of distilling this fragment pro-  vides the revised fragment of figure 2. As seen in  the fragment of figure 2 we also remove a num-  ber of utterances typical for human interaction,  as discussed below.  * System utterances are made more computer-l ike  and do not include irrelevant information. The  latter is seen in $9 in the dialogue in figure 3  where the provided information is not relevant.  It could also be possible to remove $5 and re-  spond with $7 at once. This, however, depends  on if the information grounded in $5-U6 is need-  ed for the 'system' in order to know the arrival  time or if that could be concluded from U4.  This in turn depends on the system's capabili-  ties. If we assume that the dialogue system has  a model of user tasks, the information in $5-U6  could have been concluded from that. We will,  in this case, retain $5-U6 as we do not assume a user task model (Dahlb/ick and JSnsson, 1999)  and in order to stay as close to the original di-  alogue as possible.  The next problem concerns the case when 'system'  utterances are changed or removed.   Dialogue contributions provided by something or  someone other than the user or the 'system' are  removed. These are regarded as not being part  of the interaction. This means that if some-  one interrupts the current interaction, say that  the telephone rings during a face-to-face inter-  action, the interrupting interaction is normally  removed from the corpus.  Furthermore, 'system' interruptions are re-  moved. A human can very well interrupt anoth-  er human interlocuter, but a computer system  will not do that.  However, this guideline could lead to problems,  for instance, when users follow up such interrup-  tions. If no information is provided or the in-  terrupted sequence does not affect the dialogue,  we have no problems removing the interruption.  The problem is what to do when information  from the 'system' is used in the continuing dia-  logue. For such cases we have no fixed strategy,  46  U4:  $5:  U6:  $7:  U8:  $9:  U10:  $11:  U12:  S13:  U14:  $15:  yes I wonder if you have any mm buses or (.) like express buses leaving from LinkSping  to Vadstena (.) on sunday  ja ville undra om ni hade ndgra 5h bussar eUer (.) typ expressbussar sore dkte frdn LinkSping  till Vadstena (.) pd sSnda  no the bus does not run on sundays  nej bussen g~r inte pd sSndagar  how can you (.) can you take the train and then change some way (.) because (.)  to MjSlby 'n' so  hur kan man (.) kan man ta tdg d sen byta p~ ndtt sStt (.) fSr de (.)  till mjSlby ~ sd  that you can do too yes  de kan du gSra ocksd ja  how (.) do you have any such suggestions  hut (.) har du n~ra n~gra s~na fSrslag  yes let's see (4s) a moment (15s) now let us see here (.) was it on the sunday you should travel  ja ska se h~ir (4s) eft 5gonblick (15s) nu ska vise hSr (.) va de p~ sSndagen du skulle dka pd  yes right afternoon preferably  ja just de eftermidda ggirna  afternoon preferable (.) you have train from LinkSping fourteen twenty nine  eftermidda gSrna (.) du hat t~g frdn LinkSping fjorton d tjugonie  mm  mm  and then you will change from MjSlby station six hundred sixty  sd byter du frdn MjSlby station sexhundrasexti  sixhundred sixty  sexhundrasexti  fifteen and ten  femton ~ tie  Figure 1: Dialogue fragment from a real interaction on bus time-table information  U4: I wonder if you have any buses or (.) like express buses going from LinkSping  to Vadstena (.) on sunday  S5: no the bus does not run on sundays  U6: how can you (.) can you take the train and then change some way (.) because (.)  to MjSlby and so  $7: you can take the train from LinkSping fourteen and twenty nine and then you will  change at MjSlby station to bus six hundred sixty at fifteen and ten  Figure 2: A distilled version of the dialogue in figure 1  the dialogue needs to be rearranged epending  on how the information is to be used (c.f. the  discussion in the final section of this paper).   'System' utterances which are no longer valid  are removed. Typical examples of this are the  utterances $7, $9, $11 and $13 in the dialogue  fragment of figure 1.  * Remove sequences of utterances where the 'sys-  tem' behaves in a way a computer would not do.  For instance jokes, irony, humor, commenting  on the other dialogue participant, or dropping  the telephone (or whatever is going on in $7  in figure 4). A common case of this is when  the 'system' is talking while looking for infor-  mation, $5 in the dialogue fragment of figure 4  is an example of this. Related to this is when  the system provides its own comments. If we  can assume that it has such capabilities they  are included, otherwise we remove them.  The system does not repeat information that has  already been provided unless explicitly asked to  do so. In human interaction it is not uncommon  to repeat what has been uttered for purposes  other than to provide grounding information or  feedback. This is for instance common during  47   U4: 'n' I must be at Resecentrum before fourteen and thirty five (.) 'cause we will going to the  interstate buses  ja ska va p~ rececentrum innan \\]jorton ~ trettifem (.) f5 vi ska till  l~ngf~irdsbussarna  $5: aha (.) 'n' then you must be there around twenty past two something then  jaha (.) ~ dd behhver du va here strax e~ter tjuge 5vet tvd n~nting d~  U6: yes around that  ja ungefgir  $7: let's see here ( l ls)  two hundred and fourteen Ryd end station leaves forty six (.) thirteen 'n'  forty six then you will be down fourteen oh seven (.)  d~ ska vise hSr (11s) tv~hundrafjorton Ryd 5ndh~llplatsen gdr ~5rtisex (.) tretton d  \\]Srtisex d~ dr du nere ~jorton noll sju 5)  U8: aha  jaha  $9: 'n' (.) the next one takes you there (.) fourteen thirty seven (.) but that is too late  (.) ndsta dr du nere 5) ~jorton d trettisju (.) men de 5 ju ~Sr sent  Figure 3: Dialogue fragment from a real interaction on bus time-table information  U2: Well, hi (.) I am going to Ugglegatan eighth  ja hej (.) ja ska till Ugglegatan dtta  $3: Yes  ja  U4: and (.) I wonder (.) it is somewhere in Tannefors  och (.) jag undrar (.) det ligger ndnstans i Tannefors  $5: Yes (.) I will see here one one I will look exactly where it is one moment please  ja (.) jag ska se hhr eft eft jag ska titta exakt vat det ligger eft 6gonblick barn  U6: Oh Yeah  jar~  $7: (operator disconnects) (25s) mm (.) okey (hs) what the hell (2s)  (operator connects again) hello yes  ((Telefonisten kopplar ur sig)) (25s) iihh (.) okey (hs) de va sore \\]aan (2s)  ((Telefonisten kopplar in sig igen)) halld ja  U8: Yes hello  ja hej  $9: It is bus two hundred ten which runs on old tannefors road that you have to take and get off at  the bus stop at that bus stop named vetegatan  det ~i buss tv~hundratio sore g~r gamla tanne~orsvSgen som du ~r  ~ka ~ g~ av rid  den hdllplatsen rid den hdllplatsen sore heter vetegatan.  Figure 4: Dialogue fragment from a \u001b[1;32;40m natural \u001b[0;0m bus timetable interaction  search procedures as discussed above.   The system does not ask for information it has  already achieved. For instance asking again if it  is on Sunday as in $9 in figure 1. This is not un-  common in human interaction and such utter-  ances from the user are not removed. However,  we can assume that the dialogue system does  not forget what has been talked about before.  4.2 Mod i fy ing  user  u t te rances   The general rule is to change user utterances as lit-  tle as possible. The reason for this is that we do not  want to develop systems where the user needs to  restrict his/her behaviour to the capabilities of the  dialogue system. However, there are certain changes  made to user utterances, in most cases as a conse-  quence of changes of system utterances.  Utterances that are no longer valid are removed.  The most common cases are utterances whose  request has already been answered, as seen in  the distilled dialogue in figure 2 of the dialogue  in figure 1.  48  Sl1: sixteen fifty five  sexton \\]emti/em  U12: sixteen fifty five (.) aha  sexton femti/em (.) jaha  S13: bus line four hundred thirty five  linje \\]yrahundra tretti/em  Figure 5: Dialogue fragment from a \u001b[1;32;40m natural \u001b[0;0m bus  timetable interaction   Utterances are removed where the user discuss-  es things that are in the environment. For  instance commenting the 'systems' clothes or  hair. This also includes other types of commu-  nicative signals such as laughter based on things  outside the interaction, for instance, in the en-  vironment of the interlocuters.   User utterances can also be added in order to  make the dialogue continue. In the dialogue in  figure 5 there is nothing in the dialogue xplain-  ing why the system utters S13. In such cases  we need to add a user utterance, e.g. Which  bus is that?. However, it might turn out that  there are cues, such as intonation, found when  listening to the tapes. If  such detailed analyses  are carried out, we will, of course, not need to  add utterances. Furthermore, it is sometimes  the case that the telephone operator deliberate-  ly splits the information into chunks that can  be comprehended by the user, which then must  be considered in the distillation.  5 App ly ing  the  method  To illustrate the method we will in this section try to  characterise the results from our distillations. The  illustration is based on 39 distilled dialogues from  the previously mentioned corpus collected with a  telephone operator having information on local bus  time-tables and persons calling the information ser-  vice.  The distillation took about three hours for all 39  dialogues, i.e. it is reasonably fast. The distilled  dialogues are on the average 27% shorter. However,  this varies between the dialogues, at most 73% was  removed but there were also seven dialogues that  were not changed at all.  At the most 34 utterances where removed from  one single dialogue and that was from a dialogue  with discussions on where to find a parking lot, i.e.  discussions outside the capabilities of the applica-  tion. There was one more dialogue where more than  30 utterances were removed and that dialogue is a  typical example of dialogues where distillation actu-  ally is very useful and also indicates what is normal-  ly removed from the dialogues. This particular dia-  logue begins with the user asking for the telephone  number to 'the Lost property office' for a specific bus  operator. However, the operator starts a discussion  on what bus the traveller traveled on before provid-  ing the requested telephone number. The reason for  this discussion is probably that the operator knows  that different bus companies are utilised and would  like to make sure that the user really understands  his/her request. The interaction that follows can,  thus, in that respect be relevant, but for our pur-  pose of developing systems based on an overall goal  of providing information, not to understand human  interaction, our dialogue system will not able to han-  dle such phenomenon (JSnsson, 1996).  The dialogues can roughly be divided into five dif-  ferent categories based on the users task. The dis-  cussion in twenty five dialogues were on bus times  between various places, often one departure and one  arrival but five dialogues involved more places. In  five dialogues the discussion was one price and var-  ious types of discounts. Five users wanted to know  the telephone number to 'the Lost property office',  two discussed only bus stops and two discussed how  they could utilise their season ticket to travel out-  side the trafficking area of the bus company. It is  interesting to note that there is no correspondence  between the task being performed uring the inter-  action and the amount of changes made to the dia-   logue. Thus, if we can assume that the amount of  distillation indicates omething about a user's inter-  action style, other factors than the task are impor-  tant when characterising user behaviour.  Looking at what is altered we find that the most  important distilling principle is that the 'system'  provides all relevant information at once, c.f. fig-  ures 1 and 2. This in turn removes utterances pro-  vided by both 'system' and user.  Most added utterances, both from the user and  the 'system', provide explicit requests for informa-  tion that is later provided in the dialogue, e.g. ut-  terance $3 in figure 6. We have added ten utterances  in all 39 dialogues, five 'system' utterances and five  user utterances. Note, however, that we utilised the  transcribed ialogues, without information on into-  nation. We would probably not have needed to add  this many utterances if we had utilised the tapes.  Our reason for not using information on intonation  is that we do not assume that our system's peech  recogniser can recognise intonation.  Finally, as discussed above, we did not utilise the  full potential of multi-modality when distilling the  dialogues. For instance, some dialogues could be  further distilled if we had assumed that the system  had presented a time-table. One reason for this is  that we wanted to capture as many interesting as-  pects intact as possible. The advantage is, thus, that  we have a better corpus for understanding human-  49  U2: Yees hi Anna Nilsson is my name and I would like to take the bus from Ryd center to Resecentrum  in LinkSping  jaa hej Anna Nilsson heter jag och jag rill ~ka buss ~r~n Ryds centrum till resecentrum  i LinkSping.  $3: mm When do you  want  to  leave?  mm N~ir r i l l  du  ka?  U4: 'n' I must be at Resecentrum before fourteen and thirty five (.) 'cause we will going to the  interstate buses  ja ska va p~ rececentrum innan fjorton d trettifem (.) f5 vi ska till  l~ngfiirdsbussarna  Figure 6: Distilled dialogue fragment with added utterance  computer interaction and can from that corpus do  a second distillation where we focus more on multi-  modal interaction.  6 Discuss ion  We have been presenting a method for distilling hu-  man dialogues to make them resemble human com-  puter interaction, in order to utilise such dialogues  as a knowledge source when developing dialogue sys-  tems. Our own main purpose has been to use them  for developing multimodal systems, however, as dis-  cussed above, we have in this paper rather assumed  a speech-only system. But we believe that the basic  approach can be used also for multi-modal systems  and other kinds of \u001b[1;32;40m natural \u001b[0;0m \u001b[1;32;40m language \u001b[0;0m dialogue sys-  tems.  It is important o be aware of the limitations of  the method, and how 'realistic' the produced result  will be, compared to a dialogue with the final sys-  tem. Since we are changing the dialogue moves, by  for instance providing all required information in one  move, or never asking to be reminded of what the us-  er has previously requested, it is obvious that what  follows after the changed sequence would probably  be affected one way or another. A consequence of  this is that the resulting dialogue is less accurate as  a model of the entire dialogue. It is therefore not an  ideal candidate for trying out the systems over-all  performance during system development. But for  the smaller sub-segments or sub-dialogues, we be-  lieve that it creates a good approximation of what  will take place once the system is up and running.  Furthermore, we believe distilled dialogues in some  respects to be more realistic than Wizard of Oz-  dialogues collected with a wizard acting as a com-  puter.  Another issue, that has been discussed previously  in the description of the method, is that the distilling  is made based on a particular view of what a dialogue  with a computer will look like. While not necessari-  ly being a detailed and specific model, it is at least  an instance of a class of computer dialogue models.  One example of this is whether the system is meant  to acquire information on the user's underlying mo-  tivations or goals or not. In the examples presented,  we have not assumed such capabilities, but this as-  sumption is not an absolute necessity. We believe,  however, that the distilling process should be based  on one such model, not the least to ensure a con-  sistent treatment of similar recurring phenomena t  different places in the corpora.  The validity of the results based on analysing dis-  tilled dialogues depends part ly on how the distilla-  tion has been carried out. Even when using \u001b[1;32;40m natural \u001b[0;0m  dialogues we can have situations where the interac-  tion is somewhat mysterious, for instance, if some of  the dialogue participants behaves irrational such as  not providing feedback or being too elliptical. How-  ever, if careful considerations have been made to stay  as close to the original dialogues as possible, we be-  lieve that distilled dialogues will reflect what a hu-  man would consider to be a \u001b[1;32;40m natural \u001b[0;0m interaction.  Acknowledgments   This work results from a number of projects on de-  velopment of \u001b[1;32;40m natural \u001b[0;0m \u001b[1;32;40m language \u001b[0;0m interfaces upported  by The Swedish Transport & Communications Re-  search Board (KFB) and the joint Research Program  for Language Technology (HSFR/NUTEK) .  We are  indebted to the participants of the Swedish Dialogue  Systems project, especially to Staffan Larsson, Lena  Santamarta, and Annika Flycht-Eriksson for inter-  esting discussions on this topic.  Re ferences   Lars Ahrenberg, Nils Dahlb~ck, Arne JSnsson,  and /~ke Thur~e. 1996. Customizing interac-  tion for \u001b[1;32;40m natural \u001b[0;0m \u001b[1;32;40m language \u001b[0;0m interfaces. LinkSpin9  Electronic articles in Computer and Informa-  tion Science, also in Notes from Workshop on  Pragmatics in Dialogue, The XIV:th Scandi-  navian Conference of Linguistics and the VI-  II:th Conference of Nordic and General Linguis-  50  tics, GSteborg, Sweden, 1993, 1(1), October, 1.  http :/ / www.ep.liu.se / ea /cis /1996 / O01/.  Sue Atkins, Jeremy Clear, and Nicholas Ostler.  1992. Corpus design criteria. Literary and Lin-  guistic Computing, 7(1):1-16.  Douglas Biber. 1993. Representativeness in cor-  pus design. Literary and Linguistic Computing,  8(4):244-257.  Jean Carletta. 1996. Assessing agreement on classi-  fication tasks: The kappa statistic. Computation-  al Linguistics, 22(2):249-254.  Steve Crowdy. 1993. Spoken corpus design. Literary  and Linguistic Computing, 8(4):259-265.  Nils Dahlb/ick and Arne JSnsson. 1999. Knowledge  sources in spoken dialogue systems. In Proceed-  ings of Eurospeech'99, Budapest, Hungary.  Nils Dahlb/ick, Arne JSnsson, and Lars Ahrenberg.  1998. Wizard of oz studies - why and how.  In Mark Maybury & Wolfgang Wahlster, editor,  Readings in Intelligent User Interfaces. Morgan  Kaufmann.  Ntis Dahlb/ick, Annika Flycht-Eriksson, Arne  JSnsson, and Pernilla Qvarfordt. 1999. An ar-  chitecture for multi-modal \u001b[1;32;40m natural \u001b[0;0m dialogue sys-  tems. In Proceedings of ESCA Tutorial and Re-  search Workshop (ETRW) on Interactive Dialogue  in Multi-Modal Systems, Germany.  Nils Dahlb/ick. 1991. Representations ofDiscourse,  Cognitive and Computational Aspects. Ph.D. the-  sis, LinkSping University.  Maxine Eskenazi, Alexander Rudnicki, Karin Grego-  ry, Paul Constantinides, Robert Brennan, Christi-  na Bennett, and Jwan Allen. 1999. Data collec-  tion and processing in the carnegie mellon com-  municator. In Proceedings of Eurospeech'99, Bu-  dapest, Hungary.  Annika Flycht-Eriksson and Arne JSnsson. 1998. A  spoken dialogue system utilizing spatial informa-  tion. In Proceedings of ICSLP'98, Sydney, Aus-  tralia.  Annika Flycht-Eriksson. 1999. A survey of knowl-  edge sources in dialogue systems. In Proceedings  of lJCAI-99 Workshop on Knowledge and Reason-  ing in Practical Dialogue Systems, August, Stock-  holm.  Roger Garside, Geoffrey Leech, and Anthony  MeEnery. 1997. Corpus Annotation. Longman.  Arne JSnsson and Nils Dahlb/ick. 1988. Talking to a  computer is not like talking to your best friend. In  Proceedings of the First Scandinavian Conference  on Artificial InterUigence, Tvoms.  Arne JSnsson. 1996. Natural \u001b[1;32;40m language \u001b[0;0m generation  without intentions. In Proceedings of ECAI'96  Workshop Gaps and Bridges: New Directions  in Planning and Natural Language Generation,  pages 102-104.  Pernilla Qvarfordt and Arne JSnsson. 1998. Effects  of using speech in timetable information systems  for www. In Proceedings of ICSLP'98, Sydney,  Australia.  Pernilla Qvarfordt. 1998. Usability of multimodal  timetables: Effects of different levels of do-  main knowledge on usability. Master's thesis,  LinkSping University.  Karen Sparck Jones and Julia R. Galliers. 1996.  Evaluating Natural Language Processing Systems.  Springer Verlag.  Marilyn A. Walker, Diane J. Litman, Candace A.  Kamm, and Alicia Abella. 1998. Paradise: A  framework for evaluating spoken dialogue agents.  In Mark Maybury & Wolfgang Wahlster, editor,  Readings in Intelligent User Interfaces. Morgan  Kaufmann.  51  \n",
            "-----------------------------\n",
            "--- document 16: --- document 16: PartslD: A Dialogue-Based System for Identifying Parts for Medical  Systems  Amit BAGGA, Tomek STRZALKOWSKI, and G. Bowden WISE  Information Technology Laboratory  GE Corporate Research and Development  1 Research Circle  Niskayuna, USA, NY 12309  { bagga, strzalkowski, wisegb } @crd.ge.com  Abstract  This paper describes a system that  provides customer service by allowing  users to retrieve identification umbers of  parts for medical systems using spoken  \u001b[1;32;40m natural \u001b[0;0m \u001b[1;32;40m language \u001b[0;0m dialogue. The paper also  presents an evaluation of the system  which shows that the system successfully  retrieves the identification numbers of  approximately 80% of the parts.  Introduction  Currently people deal with customer service  centers either over the phone or on the world  wide web on a regular basis. These service  centers upport a wide variety of tasks including  checking the balance of a bank or a credit card  account, transferring money from one account o  another, buying airline tickets, and filing one's  income tax returns. Most of these customer  service centers use interactive voice response  (IVR) systems on the front-end for determining  the user's need by providing a list of options that  the user can choose from, and then routing the  call appropriately. The IVRs also gather  essential information like the user's bank  account number, social security number, etc.  For back-end support, the customer service  centers use either specialized computer systems  (example: a system that retrieves the account  balance from a database), or, as in most cases,  human operators.  However, the IVR systems are unwieldy  to use. Often a user's needs are not covered by  the options provided by the system forcing the  user to hit 0 to transfer to a human operator. In  addition, frequent users often memorize the  sequence of options that will get them the  desired information. Therefore, any change in  the options greatly inconveniences these users.  Moreover, there are users that always hit 0 to  speak to a live operator because they prefer to  deal with a human instead of a machine.  Finally, as customer service providers continue  to rapidly add functionality to their IVR  systems, the size and complexity of these  systems continues to grow proportionally. In  some popular systems like the IVR system that  provides customer service for the Internal  Revenue Service (IRS), the user is initially  bombarded with 10 different options with each  option leading to sub-menus offering a further 3-  5 options, and so on. The total number of nodes  in the tree corresponding to the IRS' IVR system  is quite large (approximately 100) making it  extremely complex to use.  Some customer service providers have  started to take advantage of the recent advances  in speech recognition technology. Therefore,  some of the IVR systems now allow users to say  the option number (1, 2, 3 . . . . .  etc.) instead of  pressing the corresponding button. In addition,  some providers have taken this a step further by  allowing users to say a keyword or a phrase  from a list of keywords and/or phrases. For  example, AT&T, the long distance company,  provides their users the following options:  \"Please say information for information on  placing a call, credit for requesting credit, or  operator to speak to an operator.\"  However, given the improved speech  recognition technology, and the research done in  \u001b[1;32;40m natural \u001b[0;0m anguage dialogue over the last decade,  there exists tremendous potential in enhancing  29  these customer service centers by allowing users  to conduct a more \u001b[1;32;40m natural \u001b[0;0m human-like dialogue  with an automated system to provide a  customer-friendly s stem. In this paper we  describe a system that uses \u001b[1;32;40m natural \u001b[0;0m \u001b[1;32;40m language \u001b[0;0m  dialogue to provide customer service for a  medical domain. The system allows field  engineers to call and obtain identification  numbers of parts for medical systems using  \u001b[1;32;40m natural \u001b[0;0m \u001b[1;32;40m language \u001b[0;0m dialogue. We first describe  some work done previously in using \u001b[1;32;40m natural \u001b[0;0m  \u001b[1;32;40m language \u001b[0;0m dialogue for customer service  applications. Next, we present he architecture  of our system along with a description of each of  the key components. Finally, we conclude by  providing results from an evaluation of the  system.  1. Previous Work  As mentioned earlier, some customer service  centers now allow users to say either the option  number or a keyword from a list of  options/descriptions. However, the only known  work which automates part of a customer service  center using \u001b[1;32;40m natural \u001b[0;0m \u001b[1;32;40m language \u001b[0;0m dialogue is the one  by Chu-Carroll and Carpenter (1999). The  system described here is used as the front-end of  a bank's customer service center. It routes calls  by extracting key phrases from a user utterance  and then by statistically comparing these phrases  to phrases extracted from utterances in a training  corpus consisting of pre-recorded calls where  the routing was done by a human. The call is  routed to the destination of the utterance from  the training corpus that is most \"similar\" to the  current utterance. On occasion, the system will  interact with the user to clarify the user's request  by asking a question. For example, if the user  wishes to reach the loan department, the system  will ask if the loan is for an automobile, or a  home. Other related work is (Georgila et al.,  1998).  While we are aware of the work being  done by speech recognition companies like  Nuance (www.nuance.com) and Speechworks  (www.speechworks.com) in the area of  providing more \u001b[1;32;40m natural \u001b[0;0m anguage dialogue-based  customer service, we are not aware of any  conference or journal publications from them.  Some magazine articles which mention their  work are (Rosen 1999; Rossheim 1999;  Greenemeier 1999 ; Meisel 1999). In addition,  when we tried out a demo of Nuance's ystems,  we found that their systems had a very IVRish  feel to them. For example, if one wanted to  transfer $50 from one account o another, the  system would first ask the account that the  money was coming from, then the account hat  the money was going to, and finally, the amount  to be transferred. Therefore, a user could not  say \"I want to transfer $50 from my savings  account o my checking account\" and have the  system conduct that transaction.  In addition to the works mentioned above,  there have been several classic projects in the  area of \u001b[1;32;40m natural \u001b[0;0m \u001b[1;32;40m language \u001b[0;0m dialogue like  TRAINS/TRIPS project at Rochester (Allen et  al., 1989, 1995, 1996), Duke's Circuit-Fixit-  Shoppe and Pascal Tutoring System (Biermann  et al., 1997; 1995), etc. While the Circuit-Fixit-  Shoppe system helps users fix a circuit through a dialogue with the system, the TRIPS and the  TRAINS projects allow users to plan their  itineraries through dialogue. Duke's Pascal  tutoring system helps students in an introductory  programming class debug their programs by  allowing them to analyze their syntax errors, get  additional information on the error, and learn the  correct syntax. Although these systems have  been quite successful, they use detailed models  of the domain and therefore cannot be used for  diverse applications uch as the ones required  for customer service centers. Other related work  on dialogue include (Carberry, 1990; Grosz and  Sidner, 1986; Reichman, 1981).  2. PartslD: A System for Identification  of Parts for Medical Systems  Initially, we were approached by the medical  systems business of our company for help in  reducing the number of calls handled by human  operators at their call center. An analysis of the  types of customer service provided by their call  center showed that a large volume of calls  handled by their operators were placed by field  engineers requesting identification umbers of  parts for various medical systems. The ID  numbers were most often used for ordering the  corresponding parts using an automated IVR  system. Therefore, the system we have built  30  Figure 1. PartslD System Architecture  W  I Parser l  ~ User  Dia logue  Manager   F . , .   pros entetion  helps automate some percentage of these calls  by allowing the engineer to describe a part using  \u001b[1;32;40m natural \u001b[0;0m \u001b[1;32;40m language \u001b[0;0m. The rest of this section  describes our system in detail.  2.1 Data  The database we used for our system was the  same as the one used by the operators at the call  center. This database consists of the most  common parts and was built by the operators  themselves. However, the data contained in the  database is not clean and there are several types  of errors including mis-spellings, use of non-  standard abbreviations, use of several different  abbreviations for the same word, etc.  The database consists of approximately  7000 different parts. For each part, the database  contains its identification umber, a description,  and the product (machine type) that it is used in.  The descriptions consist of approximately  60,000 unique words of which approximately  3,000 are words which either are non-standard  abbreviations or are unique to the medical  domain (example: collimator).  Due to the large size of the database, we  did not attempt to clean the data. However, we  did build several data structures based on the  database which were used by the system. The  primary data structures built were two inverted  hash tables corresponding to the product, and the  part description fields in the database. The  inverted hash tables were built as follows:  1) Each product and part description field  was split into words.  2) Stop-words (words containing no  information like: a, the, an, etc.) were  filtered.  3) Each remaining word was inserted as the  index of the appropriate hash table with  the identification number of the part  being the value corresponding to the  index.  Therefore, for each non-stop-word word used in  describing a part, the hash table contains a list of  all the parts whose descriptions contained that  word. Similarly, the products hash table  contains a list of all parts corresponding to each  product word.  2.2 System Architecture  The architecture of the system is shown in  Figure 1. The system was designed in a manner  such that it could be easily ported from one  application to another with minimal effort other  than providing the domain-specific knowledge  regarding the new application. Therefore, we  decided to abstract away the domain-specific  information into self-contained modules while  keeping the other modules completely  independent. The domain-specific modules are  shown in the dark shaded boxes in Figure I.  The remainder of this section discusses each of  the modules hown in the system architecture.  2.2.1 The Speech Recognition System (ASR)  Since customer service centers are meant o be  used by a variety of users, we needed a user-  independent speech recognition system. In  31  addition, since the system could not restrict he  manner in which a user asked for service, the  speech recognition system could not be  grammar-based. Therefore, we used a general  purpose dictation engine for the system. The  dictation system used was Lernout & Hauspie's  VoiceXPress ystem (www.lhs.com). Although  the system was general purpose, we did provide  to it the set of keywords and phrases that are  commonly used in the domain thereby enabling  it to better recognize these domain-specific  keywords and phrases. The keywords and  phrases used were simply the list of descriptions  and product names corresponding to each part in  the database. It should be noted that the set of  domain-specific keywords and phrases was  provided to the speech recognition system as a  text document. In other words, the training was  not done by a human speaking the keywords and  phrases into the speech recognition system. In  addition, the speech recognition system is far  from perfect. The recognition rates hover  around 50%, and the system has additional  difficulty in identifying product names which  are most often words not found in a dictionary  (examples: 3MlaserCam, 8000BUCKY, etc.).  2.2.2 Parser and the Lexicon  The parser is domain-driven i the sense that it  uses domain-dependent information produced by  the lexicon to look for information, in a user  utterance, that is useful in the current domain.  However, it does not attempt to understand fully  each user utterance. It is robust enough to  handle ungrammatical sentences, hort phrases,  and sentences that contain mis-recognized text.  The lexicon, in addition to providing  domain-dependent keywords and phrases to the  parser, also provides the semantic knowledge  associated with each keyword and phrase.  Therefore, for each content word in the inverted  hash tables, the lexicon contains entries which  help the system determine whether the word was  used in a part description, or a product name. In  addition, the lexicon also provides the semantic  knowledge associated with the pre-specified  actions which can be taken by the user like  \"operator\" which allows the user to transfer to  an operator, and \"stop,\" or \"quit\" which allow  the user to quit the system. Some sample ntries  are:  collimator => (description_word, collimator)  camera => (product_word, camera)  operator => (user action, operator)  etc.  The parser scans a user utterance and  returns, as output, a list of semantic tuples  associated with each keyword/phrase contained  in the utterance. It is mainly interested in \"key  words\" (words that are contained in product and  part descriptions, user action words, etc.) and it  ignores all the other words in the user utterance.  The parser also returns a special tuple containing  the entire input string which may be used later  by the context-based parser for sub-string  matching specially in cases when the DM has  asked a specific question to the user and is  expecting a particular kind of response.  2.2.3 The Filler and Template Modules  The filler takes as input the set of tuples  generated by the parser and attempts to check  off templates contained in the templates module  using these tuples, The set of templates in the  templates module contains most of remaining  domain-specific knowledge required by the  system. Each template is an internal  representation of a part in the database. It  contains for each part, its ID, its description, and  the product which contains it. In addition, there  are several additional templates corresponding to pre-specified user actions like \"operator,\" and  \"quit.\" A sample template follows:  tl__I = (  'product' = > 'SFD',  'product__ids' = > 2229005\"  'product_descriptions' => 'IR RECEIVER PC  BOARD CI104 BISTABLE MEMORY')  For each tuple input from the parser, the  filler checks off the fields which correspond to  the tuple. For example, if the filler gets as input  (description_word, collimator), it checks off the  description fields of those templates containing  collimator as a word in the field. A template is  checked off iff one or more of its fields is  checked off. In addition, the filler also  maintains a list of all description and product  words passed through the tuples (i.e. these words  32 have been uttered by the user). These two lists  are subsequently passed to the dialogue  manager.  Although the filler does not appear to be  very helpful for the current application domain,  it is an important part of the architecture for  other application domains. For example, the  current PartslD system is a descendant from an  earlier system which allowed users to process  financial transactions where the filler was  instrumental in helping the dialogue manager  determine the type of transaction being carried  out by the user (Bagga et al., 2000).  2.2.4 The Dialogue Manager (DM)  The DM receives as input from the filler the set  of templates which are checked off. In addition,  it also receives two lists containing the list of  description words, and product word uttered by  the user. The DM proceeds using the following  algorithm:  1) It first checks the set of checked off  templates input from the filler. If there is  exactly one template in this set, the DM asks  the user to confirm the part that the template  corresponds to. Upon receipt of the  confirmation from the user, it returns the  identification number of the part to the user.  2) Otherwise, for each description word uttered  by the user, the DM looks up the set of parts  (or templates) containing the word from the  descriptions inverted hash table. It then  computes the intersection of these sets. If  the intersection is empty, the DM computes  the union of these sets and proceeds treating  the union as the intersection.  3) If the intersection obtained from (2) above  contains exactly one template, the DM asks  the user to confirm the part corresponding to the template as in (1) above.  4) Otherwise, the DM looks at the set of  product words uttered by the user. If this set  is empty, the DM queries the user for the  product name. Since the DM is expecting a product name here, the input provided by the  user is handled by the context-based parser.  Since most product names consist of non-  standard words consisting of alpha-numeric  characters (examples: AMX3,  8000BUCKY, etc.), the recognition quality  is quite poor. Therefore, the context-based  parser anks the input received from the user  using a sub-string matching algorithm that  uses character-based unigram and bigram  counts (details are provided in the next  section). The sub-string matching algorithm  greatly enhances the performance of the  system (as shown in the sample dialogue  below).  5) If the set of product words is non-empty, or  if the DM has successfully queried the user  for a product name, it extracts the set of  parts (templates) containing each product  word from the product words inverted hash  table. It then computes an intersection of  these sets with the intersection set of  description words obtained from (2) above.  The resulting intersection is the joint product  and description i tersection.  6) If the joint intersection has exactly one  template, the DM proceeds as in (1) above.  Alternatively, if the number of templates in  the joint intersection is less than 4, the DM  lists the parts corresponding toeach of these  and asks the user to confirm the correct one.  7) If there are more than 4 templates in the  joint intersection, the DM ranks the  templates based upon word overlap with the  description words uttered by the user. If the  number of resulting top-ranked templates i less than 4, the DM proceeds as in the  second half of (6) above.  8) If the joint intersection is empty, or in the  highly unlikely case of there being more  than 4 top-ranked templates in (7), the DM  asks the user to enter additional  disambiguating information.  The goal of the DM is to hone in on the part  (template) desired by the user, and it has to  determine this from the set of templates input to  it by the filler. It has to be robust enough to deal  with poor recognition quality, inadequate  information input by the user, and ambiguous  data. Therefore, the DM is designed to handle  these issues. For example, description words  that are mis-recognized as other description  words usually cause the intersection of the sets  of parts corresponding to these words to be  empty. The DM, in this case, takes a union of  the sets of parts corresponding to the description  333333 words thereby ensuring that the template  corresponding tothe desired part is in the union.  The DM navigates the space of possibilities  by first analyzing the intersection of the sets of  parts corresponding to the description words  uttered by the user. If no unique part emerges,  the DM then checks to see if the user has  provided any information about the product hat  the part is going to be used in. If no product was  mentioned by the user, the DM queries the user  for the product name. Once this is obtained, the  DM then checks to see if a unique part  corresponds to the product name and the part  description provided by the user. If no unique  part emerges, then the DM backs off and asks  the user to re-enter the part description.  Alternatively, if more than one part corresponds  to the specified product and part description,  then the DM ranks the parts based upon the  number of words uttered by the user.  Obviously, since the DM in this case uses a  heuristic, it asks the user to confirm the part that  ranks the highest. If more than one (although  less than 4) parts have the same rank, then the  DM explicitly lists these parts and asks the user  to specify the desired part. It should be noted  that the DM has to ensure that the information it receives is actually what the user meant. This is  especially true when the DM uses heuristics, and  sub-string matches (as in the case of product  names). Therefore, the DM occasionally asks  the user to confirm input it has received.  2.2.5 The Sub-String Matching Algorithm  When the dialogue manager is expecting a  certain type of input (examples : product names,  yes/no responses) from the user, the user  response is processed by the context-based  parser. Since the type of input is known, the  context-based parser uses a sub-string matching  algorithm that uses character-based unigram and  bigram counts to match the user input with the  expectation of the dialogue manager. Therefore,  the sub-string matching module takes as input a  user utterance string along with a list of  expected responses, and it ranks the list of  expected responses based upon the user  response. Listed below are the details of the  algorithm :  1) The algorithm first concatenates the words  of the user utterance into one long string.  This is needed because the speech  recognition system often breaks up the  utterance into words even though a single  word is being said. For example, the  product name AMXl l0  is often broken up  into the string 'Amex 110'.  2) Next, the algorithm goes through the string  formed in (1) and compares this character by  character with the list of expected responses.  It assigns one point for every common  character. Therefore, the expected response  'AMX3' gets three points for the utterance  'Amex110'.  3) The algorithm then compares the user  utterance with the list of expected responses  using 2 characters (bigrams) at a time. It  assigns 2 points for each bigram match. For  the example shown in (2), there are two  bigram matches: the first is that the  utterance starts with an 'A' (the previous  character is this case is the null character),  and the second is the bigram 'AM'.  4) The algorithm now compares the length of  the user utterance string and the expected  response. If the length of the two strings is  the same, then it assigns 2 points to the  expected response.  5) Finally, the algorithm calculates the number  of unique characters in the expected  response, and the user utterance string. If  these characters are the same, then it assigns  4 points to the expected response.  The expected response which has the highest  number of points is the most likely one. If two  or more expected responses have the same  number of points, then the system asks the user  to confh'm the correct one.  While we have not evaluated this sub-  string matching algorithm independently, a brief  evaluation in the context of the system resulted  in about 90% accuracy.  2.2.6 The Presentation Module  The presentation module works in one of two  possible modes: over the phone, and over the  web. This module takes as input a string  generated by the question-generation module  and presents this string to the user in the  appropriate mode of communication. If the  speech option for the system is turned on, the  speech-based output is generated using Lernout  \"~ld. 34 and Hauspie's RealSpeak text-to-speech system.  Although the system currently cannot use both  modes of communication simultaneously, we  plan to incorporate this feature sometime in the  future.  2.2. 7 Robustness of  the System  As with any dialogue system, it is extremely  important for the system to be robust. Our  system has the following two features which  make it extremely robust:  1) The user can, at any given moment,  say operator to transfer to an  operator, quit~stop to exit, and  back~restart to start afresh.  2) When expecting a response from the  user, if the system does not receive an  expected input, it repeats the question  at most twice before transferring  control to an operator.  3. Sample Dialogues  This section gives examples of two sample  dialogues that occur during the testing of the  system. The system's response is indicated by  \"S>\", the user's response by \"U>\", and the  recognition of the user's response by \"V>\".  S> Welcome to the PartslD Center. Please tell  me the part that you want the ID for.  U> I need the ID for  PCB Control.  V> I need the I the for PCB home thrown.  S> What product do you need this for?  U> COMPAX 40E  V> home fax for a O0 E  S> Is COMPAX 40E the product name that you  said?  U> yes  V> gas  S> Do you want the ID for PCB Display SID or  the PCB Control?  U> The second one.  V> The second home.  S> Do you need the ID for PCB Control used in  product COMPAX 40E?  U> yes  V> yes  S> The ID for PCB Control used in product  COMPAX 40E is 2168983. Do you wan to exit  or f ind the ID for another part ?  U> look up another part.  S> Welcome to the PartslD Center. Please tell  me the part that you want the ID for.  U> I need cable data for cradle 2M long.  V> I needed cable date them for cradle to be  loan.  S> Do you need this part for a ANGIXM2004,  or a AFM ?  U> AFM  V> a fan  S> Is AFM the product name that you said?  U> yes  V> yes  S> Do you want the ID for Cable Data for  Cradle 2M long ?  U> yes  V> yes  S> The ID for  Cable Data for Cradle 2M long  used in product AFM is 45438628. Do you wan  to exit or f ind the ID for another part?  U> look up another part.  4. Evaluation and Results  The goal of our evaluation was to ensure that the  system helped a user successfully identify parts  irrespective of the performance of the speech  recognition engine for the user. In other words,  we wanted to see if the system was robust  enough to conduct transactions with a diverse  mix of users. We tested the system with 4  different users two of whom had foreign accents.  For each user, we randomly selected 20 parts  from the database. The results are summarized  in Table 1.  These results show that the system was  quite successful in handling requests from users  with a variety of accents achieving varying  recognition rates. Out of the 80 parts tested,  only twice did the user feel that he/she had to  transfer to an operator. The system successfully  retrieved the identification umbers of 79% of  the parts while transferring 19% of the cases to a  human operator because of extremely bad  :$5  User Parts  successfully  identified  15  Calls system  transfers to  operator  3  Calls user  transfers to  operator  2  System  prompts per  call  3.7  Relevant words  recognized per  part  2.5  18 2 0 3 2.35  13 7 0 2.5 1.65  17 3 0 2.9 2.7  Table 1: Summary of Results  recognition. We are planning on conducting a more elaborate test which a larger set of users.  Conclusions  In this paper we have described a robust system  that provides customer service for a medical  parts application. The preliminary results are  extremely encouraging with the system being  able to successfully process approximately 80%  of the requests from users with diverse accents.  Acknowledgements  We wish to thank the GE Medical Systems team  of Todd Reinke, Jim Tierney, and Lisa  Naughton for providing support and funding for  this project. In addition, we also wish to thank  Dong Hsu of Lernout and Hauspie for his help  on the ASR and the text-to-speech systems.  Finally, we wish to thank the Information  Technology Laboratory of GE CRD for  providing additional funding for this project.  References  Allen, J. F. et al. (1995) The TRAINS Project: A  case study in building a conversational p anning  agent. Journal of Experimental nd Theoretical AI,  (7) 7-48.  Allen, J. F., Miller, B. W.; Ringer, E. K.; and  Sikorski, T. (1996) A Robust System for Natural  Spoken Dialogue. 34th Annual Meeting of the  ACL, Santa Cruz, 62-70.  Bagga, A., Stein G. C., and Strzalkowski, T. (2000)  FidelityXPress: A Multi-Modal System for  Financial Transactions. Proceedings of the 6 a~  Conference on Content-Based Multimedia  Information Access (RIAO'00).  Biermann, A.W.; Rodman, R.; Rubin, D.; and  Heidlage, J.R. (1985) Natural \u001b[1;32;40m language \u001b[0;0m with  discrete speech as a mode for human to machine  communication. Communication of the ACM  18(6): 628-636.  Biermann, Alan W.; Guinn, Curry I.; Fulkerson, M.:  Keim, G.A.; Liang, Z.; Melamed, D.M.; and  Rajagopalan, K. (1997) Goal-orientedMultimedia  Dialogue with Variable Initiative. Lecture Notes in  Artificial Intelligence 1325; Springer-Verlag, New  York; pp. 1-16.  Carberry, S. (1990) Plan Recognition in Natural  Language Dialogue. Cambridge, Mass.: The MIT  Press.  Chu-Carroll, J, and R. Carpenter. (1999) Vector-  Based Natural Language Call Routing. Journal of  Computational Linguistics, 25(30), pp. 361-388.  Georgila, K., A.Tsopanoglou, N.Fakotakis and  G.Kokkinakis. (1998) An Integrated Dialogue  System for the Automation of Call Centre Services.  ICLSP'98, 5th International Conference on Spoken  Language Processing, Sydney, Australia.  Grosz, B.J. and Sidner, C.L. (1986) Attentions,  intentions, and the structure of discourse.  Computational Linguistics 12(3): 175-204.  Greenemeier, L. (1999) Voice-Recognition  Technology Builds a Following. Information  Week, December 13.  Meisel, W. (1999) Can Speech Recognition Give  Telephones a New Face? Business  Communications Review, November 1.  Reichman, R.. (1981) Plain-speaking: A theory and  grammar of spontaneous discourse. PhD thesis,  Department of Computer Science, Harvard  University, Cambridge, Massachusetts.  Rosen, C. (1999) Speech Has Industry Talking.  Business Travel News, November.  Rossheim, J. (1999) Giving Voice to Customer  Service. Datamation, November 1.  36  \n",
            "-----------------------------\n",
            "--- document 17: --- document 17: BusTUC - A natura l  l anguage bus  route  o rac le   Tore Amble  Dept. of computer and information science  University of Trondheim  Norway, N-7491  amble@idi, ntnu. no  Abstract  The paper describes a \u001b[1;32;40m natural \u001b[0;0m anguage based expert  system route advisor for the public bus transport  in Trondheim, Norway. The system is available on  the Internet,and has been intstalled at the bus com-  pany's web server since the beginning of 1999. The  system is bilingual, relying on an internal anguage  independent logic representation.  1 Introduct ion  A \u001b[1;32;40m natural \u001b[0;0m anguage interface to a computer database  provides users with the capability of obtaining in-  formation stored in the database by querying the  system in a \u001b[1;32;40m natural \u001b[0;0m \u001b[1;32;40m language \u001b[0;0m (NL). With a \u001b[1;32;40m natural \u001b[0;0m  \u001b[1;32;40m language \u001b[0;0m as a means of communication with a com-  puter system, the users can make a question or a  statement in the way they normally think about the  information being discussed, freeing them from hav-  ing to know how the computer stores or processes  the information.  The present implementation represents a a major  effort in bringing \u001b[1;32;40m natural \u001b[0;0m anguage into practical use.  A system is developed that can answer queries about  bus routes, stated as \u001b[1;32;40m natural \u001b[0;0m \u001b[1;32;40m language \u001b[0;0m texts, and  made public through the Internet World Wide Web  ( http : //www. idi. ntnu. no/bustuc/).  Trondheim is a small city with a university and  140000 inhabitants. Its central bus systems has 42  bus lines, serving 590 stations, with 1900 depar-  tures per day (in average). That gives approximately  60000 scheduled bus station passings per day, which  is somehow represented in the route data base.  The starting point is to automate the function of  a route information agent. The following example  of a system response is using an actual request over  telephone to the local route information company:  Hi, I live in Nidarvoll and tonight i  must reach a train to Oslo at 6 oclock.  and a typical answer would follow quickly:  Bus number 54 passes by Nidarvoll skole  at 1710 and arrives at Trondheim Railway  Station at 1725.  In between the question and the answer is a pro-  cess of lexical analysis, syntax analysis, semantic  analysis, pragmatic reasoning and database query  processing.  One could argue that the information content  could be solved by an interrogation, whereby the  customer is asked to produce 4 items: s ta t ion   of departure, station of arrival, earliest  departure timeand/or latest arrival time. It  is a myth that \u001b[1;32;40m natural \u001b[0;0m \u001b[1;32;40m language \u001b[0;0m is a better way of  communication because it is \"\u001b[1;32;40m natural \u001b[0;0m \u001b[1;32;40m language \u001b[0;0m\".  The challenge is to prove by demonstration that  an NL system can be made that will be preferred  to the interrogative mode. To do that, the system  has to be correct, user friendly and almost complete  within the actual domain.  2 Previous Efforts, CHAT-80,  PRAT-89 and HSQL  The system, called BusTUC is built upon the clas-  sical system CHAT-80 (Warren and Pereira, 1982).  CHAT-80 was a state of the art \u001b[1;32;40m natural \u001b[0;0m anguage sys-  tem that was impressive on its own merits, but also  established Prolog as a viable and competitive lan-  guage for Artificial Intelligence in general. The sys-  tem was a brilliant masterpiece of software, efficient  and sophisticated. The \u001b[1;32;40m natural \u001b[0;0m anguage system was  connected to a small query system for international  geography. The following query could be analysed  and answered in a split second:  Which country bordering the Mediterranean  borders a country that is bordered by a  country whose population exceeds the  population of India?  (The answer 'Turkey' has become incorrect as  time has passed. The irony is that Geography was  chosen as a domain without time.)  The abi!ity to answer ridiculously long queries is  of course not the main goal. The main lesson is that  complex sentences are analysed with a proper under-  standing without sacrificing efficiency. Any superfi-  cial pattern matching technique would prove futile  sooner or later.  2.1 Making a Norwegian CHAT-80,  PRAT-89  At the University of Trondheim (NTNU), two stu-  dents made a Norwegian version of CHAT-80,called  PRAT-89 (Teigen and Vetland, 1988),(Teigen and  Vetland, 1989). (Also, a similar Swedish project  SNACK-85 was reported).  The dictionary was changed from English to Nor-  wegian together with new rules for morphological  analysis. The change of grammar from English to  Norwegian proved to be amazingly easy. It showed  that the langauges were more similar than one would  believe, given that the \u001b[1;32;40m language \u001b[0;0ms are incomprehen-  sible to each other's communities.  After changing the dictionary and graramar, the  following Norwegian query about the same domain  could be answered correctly in a few seconds.  Hvilke afrikanske land som hat en  befolkning stoerre enn 3 millioner  og mindre enn 50 millioner og er nord  for Botswana og oest for Libya hat en  hovedstad som hat en befolkning stoerre  enn 100 tusen.  ( A translation is beside the point o.f being a long  query in Norwegian.)  2.2 HSQL - Help System for SQL  A Nordic project HSQL (Help System for SQL) was  accomplished in 1988-89 to make a joint Nordic ef-  fort interfaces to databases.  The HSQL project was led by the Swedish State  Bureau (Statskontoret), with participants from Swe-  den, Denmark, Finland and Norway (Amble et al.,  1990). The aim of HSQL was to build a \u001b[1;32;40m natural \u001b[0;0m  \u001b[1;32;40m language \u001b[0;0m interface to SQL databases for the Scandi-  navian \u001b[1;32;40m language \u001b[0;0ms Swedish, Danish and Norwegian.  These \u001b[1;32;40m language \u001b[0;0ms are very similar, and the Norwe-  gian version of CHAT-80 was easily extended to the  other Scandinavian \u001b[1;32;40m language \u001b[0;0ms. Instead of Geogra-  phy, a more typical application area was chosen to  be a query system for hospital administration. We  decided to target an SQL database of a hospital ad-  ministration which had been developed already.  The next step was then to change the domain  of discourse from Geography to hospital adminis-  tration, using the same knowledge representation  techniques used in CHAT-80. A semantic model of  this domain was made, and then implemented in the  CHAT-80 framework.  The modelling technique that proved adequate  was to use an extended Entity Relationship (ER)  model with a class (type) hierarchy, attributes be-  longing to each class, single inheritance ofattributes  and relationships.  Coupling the system to an SQL database.  After the remodelling, the system could answer  queries in \"Scandinavian\" to an internal hospital  database as well as CHAT-80 could answer Geog-  raphy questions. HSQL produced a Prolog-like code  FOL (First Order Logic) for execution. A mapping  from FOL to the data base Schema was defined, and  a translator from FOL to SQL was implemented.  The example  Hvilke menn ligger i en kvinnes seng?  (Which men lie in a woman's bed? )  would be translated ryly into the SQL query:  SELECT DISTINCT  T3.name,Tl.sex,T2.reg_no,T3.sex,  T4.reg_no,T4.bed_no,T5.hosp_no,T5.ward_no  FROM PATIENT TI,OCCUPANCY T2,PATIENT T3,  OCCUPANCY T4,WARD T5  WHERE  (Tl.sex='f') AND  (T2.reg_no=Tl.reg_no) AND  (T3.sex='m') AND  (T4.reg_no=T3.reg_no) AND  (T4.bed_no=T2.bed_no) AND  (T5.hosp_no=T4.hosp_no) AND  (T5.ward_no=T4.ward_no)  2.3 The The Understanding Computer  The HSQL was a valuable xperience in the effort  to make transportable \u001b[1;32;40m natural \u001b[0;0m anguage interfaces.  However, the underlying system CHAT-80 restricted  the further development.  After the HSQL Project was finished, an inter-  nal reseach project TUC (the Understanding Com-  puter) was initiated at NTNU to carry on the results  from HSQL. The project goals differed from those of  HSQL in a number of ways, and would not be con-  cerned with multimedia interfaces . On the other  hand, portability and versatility were made central  issues concerning the generality of the \u001b[1;32;40m language \u001b[0;0m and  its applications. The research goals could be sum-  marised as to   Give computers an operational understanding  of \u001b[1;32;40m natural \u001b[0;0m \u001b[1;32;40m language \u001b[0;0m.   Build intelligent systems with \u001b[1;32;40m natural \u001b[0;0m \u001b[1;32;40m language \u001b[0;0m  capabilities.   Study common sense reasoning in \u001b[1;32;40m natural \u001b[0;0m an-  guage.  A test criterion for the understanding capacity is  that after a set of definitions in a Naturally Read-  able Logic, NRL, the system's answer to queries in  NRL should conform to the answers of an idealised  rational agent.  Every man that lives loves Mary.  John is a man. John lives.  Who loves Mary?  ==> John  NRL is defined in a closed context. Thus in-  terfaces to other systems are in principle defined  through simulating the environment as a dialogue  partner.  TUC is a prototypical \u001b[1;32;40m natural \u001b[0;0m \u001b[1;32;40m language \u001b[0;0m proces-  sor for English written in Prolog. It is designed to  be a general purpose easily adaptable \u001b[1;32;40m natural \u001b[0;0m lan-  guage processor. It consists of a general grammar  for a subset of English, a semantic knowledge base,  and modules for interfaces to other interfaces like  UNIX, SQL-databases and general textual informa-  tion sources.  2.4 The  TABOR Project  It so happened that a Universtity Project was start-  eded in 1996, called TABOR ( \" Speech based user  interfaces and reasoning systems \"), with the aim of  building an automatic public transport route oracle,  available over the public telephone. At the onset of  the project, the World Wide Web was fresh, and not  as widespread as today, and the telephone was still  regarded as the main source of information for the  public.  Since then, the Internet became the dominant  medium, and it is as likeley to find a computer with  Internet connection, as to find a local busroute table.  ( The consequtive wide spreading of cellular phones  changed the picture in favour of the telephone, but  that is another story).  It was decided that a text based information sys-  tem should be built, regardless of the status of the  speech rocgnition and speech synthesis effort, which  proved to lag behind after a while.  The BusTUC system  The resulting system BusTUC grew out as a \u001b[1;32;40m natural \u001b[0;0m  application of TUC, and an English prototype could  be built within a few months (Bratseth, 1997).  Since the summer 1996, the prototype was put  onto the Internet, and been developed and tested  more or less continually until today. The most im-  portant extension was that the system was made  bilingual (Norwegian and English) during the fall  1996.  In spring 1999, the BusTUC was finally adopted  by the local bus company in Trondheim ( A/S  Trondheim Trafikkselskap), which set up a server (  a 300 MHz PC with Linux).  Until today, over 150.000 questions have been an-  swered, and BusTUC seems to stabilize and grow  increasingly popular.  3  3 Anatomy o f  the  bus  route  orac le   The main components of the bus route information  systems are:   A parser system, consisting of a dictionary, a  lexical processor, a grammar and a parser.   A knowledge base (KB), divided into a semantic  KB and an application KB   A query processor, contalng a routing logic sys-  tem, and a route data base.  The system is bilingual and contains a double set  of dictionary, morphology and grammar. Actually, it  detects which \u001b[1;32;40m language \u001b[0;0m is most probable by count-  ing the number of unknown words related to each  \u001b[1;32;40m language \u001b[0;0m, and acts accordingly. The grammars are  surprisingly similar, but no effort is made to coa-  lesce them. The Norwegian grammar is slightly big-  ger than the English grammar, mostly because it is  more elaborated but also because Norwegian allows  a freer word order.  3.1 Features  of  BusTUC  For the Norwegian systems, the figures give an in-  dication of the size of the domain: 420 nouns, 150  verbs, 165 adjectives, 60 prepositions, etc.  There are 1300 grammar ules ( 810 for English)  although alf of the rules are very low level.  The semantic net described below contains about  4000 entries.  A big name table of 3050 names in addition to  the official station names, is required to capture the  variety of naming. A simple spell correction is a part  of the system ( essentially 1 character errors).  The pragmatic reasoning is needed to translate the  output from the parser to a route database query  \u001b[1;32;40m language \u001b[0;0m . This is done by a production system  called Pragma, which acts like an advanced rewrit-  ing system with 580 rules.  In addition, there is another ule base for actually  generating the \u001b[1;32;40m natural \u001b[0;0m anguage answers (120 rules).  The system is mainly written in Prolog (Sicstus  Prolog 3.7), with some Perl programs for the com-  munication and CGI-scripts.  At the moment, there are about 35000 lines of  programmed Prolog code (in addition to route tables  which are also in Prolog).  Average response time is usually less than 2 sec-  onds, but there are queries that demand up to 10  seconds.  The error rate for single, correct, complete and  relevant questions is about 2 percent.  3.2 The Parser System  The Grammar System  The grammar is based on a simple grammar for  statements, while questions and commands are de-  rived by the use of movements. The grammar  formalism which is called Consensical Grammar,  (CONtext SENSitive CompositionAL Grammar) is  an easy to use variant of Extraposition Grammar  (Pereira and Warren, 1980), which is a generalisa-  tion of Definite Clause Grammars. Compositional  grammar means that the semantics of a a phrase is  composed of the semantics of the subphrases; the ba-  sic constituents being a form of verb complements.  As for Extraposition grammars, a grammar is trans-  lated to Definite Clause Grammars, and executed as  such.  A characteristic syntactic expression in Consen-  sical Grammar  may define an incomplete construct  in terms of a \"difference \" between complete con-  structs. When possible, the parser will use the sub-  tracted part in stead of reading from the input, after  a gap if necessary. The effect is the same as for Ex-  traposition grammars, but the this format is more  intuitive.  Examples of grammar rules.  which is analysed as  for which X is it true that  the (X) person has a dog that barked?  where the last line is analysed as a statement.  Movement is easily handled in Consensical Gram-  mar without making special phrase rules for each  kind of movement. The following example shows  how TUC manages a variety of analyses using move-  ments:  Max said Bill thought  Joe believed Fido Barked.  Who said Bill thought  Joe believed Fido barked? ==> Max  Who did Max say thought  Joe believed Fido barked? ==> Bill  statement(P) --->  noun_phrase(X,VP,P),  verb_phrase(X,VP).  statement(Q) --->  verb_complementsO(VC),  ZZ initial optional verb complements  statement(Q) -...  verb_complementsO(VC).  ZZ may be inserted after a gap  whoseq(P) ---> Z whose dog barked?  \\[whose\\],  hOlm(N),  whoq(P) - ~ without gap  (\\[who\\],\\[has\\],\\[a\\],noun(N),\\[that\\]).  whoq(P) --->  \\[who\\],  whichq(P) - (\\[which\\],\\[person\\]).  whichq(which(X)::P) --->  \\[which\\],  statement(P) - the(X).  Example:  Whose dog barked?  is analysed as if the sentence had been  Who has a dog that  barked?  which is analysed as  Which person has a dog that  barked?  Who did Max say Bill thought  believed Fido barked? ==> Joe  The parser  The experiences with Consensical grammars are a  bit mixed however. The main problem is the parsing  method itself, which is top down with backtracking.  Many principles that would prove elegant for small  domains turned out to be too costly for larger do-  mains, due to the wide variety of modes of expres-  sions, incredible ambiguities and the sheer size of the  covered \u001b[1;32;40m language \u001b[0;0m.  The disambiguation is a major problem for small  grammars and large \u001b[1;32;40m language \u001b[0;0ms, and was solved by  the following guidelines:   a semantic type checking was integrated into the  parser, and would help to discard sematica/ly  wrong parses from the start.   a heuristics was followed that proved almost ir-  reproachable: The longest possible phrase of a  category that is semantically correct is in most  cases the preferred interpretation.   due to the perplexity of the \u001b[1;32;40m language \u001b[0;0m, some  committed choices (cuts) had to be inserted into  the grammar at strategic places. As one could  fear however, this implied that wrong choices  being made at some point in the parsing could  not be recovered by backtracking.  These problems also made it imperative to intro-  duce a timeout on the parsing process of embarass-  ing 10 seconds. Although most sentences, would be  parsed within a second, some legal sentences ofmod-  erate size actually need this time.  4  3.3 The semantic knowledge base  Adaptability means that the system does not need  to be reprogrammed foreach new application.  The design principle of TUC is that most of the  changes are made in a tabular semantic knowledge  base, while there is one general grammar and dictio-  nary. In general, the logic is generated automatically  from the semantic knowledge base.  The nouns play a key role in the understanding  part as they constitute the class or type hierarchy.  Nouns are defined in an a-kind-of hierarchy. The  hierarchy is tree-structured with single inheritance.  The top level also constitute the top level ontology  of TUC's world.  In fact, a type check of the compliances of verbs,  nouns adjectives and prepositions i  not only neces-  sary for the semantic processing but is essential for  the syntax analysis for the disambiguation aswell.  In TUC, the legal combinations are carefully assem-  bled in the semantic network, which then serves a  dual purpose.  These semantic definitions are necessary to allow  for instance the following sentences  The dog saw a man with a telescope.  The man saw a dog with a telescope.  to be treated differently because with telescope  may modify the noun man but not the noun dog,  while with telescope modifies the verb see, re-  stricted to person.  3.4 The Query Processor  Event Calculus  The semantics of the phrases are built up by a kind  of verb complements, where the event play a central  role.  The text is translated from Natural anguage into  a form called TQL (Temporal Query Language/  TUC Query Language) which is a first order event  calculus expression, a self contained expression con-  taining the literal meaning of an utterance.  A formalism TQL that was defined, inspired by  the Event Calculus by Kowalski and Sergot (Kowal-  ski and Sergot, 1986).  The TQL expressions consist of predicates, func-  tions, constants and variables. The textual words  of nouns and verbs are translated to generic predi-  cates using the selected interpretation. The follow-  ing question  Do you know whether the bus goes  to Nidar on Saturday ? would give the TQL expression below. Typically,  the Norwegian equivalent  Vet du om bussen gaar  til Nidar paa soendag ?  5  gives exactly the same code.  test:: %  isa(real,program,tuc), %  isa(real,bus,A), %  isa(real,saturday,B), %  isa(real,place,nidar), %  event(real,D), %  Type of question  tuc is a program  A is a real bus  B isa saturday  Nidar is a place  D is an event  know(whether,tuc,C,D), Y. C was known at D  event (C , E) , Y. E is an event in C  action(go,E), Y. the action of E is Go  actor(A,E), Y. the actor of E is A  srel(to,place,nidar,E),Y. E is to nidar  srel(on,time,B,E), y, E is on the saturday B  The event parameter plays an important role in  the semantics. It is used for various purposes. The  most salient role is to identify a subset of time and  space in which an action or event occured. Both the  actual time and space coordinates are connected to  the actions through the event parameter.  Pragmatic reasoning  The TQL is translated to a route database query  \u001b[1;32;40m language \u001b[0;0m (BusLOG) which is actually a Prolog pro-  gram. This is done by a production system called  Pragma, which acts like an advanced rewriting sys-  tem with 580 rules.  In addition, there is another rule base for actually  generating the \u001b[1;32;40m natural \u001b[0;0m \u001b[1;32;40m language \u001b[0;0m answers (120 rules).  4 Conc lus ions   The TUC approach as as its goal to automate the  creation of new \u001b[1;32;40m natural \u001b[0;0m \u001b[1;32;40m language \u001b[0;0m interfaces for a well  defined subset of the \u001b[1;32;40m language \u001b[0;0m and with a minimum  of explicit programming.  The implemented system has proved its worth,  and is interesting if for no other reason. There is  also an increasing interest from other bus compa-  nies and route information companies alike to get a  similar system for their customers.  Further work remains to make the parser really  efficient, and much work remains to make the lan-  guage coverage complete within reasonable imits.  It is an open question whether the system of this  kind will be a preferred way of offering information  to the public.  If it is, it is a fair amount of work to make it a  portable system that can be implemented lsewhere,  also connecting various travelling agencies.  If not, it will remain a curiosity. But anyway, a system like this will be a contribution to the devel-  opment of intelligent systems.  Re ferences   Tore Amble, Erik Knudsen, Aarno Lehtola, Jan  Ljungberg, and Ole Ravnholt. 1990. Naturlig  Spr~k och Grafik - nya vSgar inn i databaser.  Statskontoret. Rapport om HSQL, ett kunskaps-  baseret hj~lpsystem fSr SQL.  Jon S. Bratseth. 1997. BusTUC - A Natural Lan-  guage Bus Traffic Informations System. Master's  thesis, The Norwegian University of Science and  Technology.  R. Kowalski and M. Sergot. 1986. A logic based  calculus of events. New Generation Computing,  8(0):67-95.  F.C.N. Pereira and D.H.D. Warren. 1980. Definite  clause grammar for \u001b[1;32;40m language \u001b[0;0m analysis. Artificial  Intelligence, 0(3).  J. Teigen and V. Vetland. 1988. Syntax analysis of  norwegian \u001b[1;32;40m language \u001b[0;0m. Technical report, The Nor-  wegian Institute of Technology.  J. Teigen and V. Vetland. 1989. Handling reason-  able questions beyond  the linguistic and conceptual coverage of  \u001b[1;32;40m natural \u001b[0;0m anguage interfaces. Master's thesis, The  Norwegian Institute of Technology.  D.H.D Warren and F.C.N. Pereira. 1982. An effi-  cient and easily adaptable system for interpreting  \u001b[1;32;40m natural \u001b[0;0m \u001b[1;32;40m language \u001b[0;0m queries. Computational Linguis-  tics, 8(3-4).  6  \n",
            "-----------------------------\n",
            "--- document 3: An Automatic Reviser: The TransCheck System  Jean-Marc Jutras  RALI, Universit6 de Montr6al  C.P. 6128, succ. Centre-ville, Montr6al (QC), Canada  jutras@iro.umontreal.ca  Abstract  Over the past decade or so, a lot of work  in computational linguistics has been  directed at finding ways to exploit the  ever increasing volume of electronic  bilingual corpora. These efforts have  allowed for substantial expansion of the  computational toolbox. We describe a  system, TransCheck, which makes  intensive use of these new tools in order  to detect potential translation errors in  preliminary or non-revised translations.  Introduction  For the sake of argument, let's consider a  translator to be a black box with source text  in and target ext out. We feed that box with  texts and, to be really tricky, we input the  same text a couple of times. Looking at the  results, the first thing we notice is that  though the different ranslations are quite  similar, they're not exactly the same.  Nothing to worry about, this may simply  exemplify the potential for synonymy and  paraphrase. But let's further suppose the text  to translate is too big for one individual to  translate in the given time frame. In realistic  conditions, uch a text would be split among  perhaps half a dozen translators, each with  his own vocabulary, experience and stylistic  preferences, which would normally lead to  the well known problem of non-uniformity  of the translation.  It is therefore part of the normal  translation process to have a reviser look at a  translator's output. His job will be to spot  any typos (taken in a very broad sense to  include missing chapters!). Usually, at this  point the translator probably has submitted  the preliminary version to a spell checker, so  what could be done automatically at that  level has already been done. No automatic  detection of typical translation mistakes has  been attempted though. That's the gap  TransCheck is designed to fill. The concept  of a \"translation checker\" was initially  proposed in Isabelle and al. \\[8\\] and  eventually led to a demonstration prototype  concerned with the detection of a very  restricted type of mistake: deceptive  cognates. In comparison, the system  described in this paper goes much further  toward a \"real \" usable translation checker  by allowing for the detection of errors of  omission, the comparison of diverse  numerical expressions and the flagging of  inconsistent terminology.  On the interface side, it allows for the  automatic alignment of the source and target  texts, the flagging of potential mistakes and  the possibility of saving any modifications  made to the target ext.  I Error detection  Complete automatic modelling of the  translation process is still far beyond our  technical ability. The same is true of our  ability to detect all types of translation  mistakes. We can however, for certain well-  defined sub-types of mistake, devise specific  mechanisms. And if a program capable of  detecting all mistakes of translation would  undoubtedly be extremely useful, so would  127  one capable of detecting frequent mistakes,  especially when time is short and a thorough  revision isn't possible. Errors are then bound  to escape the reviser's attention from time to  time. This will not necessary be the case of  an \"automatic reviser\", though. In that  respect, we can compare TransCheck's  behaviour to the familiar \"find and replace\"  now common to every text editors. Who  would know consider doing that particular  task by hand? We now give a short  description of those sub-problems  TransCheck is addressing.  1.1 Errors of omission  The ability to automatically detect  unintended errors of omission would be  much valued, as they can prove quite  embarrassing to the translator. Yet a  diversity of situations can lead to such errors  among which translator's fatigue and the  accidental pressing of a key in a text editor,  as was pointed out by Melamed \\[12\\].  Unfortunately, detecting an omission is far  from being simple when taken in all its  generality (from omission of single words to  whole chapters). This is due in part to the  fact that one \u001b[1;32;40m language \u001b[0;0m may express some  ideas with a greater economy of means than  another, so length difference alone isn't  sufficient to identify omitted text. Consider:   French: Quant ~ la section 5, elle fournit les  rrsultats de nos simulations, que suit notre  conclusion, h la sixi~me t dernirre section.   English: Section 5 describes our simulation  results and the final section concludes.  Excluding punctuation, the French sentence  in the example above has twice as many  words as its English counterpart. Yet there's  nothing wrong with the French translation.  The task is therefore to determine whether  or not correspondence at the word level is  scattered throughout he whole aligned  segment. Word alignment in general tends to  be rather fuzzy though, as the following  example shows:   French: Voici le plan du document.  Literal translation: (Here's) (the) (plan) (of  the) (document)   English: The paper is organized as follows.  Literal translation: (Le) (papier) (est)  (organisr) (comme) (suit)  Independently of the exact method used,  alignment at the word level for this pair of  sentences would prove to be rather weak. It  should be noted however that the above  examples are extreme cases and, without  being extremely rare, they aren't exactly  typical either. They're still a reminder that  small omissions are unlikely to be detected  with sufficient precision considering the  methods available to TransCheck.  1.2 Normative usage of words  Entire repositories of usage mistakes and  other linguistic difficulties of translation  from English to French have been written to  help \u001b[1;32;40m language \u001b[0;0m professionals become aware  of them (Colpron \\[3\\], Dagenais \\[5\\], De  Villiers \\[6\\], Rey \\[14\\], Van Roey and al.  \\[17\\]). Unfortunately, those books are only  useful to confirm existing suspicions. To  warn the unsuspecting translator,  TransCheck incorporates a repository of that  nature.  What's particular about some of these  words, and of interest for an automatic  reviser, is that they cannot be detected by a  simple dictionary lookup, for they do appear  in a monolingual dictionary. What's wrong  isn't the words themselves but the context in  which they are used. Consider, for example,  the English word definitely (en effet)  together with the French ddfinitivement (for  good, once and for all). Though very similar  in form, and both acceptable adverbs in their  respective \u001b[1;32;40m language \u001b[0;0ms, they simply do not  mean the same thing. TransCheck, therefore,  looks through aligned pairs of sentences for  such forbidden word pairs. It also looks for  other types of mistakes, for example  caiques, which could potentially be detected  128  by a complex dictionary lookup. Calques  consist of sequences of legitimate words that  incorrectly mimic the structure of the other  \u001b[1;32;40m language \u001b[0;0m by being sort of literal translations.  1.3 Numerical expressions  A variety of phenomena c n be found under  this heading (telephone numbers,  percentages, fractions, etc.). One important  point these otherwise very diverse types of  constructions have in common is that, being  open sets, they cannot be listed in  repositories. Therefore, their detection will  require the use of grammatical tools of some  sort. But identification is not enough in most  cases. Having simply identified \"2\" in one  text and \"two\" in the other will not alone  permit heir comparison. Conversion toward  a common form is required. Part of this  normalised form must also indicate the type  of phenomenon observed. This is so  because, though there is a 6 underlying the  ordinal sixth, only alignment with an other  ordinal of the same value could be  considered an appropriate match. In  TransCheck, recognition, ormalisation and  phenomenon identification of numerical  expressions are done through appropriate  transducers as will be shown in the next  section.  1.4 Terminological coherence  It's not rare for two or more terms to refer to  the same concept. However, all things being  equal, it's generally taken to be bad practice  to use more than one of the synonyms for  technical terms in a given translation.  Failure to follow this is referred to as  terminological inconsistency. To try and  minimise this problem, each translator  working on a project is given specific  instructions that involve standardising  terminology. Unfortunately, it's not rare for  some translators to ignore these instructions  or even for these instructions never to reach  the translator. Inadequacies are therefore to  be expected, and the bigger the project he  more so. As an example, given the term air  bag and possible translations sac gonflable  and coussin gonflable (literally, inflatable  bag/cushion), it shouldn't be allowed for  both forms to appear in a given translation,  though either one of the two could actually  appear.  2 Tracking mistakes  We have presented briefly the type of errors  detection TransCheck seeks to accomplish  automatically. We will now see in more  details how they are currently being  implemented.  2.1 Prerequisites  In order for TransCheck to detect potential  translation errors, a relatively impressive set  of mechanisms is required. These include:  1. An aligner. After identification of word  and sentence boundaries the text is  processed into a bi-text by an alignment  program. This alignment is done on the  basis of both length (Gale and Church  \\[7\\]) and a notion of cognateness (Simard  \\[161).  2. Transducers. In order to compare  numerical expressions, which often  diverge in format between given pairs of  \u001b[1;32;40m language \u001b[0;0ms, normalisation toward a  common format is required. This is done  with transducers (Kaplan and Kay, \\[10\\]).  3. Part-of-speech tagger. Misleading  similarities in graphical form can  sometime induce translation mistakes  (deceptive cognates). ~ These forbidden  pairs normally involve only one of  several possible parts of speech, hence  the need to disambiguate hem. We do  this with a first-order HMM part-of-  speech tagger (Merialdo \\[13\\]).  I In the rest of the paper, we will use deceptive  cognate very Iosely often to refer to normative usage  of word in general.  129  4. Translation models. Being robust, the  alignment program will align a pair of  texts regardless of possible omissions in  the target ext. To detect such omissions  of text, a probabilistic bilingual  dictionary is called upon. This dictionary  was estimated along the line of Brown  and al.'s first translation model \\[2\\]. It is  used to align (coarsely) at the word  level.  In what follows, we assume the reader to be  at least remotely familiar with most of these  mechanisms. We will however go into more  technical details concerning the transducers  considering the central role they play in  TransCheck.  2.2 Identifying omissions  Grammatical correctors greatly relies on  complex grammars to identify \"typical\"  mistakes. We could imagine doing  something similar for omission detection  trying to construct the meaning of every  sentences in a text and then \"flag\" those  where semantic discontinuity were found,  not unlike what a human would do. This is,  of course, in our wildest dreams as, semantic  analyses still remain to this day extremely  elusive. Not only that, but unlike  grammatical errors, we cannot anticipate  something like a \"typical\" omission as they  will appear randomly and span over any  possible length of text. We must therefore  recast what appears as a semantic problem  in terms of more readily accessible data. The  basic idea here is to assimilate an omission  to a particular type of alignment where an  important contiguous et of words present in  the source text cannot be  level with the target ext.  mechanisms imilar to  Russell \\[15\\].  We can distinguish  aligned at the word  For this we rely on  those described in  between small (a  couple of sentences) and big omissions (any  thing bigger than a few paragraphs). One  might expect he detection of whole missing  pages and chapters not to be difficult, but  that's not necessarily true, as the burden of  the problem then falls on the aligning  program instead of the checker per  se.  Robustness here is the key-word since an  alignment program that couldn't fall back on  its feet after seeing big chunks of missing  text would cause TransCheck to output only  noise thereafter. The alignment program we  use is one such robust program which, as a  first step, seeks to approximate the real  alignment by drawing lines in regions with  high densities of cognate words. Since the  distribution of cognates is a pr ior i  uniform  throughout the text, omitted sections, when  big enough, will show up on the appropriate  graph as an important discontinuity in those  approximation lines. As the omissions  become smaller and smaller, however, the  cognate's uniform distribution hypothesis  becomes increasingly questionable. 2  Still, we are interested in detecting  missing sentences with acceptable precision.  Ideally, this should be reflected as an X to  zero alignment, but alignment programs tend  to associate a high penalty to these cases,  preferring to distribute xtra text on adjacent  regions. In order to recover from these  mergings, TransCheck takes a closer look at  pairs of aligned texts whenever the length  ratio between source and target text falls  under a certain threshold. It then attempts to  aligned those pairs at the word level using a  probabilistic bilingual dictionary that was  estimated on the Canadian Hansard.  The \"Art\" of omission detection can be  seen as one of trial and error in adjusting  precision and recall by choosing appropriate  values for what will constitute a significant  difference in length ratio, a significant span  of words that can't be aligned, and the  penalty to be imposed if some words  z The probability for there to be only a few cognates  between say two paragraphs i  very low for French  and English, but not that low for two sentences.  130  accidentally align due to the imprecision of  the word to word alignment algorithm.  As we have just seen, the problem of  detecting a missing portion of text is, in  TransCheck, closely related to that of  alignment, as it can be reduced to a  misalignment a the word level. All the other  types of errors TransCheck is concerned  with are different in that respect. Correct  alignment is presupposed, and when given  specific pairs of aligned \"tokens\" the task  will be to decide whether they represent  valid translations. We now present he steps  involved in this evaluation.  2.3  Ident i f i ca t ion   In order for TransCheck to evaluate a  translation pair, their constitutive lements  must first be identified. In some cases, this  process requires morphological nalysis and,  in other, a limited type of syntactical  analysis. Both type of analysis serve, to a  certain extend, a single purpose: that of  expressing compactly what would otherwise  be a big list of tokens (in some cases,  involving numerical expressions, an infinite  one). This identification step is done through  appropriate transducers. Basically, there are  two things to keep in mind when dealing  with transducers. One is that, like finite-  state-automaton, they behave like  recognisers; that is, when applied to an input  string, if it can parse it from start to finish,  the string is accepted and otherwise rejected.  The second is that when doing so, it will  produce an output as a result. TransCheck  relies on that last property of transducers to  produce a unique representation for tokens  that are different in form, but semantically  identical, as we will now see.  2.4  Normal i sa t ion   Though we will normally be interested in  the identification of every morphological  form for a given \"interesting\" token, once  identified, these differences will be  discarded by TranCheck. Compare the  examples below.   Air bag / air bags   $2,000,000 / two million dollars / $2 million   June 1st, 2000 / the first of June, 2000  The examples above are all in English, but  the same type of diversity can be found in  French too. In Figure 1 we can see an  example showing the result of both the  process of identification (underlined) and  normalisation (=>).  It  will  definitely => (FAC)  74  be  done  by  January  first  2001  => (DAT)  01012001  (DAT) <=  01012001  (FAC) <=  74  Ce  sera  fait  avant  le   le___r  janvier  2001  d6finitivement  Fig. 1: Token identification and normalisation. 3  Notice that the central part of figure 1 acts  somewhat like a partial transfer* component  (in a word to word translation model)  between the French and the English texts.  Though we haven't implemented it yet, this  could be used to present he user with proper  translation suggestions. 5  The normalisation process depicted in  figure 1, can be slightly complicated by two  factors. One is the need to disambiguate he  part of speech of the identified token.  Consider:  3 FAC stands for \"faux-amis complets\" (deceptive  cognates inall contexts)  4 In the case of deceptive congnates, we could talk of  a forbidden transfer.  5 Transducers can be inverted to create new  transducers that will recognise what was previously  outpuned and output what was recognised.  131   French and English: Local --)  (POS) NomC(FAC)22  Here, the condition field ((POS)NomC))  state that only when nouns are involved will  we be in presence of deceptive cognates (but  not, say, when adjectives are involved).  Consider now:   from May 19th to 24th, 1999  Here, the dates are intermingled. The  transducers we use to analyse such  constructs will produced two distinct  normalised forms that will both be involved  in the comparison process that follows.  2.5 Comparison  The identification and normalisation  process described in the previous two  sections are common to deceptive cognates,  technical terms and numerical expressions  altogether. However, the comparison of the  resulting normalised forms as well as the  processing they should further undergo is of  a rather case specific nature.  During the comparison process,  TransCheck will only be concerned with the  normalised forms resulting from the  previous transduction process (the two  central columns in figure 1). Each of these  two columns will be considered as a set in  the mathematical sense. As a consequence,  the English sentence in figure 1 and the one  given below are indistinguishable from  TransCheck's point of view.   It will definitely, and I mean definitely,  be done by January first, 2001.  Of course, both occurrences of the word  definitely will be flagged if the decision to  flag either one is eventually taken. Each of  these two sets will then be split into up to  three subsets depending on whether they  correspond to numerical expressions,  deceptive cognates or technical terms. At  this point the comparison process will be  very simple. Given these subsets, the  matching conditions will simply amount o  the following:   If a numeral expression appears in one  \u001b[1;32;40m language \u001b[0;0m but not in the other, flag it.   If a deceptive cognate appears in both  \u001b[1;32;40m language \u001b[0;0ms, flag it.   If a term was requested to be flagged,  flag it.  2.6 Putting it all together  To recapitulate, the transducers we use in  TransCheck all have the general form:-  String of interest -')  (condition )(type )identifier  If a transducer identifies a string of interest  and if boundary conditions are met,  information about the nature of the string  will be outputted. In a second step, the  information from one \u001b[1;32;40m language \u001b[0;0m will have to  be matched against he information from the  other in accordance with the condition  imposed by the specific nature of the  identified strings.  3 The TransCheck Prototype  In the previous section, we have described  what happens when a bi-text is submitted to  TransCheck. We now turn to the steps that  will lead to a request.  Currently, TransCheck's interface is  implemented in Tcl/Tk. This has allowed us  to develop a proof of concept without  preoccupying ourselves with word  processing particularities. The down side to  this is a limitation to ascii characters that  will eventually have to be overcome by  making TransCheck part of a text editor not  unlike a spell checker.  But for the time being, a TransCheck  session would look something like this: The  user selects through an interface a French  and an English text specifying with a radio-  button which of the two is the source text. 6  6 The system was initialy developed having in mind  132  Then the name of an alignment file is  supplied (it will be created if it doesn't  already exist). These are the minimal steps  that must be taken before any analysis can  take place. If, at this point, the bi-text is  submitted for analysis, TransCheck will use  all of it's default values and, after some  window pop-up and progress report, a  window containing the target text will  appear on screen together with the source  text facing it. All the potential errors will  appear highlighted. At this point, the user  can modify the target text to correct any  found errors. When the session ends, the  modified text will be saved (together with  the appropriately modified alignment file).  We've just seen TransCheck's default  behaviour. The user is also offered some  customisation possibilities. This includes  highlighting only those type of errors of  interest o the user and setting the alignment  parameters. The omission detection  parameters can also be modified through an  interface. Also, since as with any normative  judgement, what is and what isn't a \"correct\"  form will always be subject to debate,  TransCheck allows the user to silence those  alleged mistakes causing too much noise on  a given text. Finally, the human reviser is  allowed, any time during a session, to  modify TransCheck's behaviour so that  newly identified incorrect terms will be  flagged thereafter, this to ensure that none of  subsequent occurrences of these errors will  escape his attention. This list of forbidden  terms can be saved in order to constitute  client specific databases o that identified  problems will not be lost between projects.  4 Further development and discussion  At present, TransCheck allows for only  limited customisation. However, we are well  aware that the repositories available for say  deceptive cognates are costly to develop and  English as the source text. Currently, this is still  reflected only in the deceptive cognate database.  tend to include only those mistakes having a  certain \"history\" (stability over time). That  suggests the user should be allowed to add  new pairs of prohibited translations on the  fly. In most cases, however, adding new  behaviour is a complex process available  only to the system's designer because of  morphology and part-of-speech  considerations. Added flexibility in this  regard seems mandatory. Since we cannot  expect he human reviser to concern himself  with such technical details, these would  have to be hidden from him through  adequate input interfaces. This flexibility  seems to be desired independently from the  now emerging problem of localisation. 7 We  are currently addressing these issues one at a  time.  So far, we have described the types of  errors TransCheck is concerned with, the  way they are handled and how some aspects  of the processing can be customised. No  figures as to precision and recall have been  given though. This is in part due to the  difficulty of finding preliminary translations  and in part to TransCheck's customisability.  For example, performance on omission  detection will ultimately depend on the  user's selected values. It seems to us that the  best way to address both of these problems  should be to actually put the system in the  hands of human revisers and monitor the  changes they would actually choose to  make. Efforts in that direction are currently  being made.  Conclusion  To our knowledge, TransCheck is still  unique among text checkers in addressing  the problem of translation errors. For a long  time, only a concept without form,  TransCheck, as presented in this paper, has  shown the concept of a translation checker  7 Adaptation of a text for use in a different region.  For example, Canadian postal code (AIB 2C3)  compared to American Zip Code (12345).  133  to be sound and realistic. Admittedly, a lot  of work, especially on the specific  grammars, still has to be done. But all this  now seems like a worthwhile effort  considering that the resulting program could  help translators considerably in their efforts  to meet the quality requirements and tight  deadlines they are frequently facing. We  have also stressed TransCheck's adaptability  to be somewhat limited. The problem seems  more one of ergonomics than of principle,  though. Interfaces would have to be devised  to guide users through the sometime  complicated steps associated with adding  new restrictions. We are now considering  the possibility of integrating TransCheck in  an off-the-shelf text editor to cross the ascii  barrier.  Acknowledgements  I would like to thank Elliott Macklovich,  Claude B6dard, Mich~le Lamarche and Guy  Lapalme for their invaluable comments on  drafts of this paper.  References  \\[1\\] Brown P., Cocke J., Della Pietra S., Della Pietra  V., Jelinek F., Lafferty J., Mercer R., Roosin P., A.  (1990) Statistical Approach to Machine  Translation. Computational Linguistics, 16, pp.  79-85.  \\[2\\] Brown P., Della Pietra S., Della Pietra V.,  Mercer R. (1993) The Mathematics of Machine  Translation: Parameter Estimation tatistical  Approach to Machine Translation. Computational  Linguistics, 19, pp. 263-311.  \\[3\\] Colpron, G. (1982) Dictionnaire d'anglicismes.  Laval (Qu6bec), t~ditions Beauchemin.  \\[4l Dagan, I and Church K. (1997) Termight:  Coordinating Humans and Machines in Bilingual  Terminology Acquisition. Machine Translation, 12,  pp. 89-107.  \\[5\\] Dagenais, G. (1984) Dictionnaire des diJficultds  de la langue fran(aise au Canada. Boucherville,  Les l~ditions franqaises.  \\[6\\] De Villiers, J.-l~. (1988) Multidictionnaire des  difficult$s de la langue fran(aise. Montr6al,  l~ditions Qu6bec/Am&ique.  \\[7l Gale, W., Church K. (1991) A Program for  Aligning Sentences in Bilingual Corpora.  Proceedings of the 29 ~h Annual Meeting of the  Association for Computational Linguistics,  Berkeley, pp. 177-184.  \\[8\\] Isabelle P. and al. (1993) Translation Analysis  and Translation Automation. Proceedings of the  Fifth International Conference on Theoretical and  Methodological Issues in Machine Translation  (TMI-93), Kyoto, pp. 201-217.  \\[9\\] Justeson, J. and Slava K. (1995) Technical  Terminology: some linguistic properties and an  algorithm for identification in text. Natural  Language Engineering, 1, pp. 9-28.  \\[10\\] Kaplan, R. M., Kay, M. (1994) Regular Models  of Phonological Rule Systems, Computational  Linguistics, 20, pp. 331-378.  \\[11\\] Macklovitch, E. (1996) Peut-on v~rifier  automatiquement la cohdrence terminologique?  Meta, 41, pp. 299-316.  \\[12\\] Melamed, I. D. (1996) Automatic Detection of  Omissions in Translations. In the 16 tn International  Conference on Computational Linguistics.  Copenhagen, pp. 764-769.  \\[13\\] Merialdo, B. (1994) Tagging English Text with  a Probabilistic Model. Computational Linguistics,  20, pp. 155-168.  \\[ 14\\] Rey J. (1984) Dictionnaire s~lectif et comment~  des difficult~s de la version anglaise. Paris,  l~ditions Ophrys.  \\[15\\] Russell, G. (1999) Errors of omission in  translation. Proceedings of the Eighth International  Conference on Theoretical and Methodological  Issues in Machine Translation (TMI-99), Chester,  1999, pp. 128-138.  \\[16\\] Simard M., Foster G. and Isabelle P. (1992)  Using Cognates to Align Sentences in Parallel  Corpora. Proceedings of the Fourth International  Conference on Theoretical and Methodological  Issues in Machine Translation (TMI-92), Montr6al,  pp. 67-81.  \\[17\\] Van Roey, J., Granger S. and Swallow J. (1988)  Dictionnaire des faux amis fran(ais-anglais. Paris,  Duculot.  134  \n",
            "-----------------------------\n",
            "--- document 9: Translation using Information on Dialogue Participants  Setsuo Yamada, E i i ch i ro  Sumi ta  and  H idek i  Kashioka  ATR Interpreting Telecommunications Research Laboratories*  2-2, Hikaridai, Seika-cho, Soraku-gun,  Kyoto, 619-0288, JAPAN  { syamada, sumita, kashioka} @itl.atr.co.jp t  Abstract  This paper proposes a way to improve the trans-  lation quality by using information on dialogue  participants that is easily obtained from out-  side the translation component. We incorpo-  rated information on participants' ocial roles  and genders into transfer ules and dictionary  entries. An experiment with 23 unseen dia-  logues demonstrated a recall of 65% and a preci-  sion of 86%. These results howed that our sim-  ple and easy-to-implement method is effective,  and is a key technology enabling smooth con-  versation with a dialogue translation system.  1 I n t roduct ion   Recently, various dialogue translation systems  have been proposed (Bub and others, 1997;  Kurematsu and Morimoto, 1996; Rayner and  Carter, 1997; Ros~ and Levin, 1998; Sumita  and others, 1999; Yang and Park, 1997; Vi-  dal, 1997). If we want to make a conversation  proceed smoothly using these translation sys-  tems, it is important o use not only linguis-  tic information, which comes from the source  \u001b[1;32;40m language \u001b[0;0m, but also extra-linguistic nformation,  which does not come from the source \u001b[1;32;40m language \u001b[0;0m,  but, is shared between the participants of the  conversation.  Several dialogue translation methods that  use extra-linguistic information have been pro-  posed. Horiguchi outlined how \"spoken lan-  guage pragmatic information\" can be trans-  lated (Horiguchi, 1997). However, she did not  apply this idea to a dialogue translation system.  LuperFoy et al. proposed a software architec-  *Current affiliation is ATR Spoken Language Trans-  lation Research Laboratories  Current mail addresses are  { setsuo.yarnada, eiichiro.sumita, hideki.kashioka}  @slt. atr. co.jp  ture that uses '% pragmatic adaptation\" (Lu-  perFoy and others, 1998), and Mima et al. pro-  posed a method that uses \"situational informa-  tion\" (Mima and others, 1997). LuperFoy et al.  simulated their method on man-machine inter-  faces and Mima et al. preliminarily evaluated  their method. Neither study, however, applied  its proposals to an actual dialogue translation  system.  The above mentioned methods will need time  to work in practice, since it is hard to obtain  the extra-linguistic nformation on which they  depend.  We have been paying special attention to \"po-  liteness,\" because a lack of politeness can inter-  fere with a smooth conversation between two  participants, uch as a clerk and a customer. It  is easy for a dialogue translation system to know  which participant is the clerk and which is the  customer from the interface (such as the wires  to the microphones).  This paper describes a method of \"polite-  ness\" selection according to a participant's so-  cial role (a clerk or a customer), which is eas-  ily obtained from the extra-linguistic environ-  ment. We incorporated each participant's so-  cial role into transfer ules and transfer dictio-  nary entries. We then conducted an experiment  with 23 unseen dialogues (344 utterances). Our  method achieved a recall of 65% and a preci-  sion of 86%. These rates could be improved to  86% and 96%, respectively (see Section 4). It  is therefore possible to use a \"participant's so-  cial role\" (a clerk or a customer in this case)  to appropriately make the translation results  \"polite,\" and to make the conversation proceed  smoothly with a dialogue translation system.  Section 2 analyzes the relationship between a particular participant's social role (a clerk) and  politeness in Japanese. Section 3 describes our  proposal in detail using an English-to-Japanese  37  translation system. Section 4 shows an exper-  iment and results, followed by a discussion in  Section 5. Finally, Section 6 concludes this pa-  per.  2 A Par t i c ipant ' s  Soc ia l  Ro le  and   Po l i teness   This section focuses on one participant's social  role. We investigated Japanese outputs of a di-  alogue translation system to see how many ut-  terances hould be polite expressions in a cur-  rent translation system for travel arrangement.  We input 1,409 clerk utterances into a Transfer  Driven Machine Translation system (Sumita  and others, 1999) (TDMT for short). The in-  puts were closed utterances, meaning the sys-  tem already knew the utterances, enabling the  utterances to be transferred at a good quality.  Therefore, we used closed utterances as the in-  puts to avoid translation errors.  As a result, it was shown that about 70%  (952) of all utterances should be improved to use  polite expressions. This result shows that a cur-  rent translation system is not enough to make  a conversation smoothly. Not surprisingly, if all  expressions were polite, some Japanese speakers  would feel insulted. Therefore, Japanese speak-  ers do not have to use polite expression in all  utterances.  We classified the investigated ata into dif-  ferent ypes of English expressions for Japanese  politeness, i.e., into honorific titles, parts of  speech such as verbs, and canned phrases,  as shown in Table 1; however, not all types  appeared in the data. For example, when  the clerk said \"How will you be paying, Mr.  Suzuki,\" the Japanese translation was made  polite as \"donoyouni oshiharaininarimasu-ka  suzuki-sama\" in place of the standard expres-  sion \"donoyouni shiharaimasu-ka suzuki-san.\"  Table 1 shows that there is a difference in  how expressions should be made more polite ac-  cording to the type, and that many polite ex-  pressions can be translated by using only local  information, i.e., transfer rules and dictionary  entries. In the next section, we describe how to  incorporate the information on dialogue partic-  ipants, such as roles and genders, into transfer  rules and dictionary entries in a dialogue trans-  lation system.  3 A Method  of  Us ing  In fo rmat ion   on  D ia logue  Par t i c ipants   This section describes how to use information  on dialogue participants, such as participants'  social roles and genders. First, we describe  TDMT, which we also used in our experiment.  Second, we mention how to modify transfer  rules and transfer dictionary entries according  to information on dialogue participants.  3.1 Transfer  Dr iven  Mach ine   Trans la t ion   TDMT uses bottom-up left-to-right chart pars-  ing with transfer rules as shown in Figure 1.  The parsing determines the best structure and  best transferred result locally by performing  structural disambiguation using semantic dis-  tance calculations, in parallel with the deriva-  tion of possible structures. The semantic dis-  tance is defined by a thesaurus.  (source pattern)  ==~  J ((target pattern 1)  ((source xample 1)  (source xample 2)   \"- )  (target pattern 2)  o* )  Figure 1: Transfer ule format  A transfer ule consists of a source pattern,  a target pattern, and a source example. The  source pattern consists of variables and con-  stituent boundaries (Furuse and Iida, 1996).  A constituent boundary is either a functional  word or the part-of-speech of a left constituent's  last word and the part-of-speech of a right con-  stituent's first word. In Example (1), the con-  stituent boundary IV-CN) is inserted between  \"accept\" and \"payment,\" because \"accept\" is  a Verb and \"payment\" is a Common Noun.  The target pattern consists of variables that cor-  respond to variables in the source pattern and  words of the target \u001b[1;32;40m language \u001b[0;0m. The source exam-  ple consists of words that come from utterances  referred to when a person creates transfer ules  (we call such utterances closed utterances).  Figure 2 shows a transfer ule whose source  pattern is (X (V-CN) Y). Variable X corre-  sponds to x, which is used in the target pat-  tern, and Y corresponds to y, which is also  38  Table 1: Examples of polite expressions  Type: verb, title  Eng: How will you be paying, Mr. Suzuki  Standard: donoyouni shiharaimasu-ka suzuki-san  Polite: donoyouni o_shiharaininarimasu-ka suzuki-sama  Gloss: How pay-QUESTION suzuki-Mr.  Type: verb, common noun  Eng: We have two types of rooms available  Standard: aiteiru ni-shurui-no heya-ga ariraasu  Polite: aiteiru ni-shurui-no oheya-ga gozaimasu  Gloss: available two-types-of room-TOP have  Type: auxiliary verb  Eng: You can shop for hours  Standard: suujikan kaimono-wo surukotogadekimasu  Polite: suujikan kaimono-wo shiteitadakemasu  Gloss: for hours make-OBJ can  Type: pronoun  Eng: Your room number, please  Standard: anatano heya bangou-wo  Polite: okyakusamano heya bangou-wo  Gloss: Your room number-so obj  onegaishirnasu  onegaishimasu  please  Type: canned phrase  Eng: How can I help you  Standard: dou shimashitaka  Polite: douitta goyoukendeshouka  Gloss: How can I help you  Example (1)  Eng: We accept payment by credit card  Standard: watashitachi-wa kurejitlo-kaado-deno shiharai-wo ukelsukemasu  Polite: watashidomo-wa kurejitto-kaado-deno o_shiharai-wo ukeshimasu  Gloss: We-TOP credit-card-by payment-OBJ accept  used in the target pattern. The source exam-  ple ((\"accept\") (\"payment\")) comes from Ex-  ample (1), and the other source examples come  from the other closed utterances. This transfer  rule means that if the source pattern is (X (V-  CN) Y) then (y \"wo\" x) or (y \"ni\" x) is selected  as the target pattern, where an input word pair  corresponding to X and Y is semantically the  most similar in a thesaurus to, or exactly the  same as, the source example. For example, if  an input word pair corresponding to X and Y  is semantically the most similar in a thesaurus  to, or exactly the same as, ((\"accept\") (\"pay-  ment\")), then the target pattern (y \"wo\" x) is  selected in Figure 2. As a result, an appropriate  target pattern is selected.  After a target pattern is selected, TDMT cre-  ates a target structure according to the pattern  (X (V-CN) Y)  ((y \"wo\" x)  (((\"accept\") (\"payment\"))  ((\"take\") (\"picture\")))  (y \"hi\" x)  (((\"take\") (\"bus\"))  ((\"get\") (\"sunstroke\")))  )  Figure 2: Transfer ule example  by referring to a transfer dictionary, as shown  in Figure 3. If the input is \"accept (V -CN)   payment,\" then this part is translated into \"shi-  harai wo uketsukeru.\" \"wo\" is derived from the  target pattern (y \"wo\" x), and \"shiharai\" and  \"uketsukeru\" are derived from the transfer dic-  tionary, as shown in Figure 4.  39  (source pattern)  (((target pattern 11) :pattern-cond 11 (target pattern 12) :pattern-cond 12  itarget pattern In) :default)  ((source xample 1)   oo )  (((source xample 1) ~ (target word lt) :word-cond 11  (source example 1) --* (target word 12) :word-cond 12   .   (source example 1) --* (target word lm) :default)  o . \"  )  (((target pattern 21) :pattern-cond 21  . . .  ) ) )   Figure 5: Transfer ule format with information on dialogue participants  (((source word 1) --* (target word 11) :cond 11 I  (source word 1) -* (target word 12) :cond 12 I  I . . .   (source word 1) -~ (target word lk) :default)\\[  o*.  ) I  Figure 6: Dictionary format with information on dialogue participants  ((source word) ~ (target word)   \" .  )  Figure 3: Transfer dictionary format  ((\"accept\") --* (\"uketsukeru') I (\"payment\") --* (\"shiharai\"))   Figure 4: Transfer dictionary example  (X \"sama\")  (((\"Mr.\" x) :h-gender male  (\"Ms.\" x) :h-gender female  (\"Mr-ms.\" x))  ((\"room number\")))  )  Figure 7: Transfer ule example with the par-  ticipant's gender  3.2 Transfer Rules and Entr ies  according to Information on  Dialogue Part ic ipants  For this research, we modified the transfer ules  and the transfer dictionary entries, as shown in  Figures 5 and 6. In Figure 5, the target pattern  \"target pattern 11\" and the source word \"source  example 1\" are used to change the translation  according to information on dialogue partici-  pants. For example, if \":pattern-cond 11\" is de-  fined as \":h-gender male\" as shown in Figure 7,  then \"target pattern 11\" is selected when the  hearer is a male, that is, \"(\"Mr.\" x)\" is selected.  Moreover, if \":word-cond 11\" is defined as \":s-  role clerk\" as shown in Figure 8, then \"source  example 1\" is translated into \"target word 11\"  when the speaker is a clerk, that is, \"accept\" is  translated into \"oukesuru.\" Translations uch  as \"target word 11\" are valid only in the source  pattern; that is, a source example might not  always be translated into one of these target  words. If we always want to produce transla-  tions according to information on dialogue par-  ticipants, then we need to modify the entries  in the transfer dictionary like Figure 6 shows.  Conversely, if we do not want to always change  the translation, then we should not modify the  entries but modify the transfer ules. Several  conditions can also be given to \":word-cond\"  and \":pattern-cond.\" For example, \":s-role cus-  tomer and :s-gender female,\" which means the  speaker is a customer and a female, can be  given. In Figure 5, \":default\" means the de-  40  fault target pattern or word if no condition is  matched. The condition is checked from up to  down in order; that is, first, \":pattern-cond 11,\"  second, \":pattern-cond 1~,\" ... and so on.  (X (V-CN) Y)  ((y \"wo\" x)  (((\"accept\") (\"payment\"))  ((\"take\") (\"picture\")))  (((\"accept\") -~ (\"oukesuru\"):s-role clerk  ( \"accept\" ) --+ ( \"uketsukeru\" ) ))  )  Figure 8: Transfer ule example with a partici-  pant's role  (((\"payment\") --~ (\"oshiharai\") :s-role clerk  ( \"payment\" ) ---* ( \"shiharai\" ))  ((\"we\") --* (\"watashidomo\") :s-role clerk  (\"we\") --~ (\"watashltachi\")))  Figure 9: Transfer dictionary example with a  speaker's role  Even though we do not have rules and en-  tries for pattern conditions and word condi-  tions according to another participant's infor-  mation, such as \":s-role customer'(which means  the speaker's role is a customer) and \":s-gender  male\" (which means the speaker's gender is  male), TDMT can translate xpressions corre-  sponding to this information too. For example,  \"Very good, please let me confirm them\" will  be translated into \"shouchiitashimasita kakunin  sasete itadakimasu\" when the speaker is a clerk  or \"soredekekkoudesu kakunin sasete kudasai\"  when the speaker is a customer, as shown in  Example (2).  By making a rule and an entry like the ex-  amples shown in Figures 8 and 9, the utter-  ance of Example (1) will be translated into  \"watashidomo wa kurejitto kaado deno oshi-  harai wo oukeshimasu\" when the speaker is a  clerk.  4 An  Exper iment   The TDMT system for English-to-Japanese at  the time Of the experiment had about 1,500  transfer ules and 8,000 transfer dictionary en-  tries. In other words, this TDMT system was  capable of translating 8,000 English words into  Japanese words. About 300 transfer ules and  40 transfer dictionary entries were modified to  improve the level of \"politeness.\"  We conducted an experiment using the trans-  fer rules and transfer dictionary for a clerk with  23 unseen dialogues (344 utterances). Our input  was off-line, i.e., a transcription of dialogues,  which was encoded with the participant's social  role. In the on-line situation, our system can  not infer whether the participant's social role is  a clerk or a customer, but can instead etermine  the role without error from the interface (such  as a microphone or a button).  In order to evaluate the experiment, we clas-  sifted the Japanese translation results obtained  for the 23 unseen dialogues (199 utterances from  a clerk, and 145 utterances from a customer,  making 344 utterances in total) into two types:  expressions that had to be changed to more po-  lite expressions, and expressions that did not.  Table 2 shows the number of utterances that in-  cluded an expression which had to be changed  into a more polite one (indicated by \"Yes\") and  those that did not (indicated by \"No\"). We ne-  glected 74 utterances whose translations were  too poor to judge whether to assign a \"Yes\" or  \"No.\"  Table 2: The number of utterances to be  changed or not  Necessity | The number  of change I of utterances  Yes 104  No 166  Out of scope 74  Total \\[ 344  * 74 translations were too poor to handle for the  \"politeness\" problem, and so they are ignored in this  paper.  The translation results were evaluated to see  whether the impressions of the translated re-  sults were improved or not with/without mod-  ification for the clerk from the viewpoint of  \"politeness.\" Table 3 shows the impressions  obtained according to the necessity of change  shown in Table 2.  The evaluation criteria are recall and preci-  sion, which are defined as follows:  Recall =  number of utterances whose impression is better  number of utterances which should be more polite  41  Example (2)  Eng: Very good, please let me confirm them  Standard: wakarimasita kakunin sasete  Clerk: shouchiitashimasita kakunin sase~e  Customer: soredekekkoudesu kakunin sasete  Gloss: very good con:firm let me  kudasai  itadakimasu  kudasai  please  Table 3: Evaluation on using the speaker's role  Necessity  of change  Yes  (lo4)  No  (166)  ~ Impression  better  same  worse   no-diff  better  s alTle  worse   no-diff  The number  of utterances  68  5  3  28  0  3  0  163  bet ter :  Impression of a translation is better.  same:  Impression of a translation has not changed.  worse: Impression of a translation is worse.  no-diff: There is no difference between the two  translations.  Precision =  number of utterances whose impression is better  number of utterances whose expression has been  changed by the modified rules and entries  The recall was 65% (= 68 - (68 + 5 + 3 + 28))  and the precision was 86% (= 68 -: (68 + 5 + 3 +  0+3+0)).   There are two main reasons which bring down  these rates. One reason is that TDMT does not  know who or what the agent of the action in  the utterance is; agents are also needed to se-  lect polite expressions. The other reason is that  there are not enough rules and transfer dictio-  nary entries for the clerk.  It is easier to take care of the latter problem  than the former problem. If we resolve the lat-  ter problem, that is, if we expand the transfer  rules and the transfer dictionary entries accord-  ing to the \"participant's social role\" (a clerk and  a customer), then the recall rate and the preci-  sion rate can be improved (to 86% and 96%,  respectively, as we have found). As a result, we  can say that our method is effective for smooth  conversation with a dialogue translation system.  5 D iscuss ion   In general, extra-linguistic information is hard  to obtain. However, some extra-linguistic infor-  mation can be easily obtained:  (1) One piece of information is the participant's  social role, which can be obtained from the in-  terface such as the microphone used. It was  proven that a clerk and customer as the social  roles of participants are useful for translation  into Japanese. However, more research is re-  quired on another participant's social role.  (2) Another piece of information is the par-  ticipant's gender, which can be obtained by a  speech recognizer with high accuracy (Takezawa  and others, 1998; Naito and others, 1998). We  have considered how expressions can be useful  by using the hearer's gender for Japanese-to-  English translation.  Let us consider the Japanese honorific title  \"sama\" or \"san.\" If the heater's gender is male,  then it should be translated \"Mr.\" and if the  hearer's gender is female, then it should be  translated \"Ms.\" as shown in Figure 7. Ad-  ditionally, the participant's gender is useful for  translating typical expressions for males or fe-  males. For example, Japanese \"wa\" is often at-  tached at the end of the utterance by females.  It is also important for a dialogue translation  system to use extra-linguistic information which  the system can obtain easily, in order to make  a conversation proceed smoothly and comfort-  ably for humans using the translation system.  We expect hat other pieces of usable informa-  tion can be easily obtained in the future. For  example, age might be obtained from a cellular  telephone if it were always carried by the same  person and provided with personal information.  In this case, if the system knew the hearer was a  child, it could change complex expressions into  easier ones.  6 Conc lus ion   We have proposed a method of translation us-  ing information on dialogue participants, which  42  is easily obtained from outside the translation  component, and applied it to a dialogue trans-  lation system for travel arrangement. This  method can select a polite expression for an  utterance according to the \"participant's social  role,\" which is easily determined by the inter-  face (such as the wires to the microphones). For  example, if the microphone is for the clerk (the  speaker is a clerk), then the dialogue translation  system can select a more polite expression.  In an English-to-Japanese translation system,  we added additional transfer ules and transfer  dictionary entries for the clerk to be more po-  lite than the customer. Then, we conducted an  experiment with 23 unseen dialogues (344 ut-  terances). We evaluated the translation results  to see whether the impressions of the results im-  proved or not. Our method achieved a recall of  65% and a precision of 86%. These rates could  easily be improved to 86% and 96%, respec-  tively. Therefore, we can say that our method  is effective for smooth conversation with a dia-  logue translation system.  Our proposal has a limitation in that if the  system does not know who or what the agent  of an action in an utterance is, it cannot ap-  propriately select a polite expression. We are  considering ways to enable identification of the  agent of an action in an utterance and to ex-  pand the current framework to improve the level  of politeness even more. In addition, we intend  to apply other extra-linguistic nformation to a  dialogue translation system.  References   Thomas Bub et al. 1997. Verbmobih The  combination of deep and shallow processing  for spontaneous speech translation. In the  1997 International Conference on Acoustics,  Speech, and Signal Processing: ICASSP 97,  pages 71-74, Munich.  Osamu Furuse and Hitoshi Iida. 1996. In-  cremental translation utilizing constituent  boundary patterns. In Proceedings of  COLING-96, pages 412-417, Copenhagen.  Keiko Horiguchi. 1997. Towards translating  spoken \u001b[1;32;40m language \u001b[0;0m pragmatics in an analogical  framework. In Proceedings ofA CL/EA CL-97  workshop on Spoken Language Translation,  pages 16-23, Madrid.  Akira Kurematsu and Tsuyoshi Morimoto.  1996. Automatic Speech Translation. Gordon  and Breach Publishers.  Susann LuperFoy et al. 1998. An architecture  for dialogue management, context tracking,  and pragmatic adaptation i  spoken dialogue  system. In Proceedings of COLING-A CL'98,  pages 794-801, Montreal.  Hideki Mima et al. 1997. A situation-based  approach to spoken dialogue translation be-  tween different social roles. In Proceedings of TMI-97, pages 176-183, Santa Fe.  Masaki Naito et al. 1998. Acoustic and lan-  guage model for speech translation system  ATR-MATRIX. In the Proceedings of the  1998 Spring Meeting of the Acoustical Soci-  ety of Japan, pages 159-160 (in Japanese).  Manny Rayner and David Carter. 1997. Hy-  brid \u001b[1;32;40m language \u001b[0;0m processing in the spoken lan-  guage translator. In the 1997 International  Conference on Acoustics, Speech, and Signal  Processing: ICASSP 97, pages 107-110, Mu-  nich.  Carolyn Penstein Ros~ and Lori S. Levin. 1998.  An interactive domain independent approach  to robust dialogue interpretation. In Proceed-  ings of COLING-ACL'98, pages 1129-1135,  Montreal.  Eiichiro Sumita et al. 1999. Solutions to prob-  lems inherent in spoken-\u001b[1;32;40m language \u001b[0;0m translation:  The ATR-MATRIX approach. In the Ma-  chine Translation Summit VII, pages 229-  235, Singapore.  Toshiyuki Takezawa et al. 1998. A Japanese-  to-English speech translation system: ATR-  MATRIX. In the 5th International Con-  ference On Spoken Language Processing:  ICSLP-98, pages 2779-2782, Sydney.  Enrique Vidal. 1997. Finite-state speech-to-  speech translation. In the 1997 International  Conference on Acoustics, Speech, and Signal  Processing: ICASSP 97, pages 111-114, Mu-  nich.  Jae-Woo Yang and Jun Park. 1997. An exper-  iment on Korean-to-English and Korean-to-  Japanese spoken \u001b[1;32;40m language \u001b[0;0m translation. In the  1997 International Conference on Acoustics,  Speech, and Signal Processing: ICASSP 97,  pages 87-90, Munich.  43  \n",
            "-----------------------------\n",
            "--- document 10: Automatic construction of parallel English-Chinese corpus for  cross-\u001b[1;32;40m language \u001b[0;0m information retrieval  J i ang  Chen and  J ian -Yun  N ie   D~partement d ' In format ique et Recherche Op~rationnel le  Universit~ de Montreal   C.P. 6128, succursale CENTRE-V ILLE   Montreal  (Quebec), Canada  H3C 3J7  {chen, nie} @iro. umontreal, ca  Abst rac t   A major obstacle to the construction ofa probabilis-  tic translation model is the lack of large parallel cor-  pora. In this paper we first describe a parallel text  mining system that finds parallel texts automatically  on the Web. The generated Chinese-English paral-  lel corpus is used to train a probabilistic translation  model which translates queries for Chinese-English  cross-\u001b[1;32;40m language \u001b[0;0m information retrieval (CLIR). We will  discuss ome problems in translation model training  and show the preliminary CUR results.  1 In t roduct ion   Parallel texts have been used in a number of studies  in computational linguistics. Brown et al. (1993)  defined a series of probabilistic translation models  for MT purposes. While people may question the  effectiveness of using these models for a full-blown  MT system, the models are certainly valuable for de-  veloping translation assistance tools. For example,  we can use such a translation model to help com-  plete target ext being drafted by a human transla-  tor (Langlais et al., 2000).  Another utilization is in cross-\u001b[1;32;40m language \u001b[0;0m informa-  tion retrieval (CLIR) where queries have to be trans-  lated from one \u001b[1;32;40m language \u001b[0;0m to another \u001b[1;32;40m language \u001b[0;0m in  which the documents are written. In CLIR, the qual-  ity requirement for translation is relatively low. For  example, the syntactic aspect is irrelevant. Even if  the translated word is not a true translation but is  strongly related to the original query, it is still help-  ful. Therefore, CLIR is a suitable application for  such a translation model.  However, a major obstacle to this approach is the  lack of parallel corpora for model training. Only  a few such corpora exist, including the Hansard  English-French corpus and the HKUST English-  Chinese corpus (Wu, 1994). In this paper, we will  describe a method which automatically searches for  parallel texts on the Web. We will discuss the text  mining algorithm we adopted, some issues in trans-  lation model training using the generated parallel  corpus, and finally the translation model's perfor-  mance in CLIR.  2 Para l le l  Text  M in ing  A lgor i thm  The PTMiner system is an intelligent Web agent  that is designed to search for large amounts of paral-  lel text on the Web. The mining algorithm is largely  \u001b[1;32;40m language \u001b[0;0m independent. It can thus be adapted to  other \u001b[1;32;40m language \u001b[0;0m pairs with only minor modifications.  Taking advantage ofWeb search engines as much  as possible, PTMiner implements he following steps  (illustrated in Fig. 1):  1 Search for candidate sites - Using existing Web  search engines, search for the candidate sites  that may contain parallel pages;  2 File name fetching - For each candidate site,  fetch the URLs of Web pages that are indexed  by the search engines;  3 Host crawling - Starting from the URLs col-  lected in the previous tep, search through each  candidate site separately for more URLs;  4 Pair scan - From the obtained URLs of each  site, scan for possible parallel pairs;  5 Download and verifying - Download the parallel  pages, determine file size, \u001b[1;32;40m language \u001b[0;0m, and charac-  ter set of each page, and filter out non-parallel  pairs.  2.1 Search for candidate Sites  We take advantage of the huge number of Web sites  indexed by existing search engines in determining  candidate sites. This is done by submitting some  particular equests to the search engines. The re-  quests are determined according to the following ob-  servations. In the sites where parallel text exists,  there are normally some pages in one \u001b[1;32;40m language \u001b[0;0m con-  taining links to the parallel version in the other lan-  guage. These are usually indicated by those links'  anchor texts 1. For example, on some English page  there may be a link to its Chinese version with  the anchor text \"Chinese Version\" or \"in Chinese\".  1An anchor text  is a piece of text on a Web page which,  when clicked on, will take you to another linked page. To  be helpful, it usual ly  contains the key information about the  l inked page.  21  Figure 1: The workflow of the mining process.  The same phenomenon can be observed on Chinese  pages. Chances are that a site with parallel texts  will contain such links in some of its documents.  This fact is used as the criterion in searching for  candidate sites.  Therefore, to determine possible sites for English-  Chinese parallel texts, we can request an English  document containing the following anchor:  anchor : \"engl ish version H \\[\"in english\", ...\\].  Similar requests are sent for Chinese documents.  From the two sets of pages obtained by the above  queries we extract wo sets of Web sites. The union  of these two sets constitutes then the candidate sites.  That  is to say, a site is a candidate site when it  is found to have either an English page linking to  its Chinese version or a Chinese page linking to its  English version.  2.2 File Name Fetching  We now assume that a pair of parallel texts exists on  the same site. To search for parallel pairs on a site,  PTMiner first has to obtain all (or at least part of)  the HTML file names on the site. From these names  pairs are scanned. It is possible to use a Web crawler  to explore the candidate sites completely. However,  we can take advantage of the search engines again to  accelerate the process. As the first step, we submit  the following query to the search engines:  host : hostname  to fetch the Web pages that they indexed from this  site. If we only require a small amount of parallel  texts, this result may be sufficient. For our purpose,  however, we need to explore the sites more thor-  oughly using a host crawler. Therefore, we continue  our search for files with a host crawler which uses  the documents found by the search engines as the  starting point.  2.3 Host Crawling  A host crawler is slightly different from a Web  crawler. Web crawlers go through innumerable  pages and hosts on the Web. A host crawler is a  Web crawler that crawls through documents on a  given host only. A breadth-first crawling algorithm  is applied in PTMiner as host crawler. The principle  is that when a link to an unexplored ocument on  the same site is found in a document, it is added to  a list that will be explored later. In this way, most  file names from the candidate sites are obtained.  2.4 Pair Scan  After collecting file names for each candidate site,  the next task is to determine the parallel pairs.  Again, we try to use some heuristic rules to guess  which files may be parallel texts before downloading  them. The rules are based on external features of  the documents. By external feature, we mean those  features which may be known without analyzing the  contents of the file, such as its URL, size, and date.  This is in contrast with the internal features, such as  \u001b[1;32;40m language \u001b[0;0m, character set, and HTML structure, which  cannot be known until we have downloaded the page  and analyzed its contents.  The heuristic criterion comes from the following  observation: We observe that parallel text pairs usu-  ally have similar name patterns. The difference be-  tween the names of two parailel pages usually lies  in a segment which indicates the \u001b[1;32;40m language \u001b[0;0m. For ex-  ample, \"file-ch.html\" (in Chinese) vs. \"file-en.html\"  (in English). The difference may also appear in the  path, such as \".../chinese/.../fi le.html\" vs. \".../en-  glish/.../f i le.html'. The name patterns described  above are commonly used by webmasters to help or-  ganize their sites. Hence, we can suppose that a  pair of pages with this kind of pattern are probably  parallel texts.  22 First, we establish four lists for English pre-  fixes, English suffixes, Chinese prefixes and Chi-  nese suffixes. For example: Engl ish P re f ix  =  {e, en, e_, en_, e - ,  en - ,  ...}. For each file in one lan-  guage, if a segment in its name corresponds to one  of the \u001b[1;32;40m language \u001b[0;0m affixes, several new names are gener-  ated by changing the segment to the possible corre-  sponding affixes of the other \u001b[1;32;40m language \u001b[0;0m. If a generated  name corresponds to an existing file, then the file is  considered as a candidate parallel document of the  original file.  2.5 Filtering  Next, we further examine the contents of the paired  files to determine if they are really parallel according  to various external and internal features. This may  further improve the pairing precision. The following  methods have been implemented in our system.  2.5.1 Text Length  Parallel files often have similar file lengths. One sim-  ple way to filter out incorrect pairs is to compare  the lengths of the two files. The only problem is to  set a reasonable threshold that will not discard too  many good pairs, i.e. balance recall and precision.  The usual difference ratio depends on the \u001b[1;32;40m language \u001b[0;0m  pairs we are dealing with. For example, Chinese-  English parallel texts usually have a larger differ-  ence ratio than English-French parallel texts. The  filtering threshold had to be determined empirically,  from the actual observations. For Chinese-English,  a difference up to 50% is tolerated.  2.5.2 Language and  Character Set  It is also obvious that the two files of a pair have  to be in the two \u001b[1;32;40m language \u001b[0;0ms of interest. By auto-  matically identifying \u001b[1;32;40m language \u001b[0;0m and character set, we  can filter out the pairs that do not satisfy this basic  criterion. Some Web pages explicitly indicate the  \u001b[1;32;40m language \u001b[0;0m and the character set. More often such  information is omitted by authors. We need some  \u001b[1;32;40m language \u001b[0;0m identification tool for this task.  SILC is a \u001b[1;32;40m language \u001b[0;0m and encoding identification  system developed by the RALI laboratory at the  University of Montreal. It employs a probabilistic  model estimated on tri-grams. Using these mod-  els, the system is able to determine the most proba-  ble \u001b[1;32;40m language \u001b[0;0m and encoding of a text (Isabelle et al.,  1997).  2.5.3 HTML Structure and Alignment  In the STRAND system (Resnik, 1998), the candi-  date pairs are evaluated by aligning them according  to their HTML structures and computing confidence  values. Pairs are assumed to be wrong if they have  too many mismatching markups or low confidence  values.  Comparing HTML structures seems to be a sound  way to evaluate candidate pairs since parallel pairs  usually have similar HTML structures. However, we  also noticed that parallel texts may have quite dif-  ferent HTML structures. One of the reasons is that  the two files may be created using two HTML ed-  itors. For example, one may be used for English  and another for Chinese, depending on the \u001b[1;32;40m language \u001b[0;0m  handling capability of the editors. Therefore, cau-  tion is required when measuring structure difference  numerically.  Parallel text alignment is still an experimental  area. Measuring the confidence values of an align-  ment is even more complicated. For example, the  alignment algorithm we used in the training of the  statistical translation model produces acceptable  alignment results but it does not provide a confi-  dence value that we can \"confidently\" use as an eval-  uation criterion. So, for the moment his criterion is  not used in candidate pair evaluation.  3 Generated  Corpus  and Trans la t ion   Mode l  Tra in ing   In this section, we describe the results of our parallel  text mining and translation model training.  3.1 The Corpus  Using the above approach for Chinese-English, 185  candidate sites were searched from the domain hk.  We limited the mining domain to hk because Hong  Kong is a bilingual English-Chinese city where high  quality parallel Web sites exist. Because of the small  number of candidate sites, the host crawler was used  to thoroughly explore each site. The resulting cor-  pus contains 14820 pairs of texts including 117.2Mb  Chinese texts and 136.5Mb English texts. The entire  mining process lasted about a week. Using length  comparison and \u001b[1;32;40m language \u001b[0;0m identification, we refined  the precision of the corpus to about 90%. The preci-  sion is estimated by examining 367 randomly picked  pairs.  3.2 Statistical Translation Model  Many approaches in computational linguistics try to  extract ranslation knowledge from previous trans-  lation examples. Most work of this kind establishes  probabilistic models from parallel corpora. Based  on one of the statistical models proposed by Brown  et al. (1993), the basic principle of our translation  model is the following: given a corpus of aligned sen-  tences, if two words often co-occur in the source and  target sentences, there is a good likelihood that they  are translations of each other. In the simplest case  (model 1), the model earns the probability, p(tls), of  having a word t in the translation of a sentence con-  taining a word s. For an input sentence, the model  then calculates a sequence of words that are most  probable to appear in its translation. Using a sim-  ilar statistical model, Wu (1995) extracted a large-  scale English-Chinese l xicon from the HKUST cor-  23   <s id=\"00~\">  <HTML> <HEAD>  <META HTrP-EQUIV=\"Content-type\"  CONTENT=\"text/html; charset--iso-8859-1\">  <META HTI'P-EQUIV=\"Content-\u001b[1;32;40m language \u001b[0;0m\"  CONTENT=\"Western\">  </s>  <s id=\"0001\">  <TITLE>Journal of Primary Education 1996,  VoI., No. l&2, pp. 19-27 </TITLE>  </HEAD>  </s>  <s id=\"0002\">  <BODY BACKGROUND=\".Jgif/pejbg.jpg\"  TEXT=\"#000(3(O\" BGCOLOR=\"#ffffff\">  <CENTER>  </s>  <s id=\"0003\">  <HI>Journal of Primary Education </HI>  </s>  <s id=\"0004\">  <HR> <B>Volume 6, No l&2, pp. 19-27 (May,  1996) </B> <HR>  </s>  <s id=\"0005\">  <H3>Principles for Redesigning Teacher  Education </H3> Alan TOM </CENTER>  </s>  <s id=\"0006\">  <P> <B> <I> Abstract </I> </B>  </s>  <s id=\"0000\">  <HTML> <HEAD>  <META H'ITP-EQUW=\"Content-type\"  CONTENT=\"text/html; charset=bigS\">  <META HTTP-EQUIV=\"Content-\u001b[1;32;40m language \u001b[0;0m\"  CONTENT=\"zh\">  <Is>  <s id=\"0001\">  <TITLE> Journal of Primary Education 1996,  Vol., No. l&2, Page 19-27 </TITLE>  </HEAD>  </s>  <s id=\"0002\">  <BODY BACKGROUND=\".Jgif/pejbg.jpg\"  TEXT=\"#000000\" BGCOLOR=\"#ffffff\"> <A  HREF=\"/erdpej/b2g__pej.phtml?URL=%2fen%2fp  ej%2f0601%2f0601019c.htm\">  <IMG SRC=\"/en/gif/kan.gif\" ALT=\"~\"   BORDER=0 ALIGN=R IGHT> </A> <CENTER>  </s>  <s id=\"0003\">  <H2>~ ~ 11I ~ O.</H2>  </s>  <s id=\"0004\">  <HR> (~:~h-fv-c?.JLJl) ~,-\\]'~..  </s>  <s id=\"0005\">  ~ 19-27\\]~ <I-1R>  </s>  Figure 2: An alignment example using pure length-based method.  pus which is built manually. In our case, the prob-  abilistic translation model will be used for CLIR.  The requirement on our translation model may be  less demanding: it is not absolutely necessary that  a word t with high p(tls ) always be a true trans-  lation of s. It is still useful if t is strongly related  to s. For example, although \"railway\" is not a true  translation of \"train\" (in French), it is highly useful  to include \"railway\" in the translation of a query on  \"train\". This is one of the reasons why we think a  less controlled parallel corpus can be used to train a  translation model for CLIR.  3.3 Parallel Text Al ignment  Before the mined documents can be aligned into par-  allel sentences, the raw texts have to undergo a se-  ries of some preprocessing, which, to some extent, is  \u001b[1;32;40m language \u001b[0;0m dependent. For example, the major opera-  tions on the Chinese-English corpus include encod-  ing scheme transformation (for Chinese), sentence  level segmentation, parallel text alignment, Chinese  word segmentation (Nie et al., 1999) and English  expression extraction.  The parallel Web pages we collected from vari-  ous sites are not all of the same quality. Some are  highly parallel and easy to align while others can be  very noisy. Aligning English-Chinese parallel texts  is already very difficult because of the great differ-  ences in the syntactic structures and writing sys-  tems of the two \u001b[1;32;40m language \u001b[0;0ms. A number of alignment  techniques have been proposed, varying from statis-  tical methods (Brown et al., 1991; Gale and Church,  1991) to lexical methods (Kay and RSscheisen, 1993;  Chen, 1993). The method we adopted is that of  Simard et al. (1992). Because it considers both  length similarity and cognateness as alignment cri-  teria, the method is more robust and better able  to deal with noise than pure length-based methods.  Cognates are identical sequences of characters in cor-  responding words in two \u001b[1;32;40m language \u001b[0;0ms. They are com-  monly found in English and French. In the case of  English-Chinese alignment, where there are no cog-  nates shared by the two \u001b[1;32;40m language \u001b[0;0ms, only the HTML  markup in both texts are taken as cognates. Be-  cause the HTML structures of parallel pages are nor-  mally similar, the markup was found to be helpful  for alignment.  To illustrate how markup can help with the align-  ment, we align the same pair with both the pure  length-based method of Gale & Church (Fig. 2),  and the method of Simard et al. (Fig. 3). First of  all, we observe from the figures that the two texts are  24 <s id=\"0000\">  <HTML> <HEAD>  <META HTTP-EQUIV=\"Content-type\"  CONTENT=\"text/html; charset=iso-8859-1 \">  <META HTTP-EQUIV=\"Content-\u001b[1;32;40m language \u001b[0;0m\"  CONTENT=\"Westem\">  </s>  <s id=\"0001\">  <TITLE>Journal of Primary Education 1996,  Vol., No. l&2, pp. 19-27 </TITLE>  </HEAD>  </s>  <s id=\"0002\">  <BODY BACKGROUND=-\". Jgif/pejbg.jpg\"  TEXT=\"#000000\" BGCOLOR=\"#ffffff\">  <CENTER>  </s>  <s id=\"0003\">  <H 1 >Journal of Primary Education </H 1 >  <Is>  <s id=\"0004\">  <HR> <B>Volume 6,No l&2, pp. 19-27 (May,  1996) </B> <HR>  </$>  <s id=\"0000\">  <HTML> <HEAD>  <META HTrP-EQUIV=\"Content-type\"  CONTENT=\"text/html; charset=big5\">  <META H'lTP-EQUIV=\"Content-\u001b[1;32;40m language \u001b[0;0m\"  CONTENT=\"zh\">  <Is>  <s id=\"0001\">  :<TITLE> Journal of Primary Education 1996,  Vol., No. l&2, Page 19-27 </TITLE>  </HEAD>  </s>  <s id=\"0002\">  <BODY BACKGROUND=-\". Jgiffpejbg.jpg\"  TEXT=\"#O00000\" BGCOLOR=\"#fffffff> <A  HREF=\"/ergpej/b2g_pej.phtml?URL=%2fen%2fp  ej %2f0601%2 f0601019c.htm\">  <IMG SRC=\"/erdgif/kan.gif\" ALT=\"~k\"  BORDER={) ALIGN=R IGHT> </A> <CEHTEIL~  </s>  <s id=\"0003\">  <H2>~k ~ ~ ~\\[1.</H2>  </s>  <s id=\"0004\">  <HR> (~t~-~-#cJL.~) ,-~~.  </s>  <s id=\"0005\">  ~ $ ~  19-27 \\]~ <HR>  <\\]s>  <s id=\"0005\"> <s id=\"0006\">  <H3>Principles for Redesigning Teacher <H3>.~ k~4Vt ~'~ ~ ~J </H3> Alan TOM  Education </H3> Alan TOM </CENTER> </CENTER>  <Is> <Is>  <s id=\"0006\"> <s id=\"0007\">  <P> <B> <I> Abstract </I> </B> <P> <I> <B> ~4\\[- </B> </I> <P>  </s> </s>  Figure 3: An alignment example considering cognates.  divided into sentences. The sentences are marked by  <s id=\"xxxx\"> and </s>.  Note that we determine  sentences not only by periods, but also by means of  HTML markup.  We further notice that it is difficult to align sen-  tences 0002. The sentence in the Chinese page is  much longer than its counterpart in the English page  because some additional information (font) is added.  The length-based method thus tends to take sen-  tence 0002, 0003, and 0004 in the English page as  the translation of sentence 0002 in the Chinese page  (Fig. 2), which is wrong. This in turn provocated  the three following incorrect alignments. As we can  see in Fig. 3, the cognate method did not make the  same mistake because of the noise in sentence 0002.  Despite their large length difference, the two 0002  sentences are still aligned as a 1-1 pair, because the  sentences in the following 4 alignments (0003 - 0003;  0004 - 0004, 0005; 0005 - 0006; 0006 - 0007) have  rather similar HTML markups and are taken by the  program to be the most likely alignments.  Beside HTML markups, other criteria may also  be incorporated. For example, it would be helpful  to consider strong correspondence b tween certain  English and Chinese words, as in (Wu, 1994). We  hope to implement such correspondences in our fu-  ture research.  3.4 Lex icon  Eva luat ion   To evaluate the precision of the English-Chinese  translation model trained on the Web corpus, we  examined two sample lexicons of 200 words, one in  each direction. The 200 words for each lexicon were  randomly selected from the training source. We ex-  amined the most probable translation for each word.  The Chinese-English lexicon was found to have a  precision of 77%. The English-Chinese l xicon has  a higher precision of 81.5%. Part of the lexicons  are shown in Fig. 4, where t / f  indicates whether a  translation is true or false.  These precisions seem to be reasonably high.  They are quite comparable to that obtained by Wu  (1994) using a manual Chinese-English parallel cor-  pus.  3.5 Effect  o f  S topwords   We also found that stop-lists have significant effect  on the translation model. Stop-list is a set of the  most frequent words that we remove from the train-  2fi  English word  a .n l .   access  adaptation  add  adopt  agent  agree  airline  amendment  , appliance  apply  attendance  auditor  - ,average  base_on  t/f  t  f  t  t  t  t  t  t  t  t  t  t  f  t  f  Translmion Probability Chinese word  ~'~- 0.201472 ~t l :   ~\"  0.071705 \"~\"   ~f~.,~ 0.179633 JllL~  0.317435  ~ 0.231637 ~.~  1~tA~ 0.224902 4J~'~  0.36569  0.344001  0.367518  J~ 4~ 0.136319  i~.~I 0.19448 J~   ~',1~ 0.171769 ,~- JJ~  *~ 0.15011 -~-~  ~- ~ 0.467646 * *~  0.107304  Figure 4: Part of the evaluation lexicons.  t/f  t  t  t  t  t  f  t  f  t  t  t  t  t  t  t  Translation Probability  office 0.375868  protection 0.343071  report 0.358592  prepare 0.189513  loca l  0.421837  follow 0.023685  standard 0.445453  adu l t  0.044959  inadequate 0.093012  part 0.313676  financial 0.16608  visit 0.309642  bill 0.401997  vehicle 0.467034  saving 0.176695  Figure 5: Effect of stop lists in C-E translation.  ing source. Because these words exist in most align-  ments, the statistical model cannot derive correct  translations for them. More importantly, their ex-  istence greatly affects the accuracy of other transla-  tions. They can be taken as translations for many  words.  A priori, it would seem that both the English and  Chinese stop-lists hould be applied to eliminate the  noise caused by them. Interestingly, from our ob-  servation and analysis we concluded that for better  precision, only the stop-list of the target \u001b[1;32;40m language \u001b[0;0m  should be applied in the model training.  We first explain why the stop-list of the target lan-  guage has to be applied. On the left side of Fig. 5,  if the Chinese word C exists in the same alignments  with the English word E more than any other Chi-  nese words, C will be the most probable translation  for E. Because of their frequent appearance, some  Chinese stopwords may have more chances to be in  the same alignments with E. The probability of the  translation E --+ C is then reduced (maybe ven less  than those of the incorrect ones). This is the reason  why many English words are translated to \"~ '  (of)  by the translation model trained without using the  Chinese stop-list.  We also found that it is not necessary to remove  the stopwords of the source \u001b[1;32;40m language \u001b[0;0m. In fact, as il-  lustrated on the right side of Fig. 5, the existence of  the English stopwords has two effects on the proba-  bility of the translation E -~ C:  1 They may often be found together with the Chi-  nese word C. Owing to the Expectation Maxi-  mization algorithm, the probability of E -~ C  may therefore be reduced.  2 On the other hand, there is a greater likelihood  that English stopwords will be found together  with the most frequent Chinese words. Here,  we use the term \"Chinese frequent words\" in-  stead of \"Chinese stopwords\" because ven if a  stop-list is applied, there may still remain some  common words that have the same effect as the  stopwords. The coexistence ofEnglish and Chi-  nese frequent words reduces the probability that  the Chinese frequent words are the translations  of E, and thus raise the probability of E -+ C.  The second effect was found to be more signifi-  cant than the first, since the model trained without  the English stopwords has better precision than the  model trained with the English stopwords. For the  correct ranslations given by both models, the model  26 Mono-Lingual IR  Translation Model  Dictionary  C-E CLIR  0.3861  0.1504 (39.0%mono)  0.1530 (39.6%mono)  0.2583 (66.9%mono)  E-C CLIR  0.3976  0.1841 (46.3%mono)  0.1427 (35.9%mono)  0.2232 (56.1%mono)  Table 1: CLIR results.  trained without considering the English stopwords  gives higher probabilities.  4 Eng l i sh -Ch inese  CL IR  Resu l ts   Our final goal was to test the performance of the  translation models trained on the Web parallel cor-  pora in CLIR. We conducted CLIR experiments u -  ing the Smart IR system.  4.1 Results   The English test corpus (for C-E CLIR) was the  AP corpus used in TREC6 and TREC7. The short  English queries were translated manually into Chi-  nese and then translated back to English by the  translation model. The Chinese test corpus was the  one used in the TREC5 and TREC6 Chinese track.  It contains both Chinese queries and their English  translations.  Our experiments on these two corpora produced  the results hown in Tab. 1. The precision of mono-  lingual IR is given as benchmark. In both E-C and  C-E CLIR, the translation model achieved around  40% of monolingual precision. To compare with the  dictionary-based approach, we employed a Chinese-  English dictionary, CEDICT (Denisowski, 1999),  and an English-Chinese online dictionary (Anony-  mous, 1999a) to translate queries. For each word  of the source query, all the possible translations  given by the dictionary are included in the translated  query. The Chinese-English dictionary has about  the same performace as the translation model, while  the English-Chinese dictionary has lower precision  than that of the translation model.  We also tried to combine the translations given by  the translation model and the dictionary. In both  C-E and E-C CLIR, significant improvements were  achieved (as shown in Tab. 1). The improvements  show that the translations given by the translation  model and the dictionary complement each other  well for IR purposes. The translation model may  give either exact ranslations orincorrect but related  words. Even though these words are not correct in  the sense of translation, they are very possibly re-  lated to the subject of the query and thus helpful  for IR purposes. The dictionary-based approach ex-  pands a query along another dimension. It gives  all the possible translations for each word including  those that are missed by the translation model.  4.2 Comparison Wi th  MT Systems  One advantage of a parallel text-based translation  model is that it is easier to build than an MT system.  Now that we have examined the CLIR performance  of the translation model, we will compare it with  two existing MT systems. Both systems were tested  in E-C CLIR.  4.2.1 Sunshine WebTran Server  Using the Sunshine WebTran server (Anonymous,  1999b), an online Engiish-Chinese MT system, to  translate the 54 English queries, we obtained an  average precision of 0.2001, which is 50.3% of the  mono-lingual precision. The precision is higher than  that obtained using the translation model (0.1804)  or the dictionary (0.1427) alone, but lower than the  precison obtained using them together (0.2232).  4.2.2 Transperfect  Kwok (1999) investigated the CLIR performance of an English-Chinese MT software called Transper-  fect, using the same TREC Chinese collection as we  used in this study. Using the MT software alone,  Kwok achieved 56% of monolingual precision. The  precision is improved to 62% by refining the trans-  lation with a dictionary. Kwok also adopted pre-  translation query expansion, which further improved  the precison to 70% of the monolingual results.  In our case, the best E-C CLIR precison using the  translation model (and dictionary) is 56.1%. It is  lower than what Kwok achieved using Transperfect,  however, the difference is not large.  4.3 Further  Problems  The Chinese-English translation model has a fax  lower CLIR performance than that of the English-  French model established using the same method  (Nie et al., 1999). The principal reason for this is the  fact that English and Chinese are much more differ-  ent than English and French. This problem surfaced  in many phases of this work, from text alignment to  query translation. Below, we list some further fac-  tors affecting CLIR precision.   The Web-collected corpus is noisy and it is dif-  ficult to align English-Chinese t xts. The align-  ment method we employed has performed more  poorly than on English-French alignment. This  in turn leads to poorer performance of the trans-  lation model. In general, we observe a higher  27  variability in Chinese-English translations than  in English-French translations.   For E-C CLIR, although queries in both lan-  guages were provided, the English queries were  not strictly translated from the original Chi-  nese ones. For example, A Jg ,~ (human right  situation) was translated into human right is-  sue. We cannot expect he translation model  to translate issue back to ~ (situation).   The training source and the CLIR collections  were from different domains. The Web cor-  pus are retrieved from the parallel sites in Hong  Kong while the Chinese collection is from Peo-  ple's Daily and Xinhua News Agency, which are  published in mainland China. As the result,  some important erms such as ~$ $ (most-  favored-nation) and --- I!! ~ ~ (one-nation-two-  systems) in the collection are not known by the  model.  5 Summary   The goal of this work was to investigate he feasibil-  ity of using a statistical translation model trained on  a Web-collected corpus to do English-Chinese CLIR.  In this paper, we have described the algorithm and  implementation we used for parallel text mining,  translation model training, and some results we ob-  tained in CLIR experiments. Although further work  remains to be done, we can conclude that it is pos-  sible to automatically construct a Chinese-English  parallel corpus from the Web. The current system  can be easily adapted to other \u001b[1;32;40m language \u001b[0;0m pairs. De-  spite the noisy nature of the corpus and the great  difference in the \u001b[1;32;40m language \u001b[0;0ms, the evaluation lexicons  generated by the translation model produced accept-  able precision. While the current CLIR results are  not as encouraging asthose of English-French CLIR,  they could be improved in various ways, such as im-  proving the alignment method by adapting cognate  definitions to HTML markup, incorporating a lexi-  con and/or removing some common function words  in translated queries.  We hope to be able to demonstrate in the near  future that a fine-tuned English-Chinese translation  model can provide query translations for CLIR with  the same quality produced by MT systems.  Re ferences   Anonymous. 1999a. Sunrain.net - English-Chinese  dictionary, http://sunrain.net/r_ecdict _e.htm.  Anonymous. 1999b. Sunshine WebTran server.  http://www.readworld.com/translate.htm.  P. F. Brown, J. C. Lai, and R. L. Mercer. 1991.  Aligning sentences in parallel corpora. In 29th  Annual Meeting of the Association for Computa-  tional Linguistics, pages 89-94, Berkeley, Calif.  P. F. Brown, S. A. Della Pietra, V. J. Della Pietra,  and R. L. Mercer. 1993. The mathematics of ma-  chine translation: Parameter estimation. Compu-  tational Linguistics, 19:263-311.  S. F. Chen. 1993. Aligning sentences in bilingual  corpora using lexical information. In Proceedings  of the 31th Annual Meeting of the Association for  Computational Linguistics, pages 9-16, Colum-  bus, Ohio.  Paul Denisowski. 1999. Cedict (chinese-english dic-  tionary) project, http://www.mindspring.com/  paul_denisowski/cedict.html.  William A. Gale and Kenneth W. Church. 1991. A  program for aligning sentences in bilingual cor-  pora. In Proceedings of the 29th Annual Meeting  of the Association for Computational Linguistics,  pages 177-184, Berkeley, Calif.  P. Isabelle, G. Foster, and P. Plamondon.  1997. SILC: un syst~me d'identification  de la langue et du codage, http://www-  rali.iro.umontreal.ca/ProjetSILC.en.html.  M. Kay and M. RSscheisen. 1993. Text-translation  alignment. Computational Linguistics, 19:121-  142.  K. L. Kwok. 1999. English-chinese cross-\u001b[1;32;40m language \u001b[0;0m  retrieval based on a translation package. In Work-  shop of Machine Translation for Cross Language  Information Retrieval, Machine Translation Sum-  mit VII, Singapore.  P. Langlais, G. Foster, and G. Lapalme. 2000. Unit  completion for a computer-aided translation typ-  ing system. In Applied Natural Language Pro-  cessing Conference (ANLP), Seattle, Washington,  May.  Jianyun Nie, Michel Simard, Pierre Isabelle, and  Richard Durand. 1999. Cross-\u001b[1;32;40m language \u001b[0;0m informa-  tion retrieval based on parallel texts and auto-  matic mining parallel texts from the Web. In  ACM SIGIR '99, pages 74-81, August.  Philip Resnik. 1998. Parallel stands: A preliminary  investigation i to mining the Web for bilingual  text. In AMTA '98, October.  Michel Simard, George F. Foster, and Pierre Is-  abelle. 1992. Using cognates to align sentences  in bilingual corpora. In Proceedings of TMI-92,  Montreal, Quebec.  Dekai Wu. 1994. Aligning a parallel English-  Chinese corpus statistically with lexical criteria.  In ACL-9$: 32nd Annual Meeting of the Assoc.  for Computational Linguistics, pages 80-87, Las  Cruces, NM, June.  Dekai Wu. 1995. Large-scale automatic extraction  of an English-Chinese l xicon. Machine Transla-  tion, 9(3-4):285-313.  28  \n",
            "-----------------------------\n",
            "--- document 18: DP: A Detector for Presuppositions in survey questions  Katja WIEMER-HASTINGS  Psychology Department / Institute for Intelligent  Systems  University of Memphis  Memphis, TN 38152  kwiemer @ latte.memphis.edu  Peter WIEMER-HASTINGS  Human Communication Research Centre  University of Edinburgh  2 Buccleuch Place  Edinburgh EH8 9LW, UK  peterwh@cogsci.ed.ac.uk  Sonya RAJAN, Art GRAESSER, Roger KREUZ, & Ashish KARNAVAT  Institute for Intelligent Systems, University of Memphis, Memphis, TN 38152  sonyarajan@hotmail.com, graesser@memphis.edu, rkreuz@memphis.edu, akarnavat@hotmail.com  Abstract  This paper describes and evaluates a detector  of presuppositions (DP) for survey questions.  Incorrect presuppositions can make it  difficult to answer a question correctly.  Since they can be difficult to detect, DP is a  useful tool for questionnaire designer. DP  performs well using local characteristics of presuppositions. It reports the presupposition  to the survey methodologist who can  determine whether the presupposition is  valid.  Introduction  Presuppositions are propositions that take some  information as given, or as \"the logical  assumptions underlying utterances\" (Dijkstra &  de Smedt, 1996, p. 255; for a general overview,  see McCawley, 1981). Presupposed information  includes state of affairs, such as being married;  events., such as a graduation; possessions, uch as  a house, children, knowledge about something;  and others. For example, the question, \"when did  you graduate from college\", presupposes the  event that the respondent did in fact graduate  from college. The answer options may be ranges  of years, such as \"between 1970 and 1980\".  Someone who has never attended college can  either not respond at all, or give a random (and  false) reply. Thus, incorrect presuppositions  cause two problems. First, the question is  difficult o answer. Second, assuming that people  feel obliged to answer them anyway, their  answers present false information. This biases  survey statistics, or, in an extreme case, makes  them useless.  The detector for presuppositions (DP) is part of the  computer tool QUAID (Graesser, Wiemer-  Hastings, Kreuz, Wiemer-Hastings & Marquis, in  press), which helps survey methodologists design  questions that are easy to process. DP detects a  presupposition and reports it to the survey  methodologist, who can examine if the  presupposition is correct. QUAID is a  computerized QUEST questionnaire valuation  aid. It is based on QUEST (Graesser & Franklin,  1990), a computational model of the cognitive  processes underlying human question answering.  QUAID critiques questions with respect to  unfamiliar technical terms, vague terms, working  memory overload, complex syntax, incorrect  presuppositions, and unclear question purpose or  category. These problems are a subset of potential  problems that have been identified by Graesser,  Bommareddy, Swamer, and Golding (1996; see  also Graesser, Kennedy, Wiemer-Hastings &  Ottati, 1999).  QUAID performs reliably on the first five problem  categories. In comparison to these five problems,  presupposition detection is even more challenging.  For unfamiliar technical terms, for example,  QUAID reports words with frequencies below a  certain threshold. Such an elegant solution is  impossible for presuppositions. Their forms vary  widely across presupposition types. Therefore,  their detection requires a complex set of rules,  carefully tuned to identify a variety of  presupposition problems. DP prints out the  90  presuppositions of a question, and relies on the  survey methodologist to make the final decision  whether the presuppositions are valid.  1 How to detect presuppositions  We conducted a content analysis of questions  with presupposition problems to construct a list  of indicators for presuppositions. 22 questions  containing problematic presuppositions were  selected from a corpus of 550 questions, taken  from questionnaires provided by the U.S. Census  Bureau. The 22 questions were identified based  on ratings by three human expert raters. It may  seem that this problem is infrequent, but then,  these questions are part of commonly used  questionnaires that have been designed and  revised very thoughtfully.  Additionally, we randomly selected a contrast  question sample of 22 questions rated  unproblematic with regard to incorrect  presuppositions by all three raters. Examples (1)  and (2) are questions rated as problematic by at  least two raters; examples (3) and (4) present  questions that do not contain presuppositions.  (1) Is that the same place you USUALLY go  when you need routine or preventive care, such as  a physical examination or check up?  (2) How much do your parents or parent know  about your close friends' parents?  (3) From date to December 31, did you take one  or more trips or outings in the United States, of at  least one mile, for the PRIMARY purpose of  observing, photographing, orfeeding wildlife?  (4) Are you now on full-time active duty with the  armed forces?  Example (1) presupposes the habit of making  use of routine / preventive care; (2)  presupposes that the respondent has close  friends.  As stated above, incorrect presuppositions are  infrequent in well-designed questionnaires. For  example, questions about details of somebody's  marriage are usually preceded by a question  establishing the person's marital status.  In spite of this, providing feedback about  presuppositions to the survey methodologist is useful. Importantly, QUAID is designed to aid in  the design process. Consider a survey on health-  related issues. In the context of this topic, a  survey methodologist may be interested in how  many days of work a person missed because of  illness, but not think about whether the person  actually has a job. Upon entering the question  \"how many days of work did you miss last year  because of illness\" into the QUAID tool, DP  would report that the question presupposes  employment. The survey methodologist could  then insert a question about employment.  Second, there are subtle presuppositions that may  go undetected even by a skilled survey designer.  These are presuppositions about things that are  likely (but not necessarily) true. For example, a question may inquire about a person's close  friends (presupposing close friends) or someone's  standard place for preventive care (presupposing  the habit of making use of preventive care). DP  does not know which presuppositions are likely to  be valid or invalid, and is therefore more likely to  detect such subtle incorrect presuppositions than a  human expert.  1.1 The presupposition detector (DP)  We constructed a set of presupposition detection  rules based on the content analysis. The rules use  a wide range of linguistic information about the  input sentences, including particular words (such  as \"why\"), part of speech categories (e.g., wh-  pronoun), and complex syntactic subtrees (such as  a quantification clause, followed by a noun  phrase).  1.1.1 The syntactic analysis component  We used Eric Brill's rule-based word tagger (1992,  1994a, 1994b), the de facto state of the art tagging  system, to break the questions down into part-of-  speech categories. Brill's tagger produces a single  lexical category for each word in a sentence by  first assigning tags based on the frequency of  occurrence of the word in that category, and then  applying a set of context-based re-tagging rules.  The tagged text was then passed on to Abney's  SCOL/CASS system (1996a, 1996b), an extreme  bottom-up parser. It is designed to avoid  ambiguity problems by applying rammar rules on  a level-by-level basis. Each level contains rules  that will only fire if they are correct with high  probability. Once the parse moves on to a higher  level, it will not attempt to apply lower-level rules.  In this way, the parser identifies chunks of  information, which it can be reasonably certain are  91  connected, even when it cannot create a complete  parse of a sentence.  1.1.2 The presupposition i dicators  The indicators for presuppositions were tested  against questions rated as \"unproblematic\" to  eliminate items that failed to discriminate  questions with versus without presuppositions.  We constructed a second list of indicators that  detect questions containing no presuppositions.  All indicators are listed in Table 1. These lists  are certainly far from complete, but they present a good basis for evaluating of  how well  presuppositions can be detected by an NLP  system. These rules were integrated into a  decision tree structure, as illustrated in Figure 1.  Table 1: Indicators of absence or presence  presuppositions  First word(s)  Presupposition No presupposition  When VP Initial or following  What time comma:  Who VP - is there  Why - are there  How much  How many Does / do NP have ...  How often etc. Will NP have ...  How VP Has / Have NP ...  Where V NP Is / are NP ...  Keywords usually ever  Possessives: any  mine, yours, anybody  NP's anything  while whether  Indexicals: if  this, these, such could, would  Specific V infinitive  constructions when NP  of  YES  I  Are indicators present hat question \\[  does not contain presuppositon? I  No/  /  Are indicators present hat question  contains a presupposition?  Is indicator reliable?  J  Figure 1 : The DP decision structure tree  92  1.2 Classifying presuppositions  Different types of presuppositions can be  distinguished based on particular indicators.  Examples for presupposition types, such as  events or possessions, were mentioned above.  Table 2 presents an exhaustive overview of  presupposition types identified in our analysis.  Note that some indicators can point to more than  one type of presupposition.  Table 2 : Classification of presupposition based on  indicators. In the right column, expressions in  parentheses identify the presupposed unit.  Indicator  \"how often\" ...VP  \"how\" aux NP VP  \"while\"... VP  \"where\"... VP  \"why\"... VP  Presupposition type: The  question presupposes...  an action (V)  \"usually\"... VP  \"how often\",  \"frequently\", etc.  a habit (V)  \"how many\" NP  \"where is\" NP  Indexicals:  \"this\" / \"that\" NP  \"these\" / \"those\" NP  \"such a(n)\" NP  an entity: object, state, or  person (NP)  a shared referent orcommon  ground (NP)  \"how much\" NP ...  \"how much does\" NP  \"know\"  \"how many\" NP ...  Possessive pronouns  Apostrophe 's': NP's  a possession (NP);  exception list: NP's that can be  presupposed (name, age, etc.)  \"why\" S a state of affairs, fact, or  assertion (S)  VP infinitive an intention / a goal (infinitive /  \"why\" VP NP NP VP)  \"who\" VP  \"When\" VP  ...\"when\" NP VP  an a~ent (A person who VP)  an event (VP)  DP reports when a presupposition is present, and  it also indicates the type of presupposition that is  made (e.g., a common ground presupposition or the presupposition f a habit) in order to point the  question designer to the potential presupposition  error. DP uses the expressions in the right  column in Table 2, selected in accordance with  the indicators, and fills them into the brackets in  its output (see Figure 1). For example, given the  question \"How old is your child?\", DP would  detect the possessive pronoun \"your\", and  accordingly respond: \"It looks like you are  presupposing a possession (child). Make sure that  the presupposition is correct by consulting the  previous questions.\"  2 Evaluation  In this section, we report summary statistics for  the human ratings of our test questions and the  measures we computed based on these ratings to  evaluate DP's performance.  2.1 Human ratings  We used human ratings as the standard against  which to evaluate the performance of DP. Three  raters rated about 90 questions from 12  questionnaires provided by the Census Bureau.  DP currently does not use context. To have a fair  test of its performance, the questions were  presented to the human raters out of context, and  they were instructed to rate them as isolated  questions. Ratings were made on a four-point  scale, indicating whether the question contained  no presupposition (1), probably contained no  presupposition (2), probably contained a  presupposition (3), or definitely contained a  presupposition (4). We transformed the ratings  into Boolean ratings by combining ratings of 1 and  2 (\"no problem\") versus ratings of 3 and 4  (\"problem\"). We obtained very similar results for  analyses of the ratings based on the four-point and  the Boolean scale. For simplicity, we just report  the results for the Boolean scale.  2.2 Agreement among the raters  We evaluated the agreement among the raters with  three measures: correlations, Cohen's kappa, and  percent agreement. Correlations were significant  only between two raters (r = 0. 41); the  correlations of these two with the third rater  produced non-significant correlations, indicating  that the third rater may have used a different  strategy. The kappa scores, similarly, were  significant only for two raters (_k_ = 0.36). In terms  of percent agreement, he raters with correlated  ratings agreed in 67% of the cases. The  percentages of agreement with rater 3 were 57%  and 56%, respectively.  DP ratings were significantly correlated with the  ratings provided by the two human raters who  93  agreed well (_r = 0.32 and 0.31), resulting in  agreement of ratings in 63% and 66% of the  questions. In other words, the agreement of  ratings provided by the system and by two human  raters is comparable to the highest agreement rate  achieved between the human raters.  Some of the human ratings diverged  substantially. Therefore, we computed two  restrictive measures based on the ratings to  evaluate the performance of DP. Both scores are  Boolean. The first score is \"lenient\"; it reports a  presupposition only if at least two raters report a  presupposition for the question (rating of 3 or 4).  We call this measure P~j, a majority-based  presupposition count. The second score is strict.  It reports a presupposition only if all three raters  report a presupposition. This measure is called  Pcomp, a presupposition count based on complete  agreement. It results in fewer detected  presuppositions overall: Pcomp reports  presuppositions for 29 of the questions (33%),  whereas P~j reports 57 (64%).  2.3 Evaluation of the DP  DP ratings were significantly correlated only with  Pcomp (0.35). DP and P~o~ ratings were in  agreement for 67% of the questions. Table 3 lists  hit and false alarm rates for DP, separately for P~j  and P~omp. The hit rate indicates how many of the  presuppositions identified by the human ratings  were detected by DP. The false alarm rate  indicates how often DP reported a presupposition  when the human raters did not. The measures look  better with respect to the complete agreement  criterion, P~omp-  Table 3 further lists recall and precision scores.  The recall rate indicates how many  presuppositions DP detects out of the  presuppositions reported by the human rating  criterion (computed as hits, divided by the sum of  hits and misses). The precision score (computed  as hits, divided by the sum of hits and false  alarms) measures how many presuppositions  reported by DP are actually present, as reported by  the human ratings.  Table 3: Performance measures for DP with respect to hits, false alarms, and misses.  Hit rate False alarm rate Recall Precision d'  P~j 0.54 0.34 0.66 0,74 0.50  Pcomo 0.72 0.35 0.72 0,50 0.95  All measures, except for precision, look  comparable or better in relation to Pco~,,  including d', which measures the actual power of  DP to discriminate questions with and without  presuppositions. Of course, picking a criterion  with better matches does not improve the  system's performance in itself.  3 An updated version of DP  Based on the first results, we made a few  modifications and then reevaluated DP. In  particular, we added items to the possession  exception list based on the new corpus and made  some of the no-presupposition rules more  specific. As a more drastic change, we updated  the decision tree structure so that presupposition  indicators overrule indicators against  presuppositions, increasing the number of  reported presuppositions for cases of conflicting  indicators:  If there is evidence for a problem, report \"Problem\"  Else  if evidence against problem, report \"No problem\"  else, report \"Probably not a problem\"  Separate analyses show that the modification of  the decision tree accounts for most of the  performance improvement.  3.1 Results  Table 4 lists the performance measures for the  updated DP. Hit and recall rate increased, but so  did the false alarm rate, resulting in a lower  precision score. The d' score of the updated  system with respect o Pcomp (1.3) is substantially  better. The recall rate for this setting is perfect,  i.e., DP did not miss any presuppositions. Since  survey methodologists will decide whether the  presupposition is really a problem, a higher false  alarm rate is preferable to missing out  presupposition cases. Thus, the updated DP is an  improvement over the first version.  R/t 94 Table 4: Performance measures for the updated DP with respect to hits, false alarms, and misses.  Hit rate False alarm rate Recall Precision d'  Pmai 0.75 0.44 0.84 0.75 0.8  P~o,~p 0.90 0.52 1.00 0.46 1.3  Conclusion  DP can detect presuppositions, and can thereby  reliably help a survey methodologist to eliminate  incorrect presuppositions. The results for DP  with respect o Pco~p are comparable to, and in  some cases even better than, the results for the  other five categories. This is a very good result,  since most of the five problems allow for \"easy\"  and \"elegant\" solutions, whereas DP needs to be  adjusted to a variety of problems.  It is interesting that the performance of DP looks  so much better when compared to the complete  agreement score, Pcomp than when compared to  P~j. Recall that Pcomp only reports a  presupposition if all the raters report one. The  high agreement of the raters in these cases can  presumably be explained by the salience of the  presupposition problem. This indicates that DP  makes use of reliable indicators for its  performance. Good agreement with the other  measure, Pmaj, would suggest that DP additionally  reports presuppositions i  cases where humans do  not agree that a presupposition is present. The  higher agreement with the stricter measure is thus  a good result.  DP currently works like the other modules of  QUA\\]D: it reports potential problems, but leaves  it to the survey methodologist to decide whether  to act upon the feedback. As such, DP is a  substantial addition to QUA\\]D. A future  challenge is to turn DP into a DIP (detector of  incorrect presuppositions), that is, to reduce the  number of reported presuppositions to those  likely to be incorrect. DP currently evaluates all  questions independent of context, resulting in  frequent detections. For example, 20 questions  about \"this person\" may follow one question that  establishes the referent. High-frequency  repetitive presupposition reports could easily get  annoying.  Is a DIP system feasible? At present, it is  difficult for NLP systems to use information from  context in the evaluation of a statement. What is  required to solve this problem is a mechanism that  determines whether a presupposed entity (an  object, an activity, an assertion, etc.) has been  established as applicable in the previous discourse  (e.g., in preceding questions).  The Construction Integration (CI) model by  Kintsch (1998) provides a good example for how  such reference ambiguity can be resolved. CI uses  a semantic network that represents an entity in the  discourse focus (such as \"this person\") through  higher activations of its links to other concept  nodes. Perhaps models such as the CI model can  be integrated into the QUAID model to perform  context analyses, in combination with tools like  Latent Semantic Analysis (LSA, Landauer &  Dumais, 1997), which represents text units as  vectors in a high-dimensional semantic space.  LSA measures the semantic similarity of text units  (such as questions) by computing vector cosines.  This feature may make LSA a useful tool in the  detection of a previous question that establishes a  presupposed ntity in a later question.  However, questionnaires differ from connected  discourse, such as coherent stories, in aspects that  make the present problem rather more difficult.  Most importantly, the referent for \"this person\"  may have been established in question umber 1,  and the current question containing the  presupposition \"this person\" is question umber  52. A DIP system would have to handle a flexible  amount of context, because the distance between  questions establishing the correctness of a  presupposition a d a question building up on it can  vary. On the one hand, one could limit the  considered context to, say, three questions and risk  missing the critical question. On the other hand, it  is computationally expensive to keep the complete  previous context in the systems \"working  memory\" to evaluate the few presuppositions  which may refer back over a large number of  questions. Solving this problem will likely require  comparing a variety of different settings.  Q~ 95 Acknowledgements  This work was partially supported by the Census  Bureau (43-YA-BC-802930) and by a grant from  the National Science Foundation (SBR 9720314  and SBR 9977969). We wish to acknowledge  three colleagues for rating the questions in our  evaluation text corpus, and our collaborator  Susan Goldman as well as two anonymous  reviewers for helpful comments.  References  Abney, S. (1996a). Partial parsing via finite-state  cascades. In Proceedings of the ESSLLI '96 Robust  Parsing Workshop.  Abney, S. (1996b). Methods and statistical linguistics.  In J. Klavans & P. Resnik (Eds.), The Balancing  Act. Cambridge, MA: MIT Press  Brill, E. (1992). A simple rule-based part of speech  tagger. In Proceedings of the Third Conference on  Applied Natural Language Processing. ACL.  Brill, E. (1993). A corpus-based approach to \u001b[1;32;40m language \u001b[0;0m  learning. Ph.D. thesis, University of Pennsylvania,  Philadelphia, PA.  Brill, E. (1994). Some advances in rule-based part of  speech tagging. In Proceedings of the Twelfth  National Conference on Articial Intelligence. AAAI  Press.  Dijkstra, T., & de Smedt, K. (1996). Computational  psycholinguistics. AI and connectionist models of  human \u001b[1;32;40m language \u001b[0;0m processing. London: Taylor &  Francis.  Graesser, A. C., Bommareddy, S., Swamer, S., &  Golding, J. (1996). Integrating questionnaire design  with a cognitive computational model of human  question answering. In N. Schwarz & S. Sudman  (Eds.), Answering questions: Methods of  determining cognitive and communicative processes  in survey research (pp. 343-175). San Francisco,  CA: Jossey-Bass.  Graesser, A.C., & Franklin, S.P. (1990). QUEST: A  cognitive model of question answering. Discourse  Processes, 13, 279-304.  Graesser, A.C., Kennedy, T., Wiemer-Hastings, P., &  Ottati, V. (1999). The use of computational  cognitive models to improve questions on surveys  and questionnaires. In M. Sirken, D. Herrrnann, S.  Schechter, N. Schwarz, J. Tanur, & R. Tourangeau  (Eds.), Cognition and Survey Research (pp. 199-  216). New York: John Wiley & Sons.  Graesser, A.C., Wiemer-Hastings, K., Kreuz, R.,  Wiemer-Hastings, P., & Marquis, K. (in press).  QUAID: A questionnaire evaluation aid for survey  methodologists. Behavior Research Methods,  Instruments, & Computers.  Kintsch, W. (1998). Comprehension. A paradigm for  cognition. Cambridge, UK: Cambridge University  Press.  Landauer, T.K., & Dumais, S.T. (1997). A solution to  Plato's problem: The latent semantic analysis theory  of acquisition, induction, and representation of  knowledge. Psychological Review, 104, 211-240.  McCawley, J.D. (1981). Everything that linguists have  always wanted to know about logic. Chicago:  University of Chicago Press.  QI~ 96 \n",
            "-----------------------------\n",
            "--- document 20: Experiments on Sentence Boundary Detection  Mark  Stevenson  and  Rober t  Ga izauskas   Depar tment  of  Computer  Science,  Un ivers i ty  of  Sheff ield  Regent  Cour t ,  211 Por tobe l lo  St reet ,   Sheff ield  S1 4DP Un i ted  K ingdom  {marks, robertg}@dcs, shef. ac.uk  Abst ract   This paper explores the problem of identifying sen-  tence boundaries in the transcriptions produced by  automatic speech recognition systems. An experi-  ment which determines the level of human perform-  ance for this task is described as well as a memory-  based computational pproach to the problem.  1 The  Prob lem  This paper addresses the problem of identifying sen-  tence boundaries in the transcriptions produced by  automatic speech recognition (ASR) systems. This  is unusual in the field of text processing which has  generally dealt with well-punctuated text: some of  the most commonly used texts in NLP are machine  readable versions of highly edited documents uch  as newspaper articles or novels. However, there are  many types of text which are not so-edited and the  example which we concentrate on in this paper is  the output from ASR systems. These differ from  the sort of texts normally used in NLP in a number  of ways; the text is generally in single case (usually  upper), unpunctuated and may contain transcrip-  tion errors. 1 Figure 1 compares a short text in the  format which would be produced by an ASR system  with a fully punctuated version which includes case  information. For the remainder of this paper error-  free texts such as newspaper articles or novels shall  be referred to as \"standard text\" and the output  from a speech recognition system as \"ASR text\".  There are many possible situations in which an  NLP system may be required to process ASR text.  The most obvious examples are NLP systems which  take speech input (eg. Moore et al. (1997)). Also,  dictation software programs do not punctuate or  capitalise their output but, if this information could  be added to ASR text, the results would be far more  usable. One of the most important pieces of inform-  1 Speech recognition systems are often evaluated in terms  of word error ate (WER), the percentage oftokens which are  wrongly transcribed. For large vocabulary tasks and speaker-  independent systems, WER varies between 7% and 50%, de-  pending upon the quality of the recording being recognised.  See, e.g., Cole (1996).  G00D EVENING GIANNI VERSACE ONE OF THE  WORLDS LEADING FASHION DESIGNERS HAS  BEEN MURDERED IN MIAMI POLICE SAY IT WAS  A PLANNED KILLING CARRIED OUT LIKE AN  EXECUTION SCHOOLS INSPECTIONS ARE GOING  TO BE TOUGHER TO FORCE BAD TEACHERS OUT  AND THE FOUR THOUSAND COUPLES WH0 SHARED  THE QUEENS GOLDEN DAY  Good evening. Gi~nni Versace, one of  the world's leading fashion designers,  has been murdered in Miami. Police say  it was a planned killing carried out  like an execution. Schools inspections  are going to be tougher to force bad  teachers out. And the four thousand  couples who shared the Queen's golden  day.  Figure 1: Example text shown in standard and ASR  format  ation which is not available in ASR output is sen-  tence boundary information. However, knowledge of  sentence boundaries i required by many NLP tech-  nologies. Part of speech taggers typically require  input in the format of a single sentence per line (for  example Brill's tagger (Brill, 1992)) and parsers gen-  erally aim to produce a tree spanning each sentence.  Only the most trivial linguistic analysis can be car-  ried out on text which is not split into sentences.  It is worth mentioning that not all transcribed  speech can be sensibly divided into sentences. It has  been argued by Gotoh and Renals (2000) that the  main unit in spoken \u001b[1;32;40m language \u001b[0;0m is the phrase rather  than the sentence. However, there are situations  in which it is appropriate to consider spoken lan-  guage to be made up from sentences. One example  is broadcast news: radio and television news pro-  grams. The DARPA HUB4 broadcast news evalu-  ation (Chinchor et al., 1998) focussed on informa-  tion extraction from ASR text from news programs.  Although news programs are scripted there are of-  ten deviations from the script and they cannot be  relied upon as accurate transcriptions of the news  84  program. The spoken portion of the British National  Corpus (Burnard, 1995) contains 10 million words  and was manually marked with sentence boundar-  ies. A technology which identifies entence boundar-  ies could be used to speed up the process of creating  any future corpus of this type.  It is important o distinguish the problem just  mentioned and another problem sometimes called  \"sentence splitting\". This problem aims to identify  sentence boundaries in standard text but since this  includes punctuation the problem is effectively re-  duced to deciding which of the symbols which poten-  tially denote sentence boundaries ( . ,  !,  ?) actually  do. This problem is not trivial since these punc-  tuation symbols do not always occur at the end of  sentences. For example in the sentence \"Dr. Jones  l ec tures  at  U.C.L.A.\" only the final full stop de-  notes the end of a sentence. For the sake of clarity  we shall refer to the process of discovering sentence  boundaries in standard punctuated text as \"punc-  tuation disambiguation\" and that of finding them  in unpunctuated ASR text as \"sentence boundary  detection\".  2 Related Work  Despite the potential application of technology  which can carry out the sentence boundary detec-  tion task, there has been little research into the  area. However, there has been work in the re-  lated field of punctuation disambiguation. Palmer  and Hearst (1994) applied a neural network to the  problem. They used the Brown Corpus for training  and evaluation, noting that 90% of the full stops in  this text indicate sentence boundaries. They used  the part of speech information for the words sur-  rounding a punctuation symbol as the input to a  feed-forward neural network. But, as we mentioned,  most part of speech taggers require sentence bound-  aries to be pre-determined and this potential cir-  cularity is avoided by using the prior probabilities  for each token, determined from the Brown corpus  markup. The network was trained on 573 potential  sentence nding marks from the Wall Street Journal  and tested on 27,294 items from the same corpus.  98.5% of punctuation marks were correctly disam-  biguated.  Reynar and Ratnaparkhi (1997) applied a max-  imum entropy approach to the problem. Their  system considered only the first word to the left  and right of any potential sentence boundary and  claimed that examining wider context did not help.  For both these words the prefix, suffix, presence of  particular characters in the prefix or suffix, whether  the candidate is honorific (Mr., Dr. etc.) and  whether the candidate is a corporate designator (eg.  Corp.) are features that are considered. This sys-  tem was tested on the same corpus as Palmer and  Hearst's system and correctly identified 98.8% of  sentence boundaries. Mikheev (1998) optimised this  approach and evaluated it on the same test corpus.  An accuracy of 99.2477% was reported, to our know-  ledge this is the highest quoted result for this test  set.  These three systems achieve very high results  for the punctuation disambiguation task. It would  seem, then, that this problem has largely been  solved. However, it is not clear that these techniques  will be as successful for ASR text. We now go on to  describe a system which attempts a task similar to  sentence boundary detection of ASR text.  Beeferman et al. (1998) produced a system, \"CY-  BERPUNC\", which added intra-sentence punctu-  ation (i.e. commas) to the output of an ASR system.  They mention that the comma is the most frequently  used punctuation symbol and its correct insertion  can make a text far more legible. CYBERPUNC  operated by augmenting a standard trigram speech  recognition model with information about commas;  it accesses only lexical information. CYBERPUNC  was tested by separating the trigram model from  the ASR system and applying it to 2,317 sentences  from the Wall Street Journal. The system achieved  a precision of 75.6% and recall of 65.6% compared  against he original punctuation i  the text. 2 A fur-  ther qualitative valuation was carried out using 100  randomly-drawn output sentences from the system  and 100 from the Wall Street Journal. Six human  judges blindly marked each sentence as either ac-  ceptable or unacceptable. It was found that the  Penn TreeBank sentences were 86% correct and the  system output 66% correct. It is interesting that the  human judges do not agree completely on the ac-  ceptability of many sentences from the Wall Street  Journal.  In the next section we go on to describe exper-  iments which quantify the level of agreement that  can be expected when humans carry out sentence  boundary detection. Section 4 goes on to describe a computational pproach to the problem.  3 Determining Human Abi l i ty  Beeferman et. al.'s experiments demonstrated that  humans do not always agree on the acceptability of  comma insertion and therefore it may be useful to  determine how often they agree on the placing of  sentence boundaries. To do this we carried out ex-  periments using transcriptions ofnews programmes,  specifically the transcriptions of two editions of the  ~Precision and recall are complementary  evaluation met-  rics commonly  used in Information Retrieval (van Rijsbergen,  1979). In this case precision is the percentage of commas pro-  posed by the system which are correct while recall is the per-  centage of the commas occurring in the test corpus which the  system identified.  R~  BBC television program \"The Nine O'Clock News\" .3  The transcriptions consisted of punctuated mixed  case text with sentences boundaries marked using a  reserved character (\"; \"). These texts were produced  by trained transcribers listening to the original pro-  gram broadcast.  Six experimental subjects were recruited. All sub-  jects were educated to at least Bachelor's degree  level and are either native English speakers or flu-  ent second \u001b[1;32;40m language \u001b[0;0m speakers. Each subject was  presented with the same text from which the sen-  tence boundaries had been removed. The texts were  transcriptions of two editions of the news program  from 1997, containing 534 sentences and represented  around 50 minutes of broadcast news. The subjects  were randomly split into two groups. The subjects  in the first group (subjects 1-3) were presented with  the text stripped of punctuation and converted to  upper case. This text simulated ASR text with no  errors in the transcription. The remaining three sub-  jects (4-6) were presented with the same text with  punctuation removed but case information retained  (i.e. mixed case text). This simulated unpunctuated  standard text. All subjects were asked to add sen-  tence boundaries to the text whenever they thought  they occurred.  The process of determining human ability at some  linguistic task is generally made difficult by the lack  of an appropriate reference. Often all we have to  compare one person's judgement with is that of an-  other. For example, there have been attempts to  determine the level of performance which can be ex-  pected when humans perform word sense disambig-  uation (Fellbaum et al., 1998) but these have simply  compared some human judgements against others  with one being chosen as the \"expert\". We have  already seen, in Section 2, that there is a signific-  ant degree of human disagreement over the accept-  ability of intra-sentential punctuation. The human  transcribers ofthe \"Nine O'Clock News\" have access  to the original news story which contains more in-  formation than just the transcription. Under these  conditions it is reasonable to consider their opinion  as expert.  Table 1 shows the performance of the human sub-  jects compared to the reference transcripts. 4  An algorithm was implemented to provide a  baseline tagging of the text. The average length of  sentences in our text is 19 words and the baseline al-  gorithm randomly assigns a sentence break at each  word boundary with a probability of ~ .  The two  annotators labelled \"random\" show the results when  this algorithm is applied. This method produced a 3This is a 25 minute long television ews program broad-  cast in the United Kingdom on Monday to Friday evenings.  4F-measure (F) is a weighted harmonic ombining preci-  sion (P) and recall (R) via the formula 2PR  PTR \"  very low result in comparison to the expert annota-  tion.  1 Upper 84 68 76  2 Upper 93 78 85  3 Upper 90 76 82  4 Mixed 97 90 94  5 Mixed 96 89 92  6 Mixed 97 67 79  Random Upper 5 5 5  Random Mixed 5 5 5  Table 1: Results from Human Annotation Experi-  ment  The performance of the human annotators on the  upper case text is quite significantly lower than  the reported performance of the algorithms which  performed punctuation disambiguation on standard  text as described in Section 2. This suggests that  the performance which may be obtained for this task  may be lower than has been achieved for standard  text.  ~Sarther insight into the task can be gained from  determining the degree to which the subjects agreed.  Carletta (1996) argues that the kappa statistic (a)  should be adopted to judge annotator consistency  for classification tasks in the area of discourse and  dialogue analysis. It is worth noting that the prob-  lem of sentence boundary detection presented so far  in this paper has been formulated as a classification  task in which each token boundary has to be clas-  sifted as either being a sentence boundary or not.  Carletta argues that several incompatible measures  of annotator agreement have been used in discourse  analysis, making comparison impossible. Her solu-  tion is to look to the field of content analysis, which  has already experienced these problems, and adopt  their solution of using the kappa statistic. This de-  termines the difference between the observed agree-  ment for a linguistic task and that which would be  expected by chance. It is calculated according to for-  mula 1, where Pr(A) is the proportion of times the  annotators agree and Pr(E) the proportion which  would be expected by chance. Detailed instructions  on calculating these probabilities are described by  Siegel and Castellan (1988).  Pr(A) - Pr(E)   = (1)  1 - Pr(E)  The value of the kappa statistic ranges between  1 (perfect agreement) and 0 (the level which would  be expected by chance). It has been claimed that  content analysis researchers usually regard a > .8 to  demonstrate good reliability and .67 < ~ < .8 al-  f16  lows tentative conclusions to be drawn (see Carletta  (1996)).  We began to analyse the data by computing the  kappa statistic for both sets of annotators. Among  the two annotators who marked the mixed case (sub-  jects 4 and 5) there was an observed kappa value of  0.98, while there was a measure of 0.91 for the three  subjects who annotated the single case text. These  values are high and suggest a strong level of agree-  ment between the annotators. However, manual  analysis of the annotated texts suggested that the  subjects did not agree on many cases. We then ad-  ded the texts annotated by the \"random\" annotation  algorithm and calculated the new ~ values. It was  found that the mixed case test produced a kappa  value of 0.92 and the upper case text 0.91. These  values would still suggest a high level of agreement  although the sentences produced by our random al-  gorithm were nonsensical.  The problem seems to be that most word bound-  aries in a text are not sentence boundaries. There-  fore we could compare the subjects' annotations  who had not agreed on any sentence boundaries but  find that they agreed most word boundaries were  not sentence boundaries. The same problem will  effect other standard measures of inter-annotator  agreement such as the Cramer, Phi and Kendall  coefficients (see Siegel and Castellan (1988)). Car-  letta mentions this problem, asking what the dif-  ference would be if the kappa statistic were com-  puted across \"clause boundaries, transcribed word  boundaries, and transcribed phoneme boundaries\"  (Carletta, 1996, p. 252) rather than the sentence  boundaries he suggested. It seems likely that more  meaningful ~ values would be obtained if we restric-  ted to the boundaries between clauses rather than  all token boundaries. However, it is difficult to ima-  gine how clauses could be identified without parsing  and most parsers require part of speech tagged input  text. But, as we already mentioned, part of speech  taggers often require input text split into sentences.  Consequently, there is a lack of available systems for  splitting ASR text into grammatical clauses.  4 A Computat iona l  Approach  to   Sentence  Boundary  Detect ion   The remainder of this paper describes an implemen-  ted program which attempts entence boundary de-  tection. The approach is based around the Timbl  memory-based learning algorithm (Daelemans et al.,  1999) which we previously found to be very success-  ful when applied to the word sense disambiguation  problem (Stevenson and Wilks, 1999).  Memory-based learning, also known as case-based  and lazy learning, operates by memorising a set of  training examples and categorising new cases by as-  signing them the class of the most similar learned  example. We apply this methodology to the sen-  tence boundary detection task by presenting Timbl  with examples of word boundaries from a train-  ing text, each of which is categorised as either  sentence_boundary or no_boundary. Unseen ex-  amples are then compared and categorised with the  class of the most similar example. We shall not  discuss the method by which Timbl determines the  most similar training example which is described by  Daelemans et al. (1999).  Following the work done on punctuation disambig-  uation and that of Beeferman et. al. on comma in-  sertion (Section 2), we used the Wall Street Journal  text for this experiment. These texts are reliably  part of speech tagged 5 and sentence boundaries can  be easily derived from the corpus. This text was  initially altered so as to remove all punctuation and  map all characters into upper case. 90% of the cor-  pus, containing 965 sentence breaks, was used as a  training corpus with the remainder, which contained  107 sentence breaks, being held-back as unseen test  data. The first stage was to extract some statistics  from the training corpus. We examined the training  corpus and computed, for each word in the text, the  probability that it started a sentence and the prob-  ability that it ended a sentence. In addition, for each  part of speech tag we also computed the probability  that it is assigned to the first word in a sentence and  the probability that it is assigned to the last word. 6  Each word boundary in the corpus was translated to  a feature-vector representation consisting of 13 ele-  ments, shown in Table 2. Vectors in the test corpus  are in a similar format, the difference being that the  classification (feature 13) is not included.  The results obtained are shown in the top row of  Table 3. Both precision and recall are quite prom-  ising under these conditions. However, this text is  different from ASR text in one important way: the  text is mixed case. The experimented was repeated  with capitalisation information removed; that is,  features 6 and 12 were removed from the feature-  vectors. The results form this experiment are shown  in the bottom row of Table 3. It can be seen that  the recorded performance is far lower when capital-  isation information is not used, indicating that this  is an important feature for the task.  These experiments have shown that it is much  easier to add sentence boundary information to  mixed case test, which is essentially standard text  with punctuation removed, than ASR text, even as-  5Applying a priori tag probability distributions could have  been used rather than the tagging in the corpus as such re-  liable annotations may not be available for the output of an  ASR system. Thus, the current experiments should be viewed  as making an optimistic assumption.  eWe attempted to smooth these probabilities using Good-  Turing frequency estimation (Gale and Sampson, 1996) but  found that it had no effect on the final results.  87  Position Feature  1  2  3  4  5  6  7  8  9  10  11  12  13  Preceding word  Probability preceding word ends a sentence  Part of speech tag assigned to preceding word  Probability that part of speech tag (feature 3) is assigned to last word in a sentence  Flag indicating whether preceding word is a stop word  Flag indicating whether preceding word is capitalised  Following word  Probability following word begins a sentence  Part of speech tag assigned to following word  Probability that part of speech (feature 9) is assigned to first word in a sentence  Flag indicating whether following word is a stop word  Flag indicating whether following word is capitalised word  sentence_boundary or no_boundary  Table 2: Features used in Timbl representation  Case information \\[I P I R I F  Applied I 78 \\[ 75 \\[ 76  Not applied 36 35 35  Table 3: Results of the sentence boundary detection  program  suming a zero word error rate. This result is in  agreement with the results from the human annota-  tion experiments described in Section 3. However,  there is a far greater difference between the auto-  matic system's performance on standard and ASR  text than the human annotators.  Reynar and Ratnaparkhi (1997) (Section 2) ar-  gued that a context of one word either side is suf-  ficient for the punctuation disambiguation problem.  However, the results of our system suggest that this  may be insufficient for the sentence boundary detec-  tion problem even assuming reliable part of speech  tags (cf note 5).  These experiments do not make use of prosodic in-  formation which may be included as part of the ASR  output. Such information includes pause length,  pre-pausal lengthening and pitch declination. If this  information was made available in the form of extra  features to a machine learning algorithm then it is  possible that the results will improve.  5 Conc lus ion   This paper has introduced the problem of sentence  boundary detection on the text produced by an ASR  system as an area of application for NLP technology.  An attempt was made to determine the level of  human performance which could be expected for the  task. It was found that there was a noticeable dif-  ference between the observed performance for mixed  and upper case text. It was found that the kappa  statistic, a commonly used method for calculating  inter-annotator agreement, could not be applied dir-  ectly in this situation.  A memory-based system for identifying sentence  boundaries in ASR text was implemented. There  was a noticeable difference when the same system  was applied to text which included case information  demonstrating that this is an important feature for  the problem.  This paper does not propose to offer a solution to  the sentence boundary detection problem for ASR  transcripts. However, our aim has been to high-  light the problem as one worthy of further explor-  ation within the field of NLP and to establish some  baselines (human and algorithmic) against which  further work may be compared.  Acknowledgements   The authors would like to thank Steve Renals and  Yoshihiko Gotoh for providing the data for human  annotation experiments and for several useful con-  versations. They are also grateful to the following  people who took part in the annotation experiment:  Paul Clough, George Demetriou, Lisa Ferry, Michael  Oakes and Andrea Setzer.  References   D. Beeferman, A. Berger, and J. Lafferty. 1998. CY-  BERPUNC: A lightweight punctuation annota-  tion system for speech. In Proceedings of the IEEE  International Conference on Acoustics, Speech  and Signal Processing, pages 689-692, Seattle,  WA.  E. Brill. 1992. A simple rule-based part of speech  tagger. In Proceeding of the Third Conference on  Applied Natural Language Processing (ANLP-92),  pages 152-155, Trento, Italy.  R~ 88 L. Burnard, 1995. Users Reference Guide for the  British National Corpus. Oxford University Com-  puting Services.  J. Carletta. 1996. Assessing agreement on classific-  ation tasks: the kappa statistic. Computational  Linguistics, 22(2):249-254.  N. Chinchor, P. Robinson, and E. Brown.  1998. HUB-4 Named Entity Task Defini-  tion (version 4.8). Technical report, SAIC.  http ://www. nist. gov/speech/hub4-98.  R. Cole, editor. 1996. Survey of the State of the  Art in Human Language Technology. Available at:  http://cslu.cse.ogi.edu/HLTsurvey/HLTsurvey.html.  Site visited 17/11/99.  W. Daelemans, J. Zavrel, K. van der Sloot,  and A. van den Bosch. 1999. TiMBL: Tilburg  memory based learner version 2.0, reference guide.  Technical report, ILK Technical Report 98-03.  ILK Reference Report 99-01, Available from  http ://ilk. kub. nl/\" ilk/papers/ilk9901, ps. gz.  C. Fellbaum, J. Grabowski, S. Landes, and A. Ban-  mann. 1998. Matching words to senses in Word-  Net: Naive vs. expert differentiation of senses. In  C. Fellbaum, editor, WordNet: An electronic lex-  ieal database and some applications. MIT Press,  Cambridge, MA.  W. Gale and G. Sampson. 1996. Good-Turing  frequency estimation without tears. Journal of  Quantitave Linguistics, 2(3):217-37.  Y. Gotoh and S. Renals. 2000. Information extrac-  tion from broadcast news. Philosophical Trans-  actions of the Royal Society of London, series A:  Mathematical, Physical and Engineering Sciences.  (to appear).  A. Mikheev. 1998. Feature lattices for maximum en-  tropy modelling. In Proceedings of the 36th Meet-  ing of the Association for Computational Linguist-  ics (COLING-ACL-98), pages 848-854, Montreal,  Canada.  R. Moore, J. Dowding, H. Bratt, J. Gawron,  Y. Gorfu, and A. Cheyer. 1997. CommandTalk:  A Spokcaa-Language Interface to Battlefield Simu-  lations. In Proceedings of the Fifth Conference on  Applied Natural Language Processing, pages 1-7,  Washington, DC.  D. Palmer and M. Hearst. 1994. Adaptive sen-  tence boundary disambiguation. In Proceedings of  the 1994 Conference on Applied Natural Language  Processing, pages 78-83, Stutgart, Germany.  J. Reynar and A. Ratnaparkhi. 1997. A max-  imum entropy approach to identifying sentence  boundries. In Proceedings of the Fifth Conference  on Applied Natural Language Processing, pages  16-19, Washington, D.C.  S. Siegel and N. Castellan. 1988. Nonparametrie  Statistics for the Behavioural Sciences. McGraw-  Hill, second edition.  M. Stevenson and Y. Wilks. 1999. Combining weak  knowledge sources for sense disambiguation. In  Proceedings of the Sixteenth International Joint  Conference on Artificial Intelligence, pages 884-  889. Stockholm, Sweden.  C. van Rijsbergen. 1979. Information Retrieval.  Butterworths, London.  89  \n",
            "-----------------------------\n"
          ]
        }
      ]
    }
  ]
}