{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vaC5AMGXyO_7"
      },
      "source": [
        "This notebook demonstrates the basic steps of text preprocessing.\n",
        "\n",
        "\n",
        "*   converting all letters to lower or upper case\n",
        "*   converting numbers into words or removing numbers\n",
        "*   removing punctuations, accent marks and other diacritics\n",
        "*   removing white spaces\n",
        "*   expanding abbreviations\n",
        "*   removing stop words, sparse terms, and particular words\n",
        "*   text canonicalization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atRzc43Zywrq"
      },
      "source": [
        "# Convert text to lowercase\n",
        "simply use the standard python string (str) package "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bK8yF2yYyEor",
        "outputId": "971695d1-ffc9-4952-b441-5b59bf524ae9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "the 5 biggest countries by population in this world are china, india, united states, indonesia, and brazil.\n"
          ]
        }
      ],
      "source": [
        "input_str = \"The 5 biggest countries by population in this world are China, India, United States, Indonesia, and Brazil.\"\n",
        "input_str = input_str.lower()\n",
        "print(input_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4TUOerazOGn"
      },
      "source": [
        "# Remove numbers\n",
        "use the regex (re) package to remove numbers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_AES3oQJ1UNl",
        "outputId": "47a0db89-c410-4c49-926a-8e6f657d9066"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " A sample space contains  red balls,  white balls,  pink balls, and  blue balls.\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "input_str = \" A sample space contains 3 red balls, 5 white balls, 7 pink balls, and 2 blue balls.\"\n",
        "result = re.sub(r'\\d+', '', input_str)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmQrPIxX18a6"
      },
      "source": [
        "# Remove punctuation\n",
        "use the string package to remove a predefined set of symbols [!”#$%&’()*+,-./:;<=>?@[\\]^_`{|}~]:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w55anIZY1ztO",
        "outputId": "95dc3397-cd16-4508-8840-547dbfd9f222"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "This is an example of string with many  punctuation\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "input_str = \"This &is [an] example? {of} string. with.?+-*=%& many ?@ punctuation!!!!\"\n",
        "# result = input_str.translate(str.maketrans(string.punctuation,\"\"))\n",
        "result = [c for c in input_str if c not in string.punctuation]\n",
        "result = ''.join(result)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1V7YAM9AXLq"
      },
      "source": [
        "# Remove whitespaces"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWKn2MdL-eSn",
        "outputId": "5e67e102-fcf1-4ceb-b777-fbaf5ee606c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before whitespace removed:  \t a string example with white space \n",
            "After whitespace removed: a string example with white space\n"
          ]
        }
      ],
      "source": [
        "input_str = \" \\t a string example with white space \"\n",
        "print(f'Before whitespace removed: {input_str}')\n",
        "input_str = input_str.strip()\n",
        "print(f'After whitespace removed: {input_str}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfLC_vj3BUyo"
      },
      "source": [
        "# Remove stop words\n",
        "**Stop words** are the most common words in a language like “the”, “a”, “on”, “is”, “all”. These words do not carry important meaning and are usually removed from texts. It is possible to remove stop words using Natural Language Toolkit (NLTK)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vADDDnAZ-O-m",
        "outputId": "2dacc9e7-a0ff-4578-863e-febb2ccb1a5d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to C:\\Users\\Duc Minh\n",
            "[nltk_data]     Nguyen\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to C:\\Users\\Duc Minh\n",
            "[nltk_data]     Nguyen\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before removing: ['NLTK', 'is', 'a', 'leading', 'platform', 'for', 'building', 'Python', 'programs', 'to', 'work', 'with', 'human', 'language', 'data', '.']\n",
            "After removing: ['NLTK', 'leading', 'platform', 'building', 'Python', 'programs', 'work', 'human', 'language', 'data', '.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "stop_words= stopwords.words('english')\n",
        "input_str = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
        "tokens = word_tokenize(input_str)\n",
        "print(f'Before removing: {tokens}')\n",
        "result = [i for i in tokens if not i in stop_words]\n",
        "print(f'After removing: {result}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTdzK0A1E_Tj"
      },
      "source": [
        "# Part of speech tagging (POS)\n",
        "POS aims to assign parts of speech to each word of a given text (such as nouns, verbs, adjectives, and others) based on its definition and its context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGrfEcFCE46W",
        "outputId": "70ea75c9-ba37-4572-d0c1-982a451a0b89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[('Parts', 'NNS'), ('of', 'IN'), ('speech', 'NN'), ('examples', 'NNS'), (':', ':'), ('an', 'DT'), ('article', 'NN'), (',', ','), ('to', 'TO'), ('write', 'VB'), (',', ','), ('interesting', 'VBG'), (',', ','), ('easily', 'RB'), (',', ','), ('and', 'CC'), (',', ','), ('of', 'IN')]\n"
          ]
        }
      ],
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "input_str = \"Parts of speech examples: an article, to write, interesting, easily, and, of\"\n",
        "tokens = word_tokenize(input_str)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "print(pos_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Af5HYV9_HCGy"
      },
      "source": [
        "# Chunking (shallow parsing)\n",
        "Chunking is a natural language process that identifies constituent parts of sentences (nouns, verbs, adjectives, etc.) and links them to higher order units that have discrete grammatical meanings (noun groups or phrases, verb groups, etc.) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uc_0VfrWHHME",
        "outputId": "13da5244-4e17-4993-8439-fffdd7a24b12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (NP A/DT black/JJ television/NN)\n",
            "  and/CC\n",
            "  (NP a/DT white/JJ stove/NN)\n",
            "  were/VBD\n",
            "  bought/VBN\n",
            "  for/IN\n",
            "  (NP the/DT new/JJ apartment/NN)\n",
            "  of/IN\n",
            "  John/NNP\n",
            "  ./.)\n"
          ]
        }
      ],
      "source": [
        "# first, we add pos tags to the text - define the part of the speech for each word\n",
        "input_str = \"A black television and a white stove were bought for the new apartment of John.\"\n",
        "tokens = word_tokenize(input_str)\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "\n",
        "# second, chunking the text\n",
        "reg_exp = \"NP: {<DT>?<JJ>*<NN>}\"\n",
        "rp = nltk.RegexpParser(reg_exp)\n",
        "result = rp.parse(pos_tags)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03V9PoloIYwK"
      },
      "source": [
        "# Named entity recognition\n",
        "Named-entity recognition (NER) aims to find named entities in text and classify them into pre-defined categories (names of persons, locations, organizations, times, etc.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdeDDfRVIYjF",
        "outputId": "996ce4b2-00d3-4dc0-debc-4c5cba56dae6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "(S\n",
            "  (PERSON Bill/NNP)\n",
            "  works/VBZ\n",
            "  for/IN\n",
            "  Apple/NNP\n",
            "  so/IN\n",
            "  he/PRP\n",
            "  went/VBD\n",
            "  to/TO\n",
            "  (GPE Boston/NNP)\n",
            "  for/IN\n",
            "  a/DT\n",
            "  conference/NN\n",
            "  ./.)\n"
          ]
        }
      ],
      "source": [
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "input_str = \"Bill works for Apple so he went to Boston for a conference.\"\n",
        "print(ne_chunk(pos_tag(word_tokenize(input_str))))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Basic_text_Preprocessing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "2ce5d6e45b08c06f5e3cf38b94c516dfbe4a3d77a6acc67d49b1ef3e77055891"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
